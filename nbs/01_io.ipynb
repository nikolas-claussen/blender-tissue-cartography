{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54aa0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01ea96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from skimage.io import imread, imsave\n",
    "import h5py\n",
    "from typing import Iterable\n",
    "import tifffile\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67c0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mcubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5514327",
   "metadata": {},
   "source": [
    "## Basic I/O\n",
    "\n",
    "In this notebook, we define a number of functions for image, mesh, and metadata loading and saving, and show how to use the with the data from the ``basics_example`` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586f315",
   "metadata": {},
   "source": [
    "## Load and subsample data for segmentation\n",
    "\n",
    "Let's load the dataset. We then enter the relevant metadata - the filename, resolution in microns, and how much we want to subsample for segmentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c7e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def adjust_axis_order(image, channel_axis=None):\n",
    "    \"\"\"\n",
    "    Adjust axis order of image (numpy array) so that the channel axis is axis 0. \n",
    "    \n",
    "    If channel axis is not specified, it is infered as the axis with the smallest number of entries.\n",
    "    If the image contains a single channel, this function adds a singleton dimension.\n",
    "    Axis order is otherwise left unchanged. Image must have 3 axes (single channel volumetric)\n",
    "    or four axes (multichannel volumetric). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image: np.ndarray with 3 or 4 axes\n",
    "        Input image.\n",
    "    channel_axis: int or None, optional\n",
    "        Channel axis\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transposed image: np.ndarray with 4 axes\n",
    "        Input image, channel now axis 0.\n",
    "    \"\"\"\n",
    "    assert 2 < len(image.shape) <5 , \"image must have 3 or 4 axes\"\n",
    "    if len(image.shape) == 3:\n",
    "        return image[np.newaxis]\n",
    "    if channel_axis is None:\n",
    "        channel_axis = np.argmin(image.shape)\n",
    "    return np.moveaxis(image, channel_axis, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2346a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = {'filename': 'basics_example/basics_example',\n",
    "                 'resolution_in_microns': (1, 0.36, 0.36), # you can typically get this from the .tif metadata\n",
    "                 'subsampling_factors': (1, 1/3, 1/3),\n",
    "                 'normal_offsets':np.linspace(-2, 2, 5) # normal offsets for map projection, in microns\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1fa308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (2, 26, 454, 511)\n"
     ]
    }
   ],
   "source": [
    "image = adjust_axis_order(imread(f\"{metadata_dict['filename']}.tif\"))\n",
    "print(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1981c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsampled image shape: (2, 26, 151, 170)\n"
     ]
    }
   ],
   "source": [
    "subsampled_image = transform.rescale(image, metadata_dict['subsampling_factors'],\n",
    "                                     channel_axis=0, preserve_range=True)\n",
    "print(\"subsampled image shape:\", subsampled_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901857d3",
   "metadata": {},
   "source": [
    "## Create 3d segmentation\n",
    "\n",
    "Now create a 3d segmentation, in this case using ilatik. ilastik works best with input saved as `.h5` data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51eb13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_h5(filename, image, h5_dataset_name=\"image\"):\n",
    "    \"\"\"Write image (numpy array) as .h5 file (e.g. as input for ilastik).\"\"\"\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset('image', data=image)\n",
    "    return None\n",
    "\n",
    "def read_h5(filename):\n",
    "    \"\"\"Read .h5 file (e.g. ilastik output) into numpy array. Loads alphabetically first entry in .h5.\"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        arr = f[sorted(f.keys())[0]][()] \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c33c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now save the subsampled image a .h5 file for input into ilastik for segmentation\n",
    "\n",
    "write_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e77650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation shape: (26, 151, 170)\n"
     ]
    }
   ],
   "source": [
    "# after creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n",
    "\n",
    "segmentation = read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\n",
    "segmentation = segmentation[0] # select the first channel of the segmentation - it's the probablity a pixel\n",
    "                               # is part of the sample\n",
    "print(\"segmentation shape:\", segmentation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba35599c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33114a60a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the segmentation in a cross section\n",
    "\n",
    "plt.imshow(segmentation[:,:,50], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c012870",
   "metadata": {},
   "source": [
    "## Meshing\n",
    "\n",
    "We convert the segmentation into a triangular mesh using the marching cubes method, and save the mesh. We introduce a couple of functions for mesh io and mesh handling. We save all meshes in as wavefront `.obj` files (see [wikipedia](https://en.wikipedia.org/wiki/Wavefront_.obj_file)). In python, we represent missing entries (such as a vertex which doesn't have a normal by `np.nan`.\n",
    "\n",
    "**Important convention** For sanities sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n",
    "\n",
    "For holding and handling `.obj` meshes, we will create a `ObjMesh` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "825202cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def flatten(lst, max_depth=1000, iter_count=0):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists into a list.\n",
    "\n",
    "    Also works with inhomogeneous lists, e.g., [[0,1],2]. The argument\n",
    "    depth determines how \"deep\" to flatten the list, e.g. with max_depth=1:\n",
    "    [[(1,0), (1,0)]] -> [(1,0), (1,0)].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lst : list\n",
    "        list-of-lists.\n",
    "    max_depth : int, optional\n",
    "        To what depth to flatten the list.\n",
    "    iter_count : int, optional\n",
    "        Helper argument for recursion depth determination.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iterator\n",
    "        flattened list.\n",
    "    \"\"\"\n",
    "    for el in lst:\n",
    "        if (isinstance(el, Iterable) and not isinstance(el, (str, bytes))\n",
    "                and iter_count < max_depth):\n",
    "            yield from flatten(el, max_depth=max_depth,\n",
    "                               iter_count=iter_count+1)\n",
    "        else:\n",
    "            yield el\n",
    "            \n",
    "def pad_list(lst, length=3, fill_value=np.nan):\n",
    "    \"\"\"Pad end of list with fill_value if shorter than desired length.\"\"\"\n",
    "    return lst + max([0, (length-len(lst))]) * [fill_value,]\n",
    "\n",
    "def unique(sequence):\n",
    "    \"\"\"Create list of unique entries in sequence while preserving order\"\"\"\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#def index_else_nan(arr, ind, target_shape=None):\n",
    "#    \"\"\"Return arr[ind] if not np.isnan(ind), else np.nan*np.ones(arr.shape[1:])\"\"\"\n",
    "#    if np.isnan(ind):\n",
    "#        target_shape = (arr.shape[1:] if len(arr.shape) > 1 else (1,)) if target_shape is None else target_shape\n",
    "#        return np.nan*np.ones(target_shape)\n",
    "#    return arr[int(ind)]\n",
    "\n",
    "def index_else_nan(arr, inds):\n",
    "    \"\"\"Return arr[inds], masked so that the result is np.nan wherever ind is nan\"\"\"\n",
    "    if len(np.shape(inds)):\n",
    "        inds = np.array(inds)\n",
    "    mask = np.isnan(inds)\n",
    "    masked_inds = np.copy(inds)\n",
    "    masked_inds[mask] = 0\n",
    "    masked_inds = masked_inds.astype(int)    \n",
    "    selected = arr[masked_inds]\n",
    "    selected[mask] = np.nan\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a02e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0364d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ObjMesh:\n",
    "    \"\"\"\n",
    "    Simple class for reading, holding, transforming, and saving 3d polygonal meshes in the .obj format.\n",
    "    See https://en.wikipedia.org/wiki/Wavefront_.obj_file.\n",
    "    Attributes\n",
    "        - vertices = [(x_0, y_0, z_0), ... ]\n",
    "        - texture_vertices = [(u_0, v_0), ...] or None\n",
    "        - normals = [(nx_0, ny_0, nz_0), ...] or None\n",
    "        - faces = [f0, ...]\n",
    "        - only_vertices = bool. \n",
    "    vertices, texture_vertices, normals are np.arrays, faces is a list of lists.\n",
    "    Each face is either a list of vertex indices (if only_vertices is True), or, if the mesh\n",
    "    has texture and normal information, a list of vertex/texture/normal index triples.\n",
    "    Missing data is represented by np.nan. Faces can be any length (triangles, quads, ...).\n",
    "    Indices start at 0!\n",
    "    \n",
    "    The method match_vertex_info can be used to match up vertices, texture vertices and\n",
    "    normals based on the face connectivity. This sets the following attributes:\n",
    "        - matched_vertices\n",
    "        - matched_texture_vertices\n",
    "        - matched_normals\n",
    "    as np.array's with correspondong rows which can be used as base points for\n",
    "    interpolation.\n",
    "\n",
    "    The property tris gets all triangles as a numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vertices, faces, texture_vertices=None, normals=None, name=None):\n",
    "        self.vertices, self.faces = (vertices, faces)\n",
    "        self.texture_vertices, self.normals = (texture_vertices, normals)\n",
    "        self.name = None\n",
    "        self.only_vertices = texture_vertices is None and normals is None\n",
    "\n",
    "    @staticmethod\n",
    "    def read_obj(filename):\n",
    "        \"\"\"\n",
    "        Return vertices, texture vertices, normals, and faces from an obj file.\n",
    "\n",
    "        Faces are lists of 3-tuples vertex/texture vertex/normal. If a certain vertex has no texture or normal \n",
    "        associated to it, the entry is np.nan, else it is an index into the vertex, texture, and normal arrays\n",
    "        (note: indices of returned faces start at 0!). See https://en.wikipedia.org/wiki/Wavefront_.obj_file.\n",
    "        \n",
    "        Intended for .obj files containing a single object only.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            filename\n",
    "        Returns\n",
    "        -------\n",
    "        mesh: ObjMesh\n",
    "        \"\"\"\n",
    "        def _str_to_int_or_nan(x):\n",
    "            \"\"\"Convert string to int or np.nan if string is empty\"\"\"\n",
    "            if x == '':\n",
    "                return np.nan\n",
    "            return int(x)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        names = [ln.split()[1:] for ln in lines if ln.startswith(\"o \")]\n",
    "        name = None if len(names) == 0 else names[0]\n",
    "        vs = np.array([ln.split()[1:] for ln in lines if ln.startswith(\"v \")]).astype(float)\n",
    "        vts = np.array([ln.split()[1:] for ln in lines if ln.startswith(\"vt \")]).astype(float)\n",
    "        ns = np.array([ln.split()[1:] for ln in lines if ln.startswith(\"vn \")]).astype(float)\n",
    "        fs = [ln.split()[1:] for ln in lines if ln.startswith(\"f \")]\n",
    "        fs = [[pad_list([_str_to_int_or_nan(y)-1 for y in x.split(\"/\")], length=3, fill_value=np.nan)\n",
    "               for x in f] for f in fs]\n",
    "        if vts.shape == (0,) and ns.shape == (0,):\n",
    "            fs = [[v[0] for v in f] for f in fs]\n",
    "            mesh = ObjMesh(vs, fs, texture_vertices=None, normals=None, name=name)\n",
    "        else:\n",
    "            mesh = ObjMesh(vs, fs, texture_vertices=vts, normals=ns, name=name)\n",
    "        return mesh\n",
    "    \n",
    "    def write_obj(self, filename,):\n",
    "        \"\"\"\n",
    "        Write mesh to .obj format.\n",
    "\n",
    "        Can write texture coordinates and normals if included. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            filename to save to\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        def _int_or_nan_to_str(x):\n",
    "            \"\"\"Convert int/nan to string. np.nan is converted to empty string\"\"\"\n",
    "            if np.isnan(x):\n",
    "                return ''\n",
    "            return str(x)\n",
    "        namelines = [\"o {}\\n\".format(*self.name)] if self.name is not None else []\n",
    "        if self.only_vertices:\n",
    "            vlines = [\"v {} {} {}\\n\".format(*v) for v in self.vertices]\n",
    "            flines = [\"f {} {} {}\\n\".format(*[int(v+1) for v in fc]) for fc in self.faces]\n",
    "            with open(filename, 'w') as f:\n",
    "                f.writelines(namelines)\n",
    "                f.writelines(vlines)\n",
    "                f.writelines(flines)\n",
    "        else:\n",
    "            assert all([len(v)==3 for v in flatten(self.faces, max_depth=1)]), \"each vertex must have 3 indices\"\n",
    "            texture_vertices = [] if self.texture_vertices is None else self.texture_vertices\n",
    "            normals = [] if self.normals is None else self.normals\n",
    "            vlines = [\"v {} {} {}\\n\".format(*v) for v in self.vertices]\n",
    "            vtlines = [\"vt {} {}\\n\".format(*vt) for vt in texture_vertices]\n",
    "            nlines = [\"vn {} {} {}\\n\".format(*n) for n in normals]\n",
    "            flines = [\"f {} {} {}\\n\".format(*[\"{}/{}/{}\".format(*[_int_or_nan_to_str(ix+1) for ix in v])\n",
    "                                              for v in fc]) for fc in self.faces]\n",
    "            with open(filename, 'w') as f:\n",
    "                f.writelines(namelines)\n",
    "                f.writelines(vlines)\n",
    "                f.writelines(vtlines)\n",
    "                f.writelines(nlines)\n",
    "                f.writelines(flines)\n",
    "        return None\n",
    "         \n",
    "    @property\n",
    "    def tris(self):\n",
    "        \"\"\"Get all triangles in mesh as a numpy array. Entries are vertex indices.\"\"\"\n",
    "        if self.only_vertices:\n",
    "            return np.array([fc for fc in self.faces if len(fc)==3])\n",
    "        else:\n",
    "            return np.array([[v[0] for v in fc] for fc in self.faces if len(fc)==3])\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def texture_tris(self):\n",
    "        \"\"\"Get all texture triangles in mesh as a numpy array. Entries are texture_vertex indices.\"\"\"\n",
    "        if self.only_vertices:\n",
    "            return np.array([np.nan for fc in self.faces if len(fc)==3])\n",
    "        else:\n",
    "            return np.array([[v[1] for v in fc] for fc in self.faces if len(fc)==3])\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def vertex_normals(self):\n",
    "        \"\"\"Get array of vertex normals. If multiple normals per vertex are stored, the last one is returned\"\"\"\n",
    "        if self.only_vertices:\n",
    "            return np.nan*np.ones_like(self.vertices)\n",
    "        v_n_pairs = {v: np.nan for v in range(self.vertices.shape[0])}\n",
    "        for v, _, n in flatten(self.faces, max_depth=1):\n",
    "            v_n_pairs[v] = n\n",
    "        return np.array([index_else_nan(self.normals, v_n_pairs[key], target_shape=(3,))\n",
    "                         for key in range(self.vertices.shape[0])])\n",
    "\n",
    "    @property\n",
    "    def vertex_textures(self):\n",
    "        \"\"\"Get array of vertex texture coordinates. If multiple textures per vertex are stored,\n",
    "        the last one is returned\"\"\"\n",
    "        if self.only_vertices:\n",
    "            return np.nan*np.ones_like(self.vertices)\n",
    "        v_vt_pairs = {v: np.nan for v in range(self.vertices.shape[0])}\n",
    "        for v, vt, _ in flatten(self.faces, max_depth=1):\n",
    "            v_vt_pairs[v] = vt\n",
    "        return np.array([index_else_nan(self.texture_vertices, v_vt_pairs[key], target_shape=(2,))\n",
    "                         for key in range(self.vertices.shape[0])])\n",
    "    \n",
    "    def match_vertex_info(self, use_vertex_normals=True, require_texture_normals=True):\n",
    "        \"\"\"\n",
    "        Match up 3d vertex coordinates / texture coordinates / normals based on face connectivity.\n",
    "        \n",
    "        Sets attributes matched_vertices, matched_texture_vertices, matched_normals.\n",
    "        These attributes can then be used for interpolating volumetric data onto the UV square.\n",
    "        The resulting arrays will _not_ match the face indices, since there can be more texture vertices\n",
    "        than vertices, and will have a shape given by the number of unique vertex/texture vertex/normal\n",
    "        pairs (note: this is much lower than the number of faces, which is numerically favorable)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        use_vertex_normals : bool, default True\n",
    "            Use vertex normals for normal info instead of face normals. Will result in arrays\n",
    "            of size O(n_vertices). Else, you get an arra of size O(n_triangles).\n",
    "        require_texture_normals : bool, default True\n",
    "            If True, discard vertices for which texture or normal info does not exist. \n",
    "            If False, attributes that do not exist for a given vertex are set to np.nan.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        assert not self.only_vertices, \"\"\"Method requires texture or normal information\"\"\"\n",
    "        if not use_vertex_normals:\n",
    "            unique_v_vt_n_pairs = unique([tuple(x) for x in flatten(self.faces, max_depth=1)])\n",
    "            matched_vertices = np.array([index_else_nan(self.normals, pair[0], target_shape=(3,))\n",
    "                                         for pair in unique_v_vt_n_pairs])\n",
    "            matched_texture_vertices = np.array([index_else_nan(self.texture_vertices, pair[1], target_shape=(2,))\n",
    "                                                 for pair in unique_v_vt_n_pairs])\n",
    "            matched_normals = np.array([index_else_nan(self.normals, pair[2], target_shape=(3,))\n",
    "                                        for pair in unique_v_vt_n_pairs])\n",
    "        else:\n",
    "            unique_v_vt_pairs = unique([tuple(x[:2]) for x in flatten(self.faces, max_depth=1)])\n",
    "            matched_vertices = np.array([index_else_nan(self.normals, pair[0], target_shape=(3,))\n",
    "                                         for pair in unique_v_vt_pairs])\n",
    "            matched_texture_vertices = np.array([index_else_nan(self.texture_vertices, pair[1], target_shape=(2,))\n",
    "                                                 for pair in unique_v_vt_pairs])\n",
    "            normals = self.vertex_normals\n",
    "            matched_normals = np.array([index_else_nan(normals, pair[0], target_shape=(3,))\n",
    "                                        for pair in unique_v_vt_pairs])\n",
    "        if require_texture_normals:\n",
    "            mask = ~np.isnan(matched_texture_vertices).any(axis=1) & ~np.isnan(matched_normals).any(axis=1)\n",
    "            matched_vertices = matched_vertices[mask,:]\n",
    "            matched_texture_vertices = matched_texture_vertices[mask,:]\n",
    "            matched_normals = matched_normals[mask,:]            \n",
    "        self.matched_vertices = matched_vertices\n",
    "        self.matched_texture_vertices = matched_texture_vertices\n",
    "        self.matched_normals = matched_normals\n",
    "        return None\n",
    "\n",
    "    def apply_affine_to_mesh(self, trafo, update_matched_data=True):\n",
    "        \"\"\"\n",
    "        Apply affine transformation to mesh.\n",
    "        \n",
    "        Rotate/shear and translate vertices, rotate/shear and renormalize normals,\n",
    "        flip faces if transformation determinant is -1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trafo : np.array of shape (4,4) or (3,3)\n",
    "            Transformation matrix. If (4,4), it is interpreted as affine transformation.\n",
    "        update_matched_data : bool, default True\n",
    "            Update matched data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        newmesh : ObjMesh\n",
    "            Transformed mesh.\n",
    "\n",
    "        \"\"\"\n",
    "        assert trafo.shape==(3,3) or trafo.shape==(4,4), \"Transformation matrix must be 3*3 or 4*4\"\n",
    "        if trafo.shape == (3,3):\n",
    "            trafo_matrix, trafo_translate = (trafo, np.zeros(3))\n",
    "        elif trafo.shape == (4,4):\n",
    "            trafo_matrix, trafo_translate = (trafo[:3,:3], trafo[:3,-1])\n",
    "        newmesh = deepcopy(self)\n",
    "        newmesh.vertices = self.vertices@trafo_matrix.T + trafo_translate\n",
    "        if self.normals is not None:\n",
    "            normals_transformed = self.normals@trafo_matrix.T\n",
    "            newmesh.normals = (normals_transformed.T / np.linalg.norm(normals_transformed, axis=-1)).T\n",
    "        if np.linalg.det(trafo_matrix) < 0:\n",
    "            newmesh.faces = [fc[::-1] for fc in self.faces]\n",
    "        if update_matched_data and not self.only_vertices:\n",
    "            newmesh.match_vertex_info()\n",
    "        return newmesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c4711d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = ObjMesh.read_obj(\"drosophila_example/Drosophila_CAAX-mCherry_mesh_uv.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "446cbacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8160, 3), (8288, 2), 16294)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.vertices.shape, mesh.texture_vertices.shape, len(mesh.faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cdf6a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.match_vertex_info(use_vertex_normals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a38337c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.match_vertex_info(use_vertex_normals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f11342b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8288, 3), (8288, 3))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.matched_normals.shape, mesh.matched_vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "898b5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this on an example\n",
    "mesh_fname_ref = \"registration_example/Drosophila_reference_preregistered.obj\"\n",
    "mesh_fname_data = \"registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\n",
    "\n",
    "mesh = ObjMesh.read_obj(mesh_fname_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "98797253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679 ms ± 8.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mesh.match_vertex_info(require_texture_normals=False, use_vertex_normals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5541aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a 3d mesh of using the marching cubes method\n",
    "\n",
    "level_set = mcubes.smooth(segmentation, method=\"gaussian\") # converts segmentation into level set, with 0=surface\n",
    "vertices, faces = mcubes.marching_cubes(level_set, 0)\n",
    "\n",
    "# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\n",
    "vertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n",
    "                                 /np.array(metadata_dict['subsampling_factors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4f42b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = ObjMesh(vertices_in_microns, faces)\n",
    "mesh.name = \"basics_example_mesh_marching_cubes\"\n",
    "mesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5c867075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.only_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2e6b37d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 ms ± 1.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "912a10af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 ms ± 1.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8bb46ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 863 ms, sys: 43.9 ms, total: 907 ms\n",
      "Wall time: 905 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## try a somewhat larger mesh with texture info etc\n",
    "\n",
    "mesh = ObjMesh.read_obj(\"registration_example/Drosophila_reference.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c90e49ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 23.9 ms, total: 1.12 s\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mesh.write_obj(\"registration_example/Drosophila_reference_test_write.obj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf752cb2",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "We can now save the cartographic projections both as `.tif` stack for quantitative analysis, and as `.png`'s for visualization as mesh texture in blender. We will also save the metadata to a `.json` file\n",
    "\n",
    "Annoyingly, we have to normalize our data and convert it to 8-bit to save it as png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce60c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def save_dict_to_json(filename, dictionary):\n",
    "    \"\"\"\n",
    "    Save dictionary to .json file.\n",
    "    \n",
    "    Will automatically convert numpy arrays to lists for saving. If you get an error like \"XXX is not JSON\n",
    "    serializable\", you need to ensure all your dictionary items are things that can be saved to text by json\n",
    "    (strings, numbers, lists).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Filename to save to\n",
    "    dictionary : dict\n",
    "        Dictionary to save\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    serializable_dictionary = {key: val.tolist() if isinstance(val, np.ndarray) else val\n",
    "                               for key, val in dictionary.items()}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(serializable_dictionary, f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823262d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata\n",
    "save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70038a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def save_for_imageJ(filename, image, z_axis=None, channel_axis=None):\n",
    "    \"\"\"\n",
    "    Save image as 32bit ImageJ compatible .tif file\n",
    "    \n",
    "    If channel_axis is not provided, it is inferred as the shortest axis.\n",
    "    If z-axis is provided for a 4d array, it will be set as the default z-axis for ImageJ.\n",
    "    \"\"\"\n",
    "    channel_axis = np.argmin(image.shape) if channel_axis is None else channel_axis\n",
    "    if len(image.shape) == 3:\n",
    "        transposed_image = np.moveaxis(image, channel_axis, 0)\n",
    "        tifffile.imwrite(filename, transposed_image.astype(np.float32),\n",
    "                         metadata={'axes': 'CYX'}, imagej=True,)\n",
    "    elif len(image.shape) == 4:\n",
    "        if z_axis is not None:\n",
    "            transposed_image = np.moveaxis(image, (z_axis, channel_axis), (0,1))\n",
    "        else:\n",
    "            transposed_image = np.moveaxis(image, channel_axis, 1)\n",
    "        tifffile.imwrite(filename, transposed_image.astype(np.float32),\n",
    "                         metadata={'axes': 'ZCYX'}, imagej=True,)\n",
    "    return None\n",
    "    \n",
    "def normalize_quantiles_for_png(image, quantiles=(0.01, 0.99)):\n",
    "    \"\"\"\n",
    "    Normalize an image by setting given quantiles to 0 and 255 and converting to 8-bit, for saving as .png\n",
    "    \n",
    "    Also replaces nan by 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.array\n",
    "        Image (should be single-channel)\n",
    "    quantiles : tuple\n",
    "        Image quantile to set to 0 and 255.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image_normalized : np.array\n",
    "        Normalized image, datatype np.uint8\n",
    "    \"\"\"\n",
    "    image_normalized = image - np.nanquantile(image, quantiles[0])\n",
    "    image_normalized /= np.nanquantile(image_normalized, quantiles[1])\n",
    "    image_normalized = np.nan_to_num(np.round(np.clip(255*image_normalized, 0, 255)), nan=0)\n",
    "    return image_normalized.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "def save_stack_for_blender(image, directory, normalization=(0.01, 0.99)):\n",
    "    \"\"\"\n",
    "    Save multichannel volumetric image as series of grayscale .png images. Can normalize data if desired.\n",
    "    \n",
    "    This function necessarily converts the image to 8bit. Use a suitable normalization to ensure nothing \n",
    "    is lost.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : 4d np.array\n",
    "        Axis 0 is assumed to be the channel axis, axis 1 is the slicing axes, i.e. images will correspond to\n",
    "        slices along axis 1.\n",
    "    directory : str\n",
    "        Path to save data to. Will create directory if it doesn't exist\n",
    "    normalization : tuple of float, or callable\n",
    "        Whether to normalize the image before saving it. If None, no normalization is performed. If a\n",
    "        tuple is given, it will be interpreted as quantiles to set to 0 and 255, respectively (over the\n",
    "        whole channel, not each slice). If a callable is provided, it will be applied to each channel.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    directory = directory.removesuffix('/')\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for ic, ch in enumerate(image):\n",
    "        if callable(normalization):\n",
    "            ch_normalized = normalization(ch)\n",
    "        if isinstance(normalization, tuple):\n",
    "            ch_normalized = normalize_quantiles_for_png(ch, quantiles=normalization)\n",
    "        for islc, slc in enumerate(ch_normalized):\n",
    "            slc = np.stack(3*[slc], axis=-1).astype(np.uint8) # necessary for saving as png\n",
    "            imsave(f'{directory}/channel_{ic}_slice_{str(islc).zfill(3)}.png', slc, check_contrast=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (2, 5, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# read the data so we can check the saving function\n",
    "projected_data = adjust_axis_order(imread(f\"{metadata_dict['filename']}_projected.tif\"))\n",
    "print(\"Image shape\", projected_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data, z_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\n",
    "save_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
