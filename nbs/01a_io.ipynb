{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54aa0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01ea96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from skimage import transform\n",
    "from skimage.io import imread, imsave\n",
    "import h5py\n",
    "from typing import Iterable\n",
    "import tifffile\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import functools\n",
    "\n",
    "import igl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67c0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9411f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5514327",
   "metadata": {},
   "source": [
    "## Basic I/O\n",
    "\n",
    "In this notebook, we define a number of functions for image, mesh, and metadata loading and saving, and show how to use the with the data from the ``basics_example`` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586f315",
   "metadata": {},
   "source": [
    "## Load and subsample data for segmentation\n",
    "\n",
    "Let's load the dataset. We then enter the relevant metadata - the filename, resolution in microns, and how much we want to subsample for segmentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c7e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def adjust_axis_order(image, channel_axis=None):\n",
    "    \"\"\"\n",
    "    Adjust axis order of image (numpy array) so that the channel axis is axis 0. \n",
    "    \n",
    "    If channel axis is not specified, it is infered as the axis with the smallest number of entries.\n",
    "    If the image contains a single channel, this function adds a singleton dimension.\n",
    "    Axis order is otherwise left unchanged. Image must have 3 axes (single channel volumetric)\n",
    "    or four axes (multichannel volumetric). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image: np.ndarray with 3 or 4 axes\n",
    "        Input image.\n",
    "    channel_axis: int or None, optional\n",
    "        Channel axis\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transposed image: np.ndarray with 4 axes\n",
    "        Input image, channel now axis 0.\n",
    "    \"\"\"\n",
    "    assert 2 < len(image.shape) <5 , \"image must have 3 or 4 axes\"\n",
    "    if len(image.shape) == 3:\n",
    "        return image[np.newaxis]\n",
    "    if channel_axis is None:\n",
    "        channel_axis = np.argmin(image.shape)\n",
    "    return np.moveaxis(image, channel_axis, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b9f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2346a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = {'filename': 'basics_example/basics_example',\n",
    "                 'resolution_in_microns': (1, 0.36, 0.36), # you can typically get this from the .tif metadata\n",
    "                 'subsampling_factors': (1, 1/3, 1/3),\n",
    "                 'normal_offsets':np.linspace(-2, 2, 5) # normal offsets for map projection, in microns\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1fa308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (2, 26, 454, 511)\n"
     ]
    }
   ],
   "source": [
    "image = adjust_axis_order(imread(f\"{metadata_dict['filename']}.tif\"))\n",
    "print(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47d1720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def subsample_image(image, subsampling_factors, use_block_averaging_if_possible=True):\n",
    "    \"\"\"\n",
    "    Subsample (downscale) image by given factors.\n",
    "    \n",
    "    Reduce image size by given factors along each dimension. The subsampling_factors\n",
    "    need to be _smaller than 1_. If the image is large,  subsampling can be performed\n",
    "    by block averaging, which is a lot faster. In this case, you need to use inverse\n",
    "    integer rescaling factors (e.g. 1/2, 1/3). If the number of pixels is not divisible\n",
    "    by those factors, the subsampled image is padded by 0.\n",
    "    \n",
    "    Important: the chanel axis must be axis 0 (automatically done by adjust_axis_order)! \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image: np.ndarray\n",
    "        Multichannel input image. Channel axis must be axis 0. (automatically done by adjust_axis_order).\n",
    "    subsampling_factors : list or tuple of float or int\n",
    "        Subsampling factors along each axis. A factor 1/2 will reduce image size by 2x along that axis.\n",
    "    use_block_averaging_if_possible : bool, default True\n",
    "        Use fast block averaging if subsampling_factors are inverses of integers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subsampled_image: np.ndarray\n",
    "        Subsampled imaged\n",
    "    \"\"\"\n",
    "    inverse_factors_are_integer = all([np.round(1/x, decimals=5) == int(1/x) for x in subsampling_factors])\n",
    "    if inverse_factors_are_integer and use_block_averaging_if_possible:\n",
    "        inverse_factors = tuple([int(1/x) for x in subsampling_factors])\n",
    "        return np.stack([transform.downscale_local_mean(chn, inverse_factors) for chn in image], axis=0)\n",
    "    return np.stack([transform.rescale(chn, subsampling_factors, preserve_range=True) for chn in image], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1981c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsampled image shape: (2, 26, 152, 171)\n"
     ]
    }
   ],
   "source": [
    "subsampled_image = subsample_image(image, metadata_dict['subsampling_factors'],\n",
    "                                  use_block_averaging_if_possible=False)\n",
    "print(\"subsampled image shape:\", subsampled_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901857d3",
   "metadata": {},
   "source": [
    "## Create 3d segmentation\n",
    "\n",
    "Now create a 3d segmentation, in this case using ilatik. ilastik works best with input saved as `.h5` data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a19a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def normalize_quantiles_zstack(image, quantiles=(0.01, 0.99)):\n",
    "    \"\"\"\n",
    "    Normalize an image by setting given quantiles to 0 and 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.array\n",
    "        Multi-dimensional image. First axis needs to be the channel axis.\n",
    "    quantiles : tuple\n",
    "        Image quantile to set to 0 and 1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image_normalized : np.array\n",
    "        Normalized image\n",
    "    \"\"\"\n",
    "    image_normalized = image - np.nanquantile(image, quantiles[0])\n",
    "    image_normalized /= np.nanquantile(image_normalized, quantiles[1])\n",
    "    image_normalized = np.nan_to_num(image_normalized)\n",
    "    return image_normalized.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51eb13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def write_h5(filename, image, h5_dataset_name=\"image\"):\n",
    "    \"\"\"Write image (numpy array) as .h5 file (e.g. as input for ilastik).\"\"\"\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset('image', data=image)\n",
    "    return None\n",
    "\n",
    "def read_h5(filename):\n",
    "    \"\"\"Read .h5 file (e.g. ilastik output) into numpy array. Loads alphabetically first entry in .h5.\"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        arr = f[sorted(f.keys())[0]][()] \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c33c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now save the subsampled image a .h5 file for input into ilastik for segmentation\n",
    "\n",
    "write_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16e77650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation shape: (26, 151, 170)\n"
     ]
    }
   ],
   "source": [
    "# after creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n",
    "\n",
    "segmentation = read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\n",
    "segmentation = segmentation[0] # select the first channel of the segmentation - it's the probablity a pixel\n",
    "                               # is part of the sample\n",
    "print(\"segmentation shape:\", segmentation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba35599c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa7f19cec10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the segmentation in a cross section\n",
    "\n",
    "plt.imshow(segmentation[:,:,50], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c012870",
   "metadata": {},
   "source": [
    "## Meshing\n",
    "\n",
    "We convert the segmentation into a triangular mesh using the marching cubes method, and save the mesh. We introduce a couple of functions for mesh io and mesh handling. We save all meshes in as wavefront `.obj` files (see [wikipedia](https://en.wikipedia.org/wiki/Wavefront_.obj_file)). In python, we represent missing entries (such as a vertex which doesn't have a normal by `np.nan`.\n",
    "\n",
    "**Important convention** For sanities sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n",
    "\n",
    "For reading, writing, holding, and handling `.obj` meshes, we will create a `ObjMesh` class. Basically, you can think of an `.obj` mesh as a _map_ between two meshes (the mesh in 3d, and its texture coordinates in 2d), defined on a per-face basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825202cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def flatten(lst, max_depth=1000, iter_count=0):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists into a list.\n",
    "\n",
    "    Also works with inhomogeneous lists, e.g., [[0,1],2]. The argument\n",
    "    depth determines how \"deep\" to flatten the list, e.g. with max_depth=1:\n",
    "    [[(1,0), (1,0)]] -> [(1,0), (1,0)].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lst : list\n",
    "        list-of-lists.\n",
    "    max_depth : int, optional\n",
    "        To what depth to flatten the list.\n",
    "    iter_count : int, optional\n",
    "        Helper argument for recursion depth determination.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iterator\n",
    "        flattened list.\n",
    "    \"\"\"\n",
    "    for el in lst:\n",
    "        if (isinstance(el, Iterable) and not isinstance(el, (str, bytes))\n",
    "                and iter_count < max_depth):\n",
    "            yield from flatten(el, max_depth=max_depth,\n",
    "                               iter_count=iter_count+1)\n",
    "        else:\n",
    "            yield el\n",
    "            \n",
    "def pad_list(lst, length=3, fill_value=np.nan):\n",
    "    \"\"\"Pad end of list with fill_value if shorter than desired length.\"\"\"\n",
    "    return lst + max([0, (length-len(lst))]) * [fill_value,]\n",
    "\n",
    "def unique(sequence):\n",
    "    \"\"\"Create list of unique entries in sequence while preserving order\"\"\"\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def index_else_nan(arr, inds):\n",
    "    \"\"\"Return arr[inds], masked so that the result is np.nan wherever ind is nan\"\"\"\n",
    "    if len(np.shape(inds)):\n",
    "        inds = np.array(inds)\n",
    "    mask = np.isnan(inds)\n",
    "    masked_inds = np.copy(inds)\n",
    "    masked_inds[mask] = 0\n",
    "    masked_inds = masked_inds.astype(int)    \n",
    "    selected = arr[masked_inds]\n",
    "    selected[mask] = np.nan\n",
    "    return selected\n",
    "\n",
    "def invert_dictionary(my_map, assume_unique=False):\n",
    "    \"\"\"\n",
    "    Invert key -> value map defined by a dictionary\n",
    "    \n",
    "    If assume_unique is True, key/value pairs are assumed to be unique.\n",
    "    Else, a dictionary of list returned. Each entry is a list\n",
    "    of keys that map to the given value.\n",
    "    \"\"\"\n",
    "    if assume_unique:\n",
    "        return {v: k for k, v in my_map.items()}\n",
    "    inv_map = {}\n",
    "    for k, v in my_map.items():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k]\n",
    "    return inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0364d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ObjMesh:\n",
    "    \"\"\"\n",
    "    Simple class for reading, holding, transforming, and saving 3d polygonal meshes in the .obj format.\n",
    "    See https://en.wikipedia.org/wiki/Wavefront_.obj_file.\n",
    "    Attributes\n",
    "        - vertices = [(x_0, y_0, z_0), ... ]\n",
    "        - texture_vertices = [(u_0, v_0), ...] or None\n",
    "        - normals = [(nx_0, ny_0, nz_0), ...] or None\n",
    "        - faces = [f0, ...]\n",
    "        - only_vertices = bool. \n",
    "    vertices, texture_vertices, normals are np.arrays, faces is a list of lists.\n",
    "    Each face is either a list of vertex indices (if only_vertices is True), or, if the mesh\n",
    "    has texture nformation, a list of vertex/normal index pairs. \n",
    "    Normals are always defined per-vertex, i.e. self.normals[i] is the normal vector at\n",
    "    self.vertices[i].   Missing data is represented by np.nan.\n",
    "    Faces can be any length (triangles, quads, ...). Indices start at 0!\n",
    "    \n",
    "    The method match_vertex_info can be used to match up vertices and normals to\n",
    "    texture vertices based on the face connectivity as base points for interpolation.\n",
    "    This sets the following attributes:\n",
    "        - matched_vertices\n",
    "        - matched_normals\n",
    "    as np.array's\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vertices, faces, texture_vertices=None, normals=None, name=None):\n",
    "        self.vertices, self.faces = (vertices, faces)\n",
    "        self.texture_vertices, self.normals = (texture_vertices, normals)\n",
    "        self.name = None\n",
    "\n",
    "    @staticmethod\n",
    "    def read_obj(filename):\n",
    "        \"\"\"\n",
    "        Return vertices, texture vertices, normals, and faces from an obj file.\n",
    "\n",
    "        Faces are lists of pairs vertex/texture vertex. If a certain vertex has no texture \n",
    "        associated to it, the entry is np.nan, else it is an index into the vertex/texture arrays\n",
    "        (note: indices of returned faces start at 0!). See https://en.wikipedia.org/wiki/Wavefront_.obj_file.\n",
    "        \n",
    "        Intended for .obj files containing a single object only. Will emit a warning if multiple objects\n",
    "        are detected.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            filename\n",
    "        Returns\n",
    "        -------\n",
    "        mesh: ObjMesh\n",
    "        \"\"\"\n",
    "        def _str_to_int_or_nan(x):\n",
    "            \"\"\"Convert string to int or np.nan if string is empty\"\"\"\n",
    "            if x == '':\n",
    "                return np.nan\n",
    "            return int(x)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        names = [ln.split()[1:] for ln in lines if ln.startswith(\"o \")]\n",
    "        if len(names) > 1:\n",
    "            warnings.warn(f\"Warning: multiple meshes in .obj file\", RuntimeWarning)\n",
    "        name = None if len(names) == 0 else names[0]\n",
    "        vs = np.array([ln.split()[1:] for ln in lines if ln.startswith(\"v \")]).astype(float)\n",
    "        vts = np.array([ln.split()[1:] for ln in lines if ln.startswith(\"vt \")]).astype(float)\n",
    "        ns = np.array([ln.split()[1:] for ln in lines if ln.startswith(\"vn \")]).astype(float)\n",
    "        fs = [ln.split()[1:] for ln in lines if ln.startswith(\"f \")]\n",
    "        fs = [[pad_list([_str_to_int_or_nan(y)-1 for y in x.split(\"/\")], length=3, fill_value=np.nan)\n",
    "               for x in f] for f in fs]\n",
    "        if vts.shape == (0,) and ns.shape == (0,): # if there is no texture information\n",
    "            fs = [[v[0] for v in f] for f in fs]\n",
    "            mesh = ObjMesh(vs, fs, texture_vertices=None, normals=None, name=name)\n",
    "        else: \n",
    "            # match up normals to vertices\n",
    "            v_n_pairs = [np.nan for v in range(vs.shape[0])]\n",
    "            for v, _, n in flatten(fs, max_depth=1):\n",
    "                v_n_pairs[v] = n\n",
    "            ns = index_else_nan(ns, np.array(v_n_pairs))\n",
    "            ns = (ns.T/np.linalg.norm(ns, axis=1)).T\n",
    "            fs = [[v[:2] for v in f] for f in fs]\n",
    "            mesh = ObjMesh(vs, fs, texture_vertices=vts, normals=ns, name=name)\n",
    "        return mesh\n",
    "        \n",
    "    def write_obj(self, filename, include_uv_and_normals=True):\n",
    "        \"\"\"\n",
    "        Write mesh to .obj format.\n",
    "\n",
    "        Can write texture coordinates and normals if included. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            filename to save to\n",
    "        include_uv_and_normals : bool, default True\n",
    "            include uv and normal information if available.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        def _int_or_nan_to_str(x):\n",
    "            \"\"\"Convert int/nan to string. np.nan is converted to empty string\"\"\"\n",
    "            if np.isnan(x):\n",
    "                return ''\n",
    "            return str(x)\n",
    "        namelines = [\"o {}\\n\".format(self.name)] if self.name is not None else []\n",
    "        if self.only_vertices:\n",
    "            vlines = [\"v {} {} {}\\n\".format(*v) for v in self.vertices]\n",
    "            flines = [\"f {} {} {}\\n\".format(*[int(v+1) for v in fc]) for fc in self.faces]\n",
    "            with open(filename, 'w') as f:\n",
    "                f.writelines(namelines)\n",
    "                f.writelines(vlines)\n",
    "                f.writelines(flines)\n",
    "        if not self.only_vertices and not include_uv_and_normals:\n",
    "            vlines = [\"v {} {} {}\\n\".format(*v) for v in self.vertices]\n",
    "            flines = [\"f {} {} {}\\n\".format(*[int(v[0]+1) for v in fc]) for fc in self.faces]\n",
    "            with open(filename, 'w') as f:\n",
    "                f.writelines(namelines)\n",
    "                f.writelines(vlines)\n",
    "                f.writelines(flines)\n",
    "        if not self.only_vertices and include_uv_and_normals:\n",
    "            assert all([len(v)==2 for v in flatten(self.faces, max_depth=1)]), \"each vertex must have 2 indices\"\n",
    "            texture_vertices = [] if self.texture_vertices is None else self.texture_vertices\n",
    "            vlines = [\"v {} {} {}\\n\".format(*v) for v in self.vertices]\n",
    "            vtlines = [\"vt {} {}\\n\".format(*vt) for vt in texture_vertices]\n",
    "            nlines = [\"vn {} {} {}\\n\".format(*n) for n in self.normals] if self.normals is not None else []\n",
    "            faces_with_normals = [[[v[0], v[1], v[0]] for v in fc] for fc in self.faces]\n",
    "            flines = [\"f {} {} {}\\n\".format(*[\"{}/{}/{}\".format(*[_int_or_nan_to_str(ix+1) for ix in v])\n",
    "                                              for v in fc]) for fc in faces_with_normals]\n",
    "            with open(filename, 'w') as f:\n",
    "                f.writelines(namelines)\n",
    "                f.writelines(vlines)\n",
    "                f.writelines(vtlines)\n",
    "                f.writelines(nlines)\n",
    "                f.writelines(flines)\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def only_vertices(self):\n",
    "        if self.texture_vertices is None:\n",
    "            assert all([not isinstance(v, Iterable) for v in flatten(self.faces, max_depth=1)]), \\\n",
    "                \"If texture_vertices is None, faces must be lists of vertex indices only\"\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def is_triangular(self):\n",
    "        \"\"\"Checks if mesh has triangular faces only.\"\"\"\n",
    "        return all([len(fc)==3 for fc in self.faces]) \n",
    "            \n",
    "    @property\n",
    "    def tris(self):\n",
    "        \"\"\"Get all 3d triangles in mesh as a numpy array. Entries are vertex indices.\"\"\"\n",
    "        if self.only_vertices:\n",
    "            return np.array([fc for fc in self.faces if len(fc)==3])\n",
    "        return np.array([[v[0] for v in fc] for fc in self.faces if len(fc)==3])\n",
    "    \n",
    "    @property\n",
    "    def texture_tris(self):\n",
    "        \"\"\"Get all texture triangles in mesh as a numpy array. Entries are texture_vertex indices.\"\"\"\n",
    "        if self.only_vertices:\n",
    "            return np.array([np.nan for fc in self.faces if len(fc)==3])\n",
    "        return np.array([[v[1] for v in fc] for fc in self.faces if len(fc)==3])\n",
    "    \n",
    "    @property\n",
    "    def vertex_textures(self):\n",
    "        \"\"\"Get array of vertex texture coordinates. If multiple textures per vertex are stored,\n",
    "        the last one is returned.\"\"\"\n",
    "        if self.only_vertices:\n",
    "            return np.nan*np.ones_like(self.vertices)[:,:2]\n",
    "        if len(self.texture_vertices) == 0:\n",
    "            return np.nan*np.ones_like(self.vertices)[:,:2]\n",
    "        v_vt_pairs = [np.nan for v in range(self.vertices.shape[0])] # in case there are stray vertices w/out face\n",
    "        for v, vt in flatten(self.faces, max_depth=1):\n",
    "            v_vt_pairs[v] = vt\n",
    "        return index_else_nan(self.texture_vertices, np.array(v_vt_pairs))\n",
    "    \n",
    "    def set_normals(self):\n",
    "        \"\"\"Recompute normals based on 3d positions. Only works for triangular meshes.\"\"\"\n",
    "        if not self.is_triangular:\n",
    "            warnings.warn(f\"Warning: mesh not triangular - normals may be incorrect\", RuntimeWarning)\n",
    "        normals = igl.per_vertex_normals(self.vertices, self.tris,)\n",
    "        self.normals = (normals.T / np.linalg.norm(normals, axis=1)).T\n",
    "        return None\n",
    "        \n",
    "    def get_uv_index_to_vertex_index_map(self):\n",
    "        \"\"\"Get map from texture vertex index to the corresponding 3d vertex index as a dictionary.\"\"\"\n",
    "        return {v[1]: v[0] for v in flatten(self.faces, max_depth=1) if not np.isnan(v[1])}\n",
    "    \n",
    "    def get_vertex_index_to_uv_index_map(self):\n",
    "        \"\"\"\n",
    "        Get map from 3d vertex index to the corresponding UV vertex index as a dictionary.\n",
    "        \n",
    "        Note: each dict item is a list, since a 3d vertex can map to multiple UV vertices.\n",
    "        \"\"\"\n",
    "        uv_to_vertex = self.get_uv_index_to_vertex_index_map()\n",
    "        return invert_dictionary(uv_to_vertex, assume_unique=False)\n",
    "\n",
    "        \n",
    "    def get_uv_matched_vertex_indices(self):\n",
    "        \"\"\"\n",
    "        Get an array of indices into 3d vertices that map them to the corresponding texture vertices.\n",
    "        \n",
    "        Useful for translating per-vertex data into per-texture-vertex data.\n",
    "        \"\"\"\n",
    "        texture_vertex_dict = self.get_uv_index_to_vertex_index_map()\n",
    "        return np.array([texture_vertex_dict[i] for i in range(self.texture_vertices.shape[0])])\n",
    "        \n",
    "    def match_vertex_info(self):\n",
    "        \"\"\"\n",
    "        Match up 3d vertex coordinates and normals to texture vertices based on face connectivity.\n",
    "        \n",
    "        Compute matched_vertices and matched_normals, which are the 3d vertices and normals\n",
    "        corresponding to each texture vertex. The resulting arrays have the shape\n",
    "        (self.texture_vertices.shape[0], 3). If normal information does not exist for a given\n",
    "        texture vertex, the entry is set to np.nan.\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        assert not self.only_vertices and len(self.normals) > 0 and len(self.texture_vertices) > 0, \\\n",
    "            \"\"\"Method requires texture or normal information\"\"\"\n",
    "        matched_vertex_inds = self.get_uv_matched_vertex_indices()\n",
    "        self.matched_vertices = self.vertices[matched_vertex_inds]\n",
    "        self.matched_normals = index_else_nan(self.normals, matched_vertex_inds)\n",
    "        return None\n",
    "\n",
    "    def cut_along_seams(self):\n",
    "        \"\"\"\n",
    "        Cut mesh along texture seams.\n",
    "\n",
    "        Returns a new ObjMesh in which the topology of the vertices matches the topology of the texture vertices,\n",
    "        by duplicating vertices along \"seams\" (i.e. which have multiple corresponding texture vertices),\n",
    "        and discarding any vertices without texture coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ObjMesh\n",
    "\n",
    "        \"\"\"\n",
    "        assert not self.only_vertices and len(self.normals) > 0 and len(self.texture_vertices) > 0, \\\n",
    "            \"\"\"Method requires texture or normal information\"\"\"\n",
    "        texture_vertex_dict = {v[1]: v[0] for v in flatten(self.faces, max_depth=1) if not np.isnan(v[1])}\n",
    "        matched_vertex_inds = np.array([texture_vertex_dict[i] for i in range(self.texture_vertices.shape[0])])\n",
    "        matched_vertices = self.vertices[matched_vertex_inds]\n",
    "        matched_normals = index_else_nan(self.normals, matched_vertex_inds)\n",
    "        cut_faces = [[[v[1], v[1]] for v in fc] for fc in self.faces if not any([np.isnan(v[1]) for v in fc])]\n",
    "        return ObjMesh(matched_vertices, cut_faces, texture_vertices=self.texture_vertices,\n",
    "                       normals=matched_normals, name=self.name)\n",
    "        \n",
    "    def apply_affine_to_mesh(self, trafo, update_matched_data=True):\n",
    "        \"\"\"\n",
    "        Apply affine transformation to mesh.\n",
    "        \n",
    "        Rotate/shear and translate vertices, rotate/shear and renormalize normals,\n",
    "        flip faces if transformation determinant is -1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trafo : np.array of shape (4,4) or (3,3)\n",
    "            Transformation matrix. If (4,4), it is interpreted as affine transformation.\n",
    "        update_matched_data : bool, default True\n",
    "            Update matched data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        newmesh : ObjMesh\n",
    "            Transformed mesh.\n",
    "\n",
    "        \"\"\"\n",
    "        assert trafo.shape==(3,3) or trafo.shape==(4,4), \"Transformation matrix must be 3*3 or 4*4\"\n",
    "        if trafo.shape == (3,3):\n",
    "            trafo_matrix, trafo_translate = (trafo, np.zeros(3))\n",
    "        elif trafo.shape == (4,4):\n",
    "            trafo_matrix, trafo_translate = (trafo[:3,:3], trafo[:3,-1])\n",
    "        newmesh = deepcopy(self)\n",
    "        newmesh.vertices = self.vertices@trafo_matrix.T + trafo_translate\n",
    "        if self.normals is not None:\n",
    "            normals_transformed = self.normals@trafo_matrix.T\n",
    "            newmesh.normals = (normals_transformed.T / np.linalg.norm(normals_transformed, axis=-1)).T\n",
    "        if np.linalg.det(trafo_matrix) < 0:\n",
    "            newmesh.faces = [fc[::-1] for fc in self.faces]\n",
    "        if update_matched_data and not self.only_vertices:\n",
    "            newmesh.match_vertex_info()\n",
    "        return newmesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b075367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_other_formats_without_uv(filename):\n",
    "    \"\"\"\n",
    "    Return vertices and faces from a non-.obj mesh file format. file.\n",
    "\n",
    "    Supported formats are: obj, off, stl, wrl, ply, mesh.\n",
    "    Will NOT read in normals or texture coordinates. If you have texture\n",
    "    coordinates, save your mesh as .obj. Will only return triangular faces.\n",
    "\n",
    "    See https://libigl.github.io/libigl-python-bindings/igl_docs/#read_triangle_mesh.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        filename\n",
    "    Returns\n",
    "    -------\n",
    "    mesh: ObjMesh\n",
    "        Only contains face and vertex info.\n",
    "    \"\"\"\n",
    "    vs, fs = igl.read_triangle_mesh(filename)\n",
    "    ns = igl.per_vertex_normals(vs, fs)\n",
    "    return ObjMesh(vs, fs, texture_vertices=None, normals=ns, name=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9fdb2",
   "metadata": {},
   "source": [
    "### Test read/write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c97963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: readOBJ() ignored non-comment line 4:\n",
      "  o Cube\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ObjMesh at 0x7f9950e3cd50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_other_formats_without_uv(\"wrapping_example/mixed_faces_example.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41aca85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 1.8 ms, total: 198 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mesh = ObjMesh.read_obj(\"drosophila_example/Drosophila_CAAX-mCherry_mesh_uv.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a4d41e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8160, 3), (8160, 3), (8288, 2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.vertices.shape, mesh.normals.shape, mesh.texture_vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76852baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16ec5a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99695517, 0.99865324, 0.98954003, ..., 0.99875341, 0.99958208,\n",
       "              nan])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('vi,vi->v', mesh.normals, igl.per_vertex_normals(mesh.vertices, mesh.tris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a66c2818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8288, 3), (8288, 3))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.match_vertex_info()\n",
    "mesh.matched_normals.shape, mesh.matched_vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22082112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this on an example\n",
    "mesh_fname_ref = \"registration_example/Drosophila_reference_preregistered.obj\"\n",
    "mesh_fname_data = \"registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\n",
    "\n",
    "mesh = ObjMesh.read_obj(mesh_fname_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b0e3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 ms ± 2.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mesh.match_vertex_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b275a68",
   "metadata": {},
   "source": [
    "## Mesh cutting and gluing\n",
    "\n",
    "The `cut_along_seams` method can be used to \"cut\" a mesh along its UV seams. This duplicates 3d vertices so that the 3d topology and the unwrapped UV topololy match. For example, if your UV map cuts your sphere mesh into a north and a south hemisphere for unwrapping, `cut_along_seams` will cut the mesh into two halves.\n",
    "\n",
    "Conversely, `glue_seams` undoes the cutting by merging close vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cffc829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def glue_seams(mesh, decimals=None):\n",
    "    \"\"\"\n",
    "    Merge close vertices.\n",
    "\n",
    "    Useful to undo cutting of meshes along UV seams. Note: the exact order of vertices will\n",
    "    not in general be recovered by glueing after cutting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mesh : ObjMesh\n",
    "    decimals : int or None, default 10\n",
    "        Vertices whose positions agree up to 'decimals' decimals are merged. Note: you can use negative values.\n",
    "        If None, estimate a value based on shortest edge length in the mesh (-2*log_10(minimum length))\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    glued_mesh : ObjMesh\n",
    "        Mesh with merged vertices.\n",
    "\n",
    "    \"\"\"\n",
    "    if decimals is None:\n",
    "        ls = igl.edge_lengths(mesh.vertices, mesh.tris)\n",
    "        decimals = np.round(-2*np.log10(ls.min())).astype(int)\n",
    "    rounded_verts = np.round(mesh.vertices, decimals=decimals)\n",
    "    unique_verts, index, inverse_index = np.unique(rounded_verts, axis=0, return_index=True, return_inverse=True)\n",
    "    sort_index = index.argsort()\n",
    "    sort_sort_index = sort_index.argsort()\n",
    "    unique_verts = unique_verts[sort_index]\n",
    "    glued_faces = [[[sort_sort_index[inverse_index[v[0]]], v[1]] for v in fc] for fc in mesh.faces]\n",
    "    glued_mesh = ObjMesh(unique_verts, glued_faces, texture_vertices=mesh.texture_vertices,\n",
    "                         normals=None, name=mesh.name)\n",
    "    glued_mesh.set_normals()\n",
    "    return glued_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd2f0a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = ObjMesh.read_obj(\"movie_example/initial_uv.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43c3fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_cut = mesh.cut_along_seams()\n",
    "glued_mesh = glue_seams(mesh_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7197ea4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20212, 3), (20623, 3), (20212, 3))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.vertices.shape, mesh_cut.vertices.shape, glued_mesh.vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ac5d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "glued_mesh.write_obj(\"movie_example/initial_uv_glued.obj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7b53e",
   "metadata": {},
   "source": [
    "## Create a 3d mesh of using the marching cubes method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5541aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_set = mcubes.smooth(segmentation, method=\"gaussian\") # converts segmentation into level set, with 0=surface\n",
    "vertices, faces = mcubes.marching_cubes(level_set, 0)\n",
    "\n",
    "# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\n",
    "vertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n",
    "                                 /np.array(metadata_dict['subsampling_factors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16e17516",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = ObjMesh(vertices_in_microns, faces)\n",
    "mesh.name = \"basics_example_mesh_marching_cubes\"\n",
    "mesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f399d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def marching_cubes(volume, isovalue=0.5, sigma_smoothing=0):\n",
    "    \"\"\"\n",
    "    Compute triangular mesh of isosurface using marching cubes as implemented by lib|igl.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    volume : 3d np.array\n",
    "        Array with scalar values from which to compute the isosurface.\n",
    "    isovalue: float, default 0.5\n",
    "        Isosurface to extract\n",
    "    sigma_smoothing: float, default 0\n",
    "        If >0, carry out Gaussian smoothing before marching cubes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vertices : np.array of shape (n_vertices, 3)\n",
    "        Vertices\n",
    "    faces : np.array of shape (n_faces, 3)\n",
    "        Triangular faces (each face is a set of indices into the vertices array)\n",
    "    \"\"\"\n",
    "    pts_grid = np.stack(np.meshgrid(*[np.arange(i) for i in volume.shape], indexing=\"ij\"),\n",
    "                        axis=-1).reshape(-1,3, order=\"F\").astype(float)\n",
    "    if sigma_smoothing>0:\n",
    "        vals = ndimage.gaussian_filter(volume, sigma=sigma_smoothing).flatten(order=\"F\")\n",
    "    else:\n",
    "        vals = volume.flatten(order=\"F\")\n",
    "    vertices, faces = igl.marching_cubes(vals, pts_grid, *volume.shape, isovalue)\n",
    "    return vertices, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a64fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using igl\n",
    "\n",
    "vertices, faces = marching_cubes(segmentation, isovalue=0.5, sigma_smoothing=3)\n",
    "\n",
    "# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\n",
    "vertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n",
    "                                 /np.array(metadata_dict['subsampling_factors']))\n",
    "\n",
    "mesh = ObjMesh(vertices_in_microns, faces)\n",
    "mesh.name = \"basics_example_mesh_marching_cubes_igl\"\n",
    "mesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes_igl.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2e6b37d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 ms ± 3.64 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "912a10af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50229/735907688.py:66: RuntimeWarning: Warning: multiple meshes in .obj file\n",
      "  warnings.warn(f\"Warning: multiple meshes in .obj file\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 ms ± 3.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8bb46ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50229/735907688.py:66: RuntimeWarning: Warning: multiple meshes in .obj file\n",
      "  warnings.warn(f\"Warning: multiple meshes in .obj file\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 12.1 ms, total: 1.36 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## try a somewhat larger mesh with texture info etc\n",
    "\n",
    "mesh = ObjMesh.read_obj(\"registration_example/Drosophila_reference.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c90e49ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 19.9 ms, total: 1.17 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mesh.write_obj(\"registration_example/Drosophila_reference_test_write.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f01f6378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 194 ms, sys: 16.2 ms, total: 211 ms\n",
      "Wall time: 207 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: readOBJ() ignored non-comment line 4:\n",
      "  o embryo_rect\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = igl.read_obj(\"registration_example/Drosophila_reference.obj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf752cb2",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "\n",
    "We can now save the cartographic projections both as `.tif` stack for quantitative analysis, and as `.png`'s for visualization as mesh texture in blender. We will also save the metadata to a `.json` file\n",
    "\n",
    "Annoyingly, we have to normalize our data and convert it to 8-bit to save it as png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce60c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def save_dict_to_json(filename, dictionary):\n",
    "    \"\"\"\n",
    "    Save dictionary to .json file.\n",
    "    \n",
    "    Will automatically convert numpy arrays to lists for saving. If you get an error like \"XXX is not JSON\n",
    "    serializable\", you need to ensure all your dictionary items are things that can be saved to text by json\n",
    "    (strings, numbers, lists).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Filename to save to\n",
    "    dictionary : dict\n",
    "        Dictionary to save\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    serializable_dictionary = {key: val.tolist() if isinstance(val, np.ndarray) else val\n",
    "                               for key, val in dictionary.items()}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(serializable_dictionary, f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823262d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata\n",
    "save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70038a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def save_for_imageJ(filename, image, z_axis=None, channel_axis=None):\n",
    "    \"\"\"\n",
    "    Save image as 32bit ImageJ compatible .tif file\n",
    "    \n",
    "    If channel_axis is not provided, it is inferred as the shortest axis.\n",
    "    If z-axis is provided for a 4d array, it will be set as the default z-axis for ImageJ.\n",
    "    \"\"\"\n",
    "    channel_axis = np.argmin(image.shape) if channel_axis is None else channel_axis\n",
    "    if len(image.shape) == 3:\n",
    "        transposed_image = np.moveaxis(image, channel_axis, 0)\n",
    "        tifffile.imwrite(filename, transposed_image.astype(np.float32),\n",
    "                         metadata={'axes': 'CYX'}, imagej=True,)\n",
    "    elif len(image.shape) == 4:\n",
    "        if z_axis is not None:\n",
    "            transposed_image = np.moveaxis(image, (z_axis, channel_axis), (0,1))\n",
    "        else:\n",
    "            transposed_image = np.moveaxis(image, channel_axis, 1)\n",
    "        tifffile.imwrite(filename, transposed_image.astype(np.float32),\n",
    "                         metadata={'axes': 'ZCYX'}, imagej=True,)\n",
    "    return None\n",
    "    \n",
    "def normalize_quantiles_for_png(image, quantiles=(0.01, 0.99)):\n",
    "    \"\"\"\n",
    "    Normalize an image by setting given quantiles to 0 and 255 and converting to 8-bit, for saving as .png\n",
    "    \n",
    "    Also replaces nan by 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.array\n",
    "        Image (should be single-channel)\n",
    "    quantiles : tuple\n",
    "        Image quantile to set to 0 and 255.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image_normalized : np.array\n",
    "        Normalized image, datatype np.uint8\n",
    "    \"\"\"\n",
    "    image_normalized = image - np.nanquantile(image, quantiles[0])\n",
    "    image_normalized /= np.nanquantile(image_normalized, quantiles[1])\n",
    "    image_normalized = np.nan_to_num(np.round(np.clip(255*image_normalized, 0, 255)), nan=0)\n",
    "    return image_normalized.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "def save_stack_for_blender(image, directory, normalization=(0.01, 0.99)):\n",
    "    \"\"\"\n",
    "    Save multichannel volumetric image as series of grayscale .png images. Can normalize data if desired.\n",
    "    \n",
    "    This function necessarily converts the image to 8bit. Use a suitable normalization to ensure nothing \n",
    "    is lost.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : 4d np.array\n",
    "        Axis 0 is assumed to be the channel axis, axis 1 is the slicing axes, i.e. images will correspond to\n",
    "        slices along axis 1.\n",
    "    directory : str\n",
    "        Path to save data to. Will create directory if it doesn't exist\n",
    "    normalization : tuple of float, or callable\n",
    "        Whether to normalize the image before saving it. If None, no normalization is performed. If a\n",
    "        tuple is given, it will be interpreted as quantiles to set to 0 and 255, respectively (over the\n",
    "        whole channel, not each slice). If a callable is provided, it will be applied to each channel.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    directory = directory.removesuffix('/')\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for ic, ch in enumerate(image):\n",
    "        if callable(normalization):\n",
    "            ch_normalized = normalization(ch)\n",
    "        if isinstance(normalization, tuple):\n",
    "            ch_normalized = normalize_quantiles_for_png(ch, quantiles=normalization)\n",
    "        for islc, slc in enumerate(ch_normalized):\n",
    "            slc = np.stack(3*[slc], axis=-1).astype(np.uint8) # necessary for saving as png\n",
    "            imsave(f'{directory}/channel_{ic}_slice_{str(islc).zfill(3)}.png', slc, check_contrast=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (2, 5, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# read the data so we can check the saving function\n",
    "projected_data = adjust_axis_order(imread(f\"{metadata_dict['filename']}_projected.tif\"))\n",
    "print(\"Image shape\", projected_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data, z_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\n",
    "save_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:blender-tissue-cartography] *",
   "language": "python",
   "name": "conda-env-blender-tissue-cartography-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
