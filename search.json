[
  {
    "objectID": "00_tissue_cartography_overview.html",
    "href": "00_tissue_cartography_overview.html",
    "title": "Tissue cartography with blender_tisssue_cartography",
    "section": "",
    "text": "Tissue cartography (TC) is a tool for analyzing 3d biological image data by taking advantage of the laminar, sheet-like organization of many biological tissues. Tissue cartography extracts a surface of interest (SOI) from the volumetric image data, for example, the tube-like structure of the developing fly gut, and cartographically projects it into a 2d plane (see figure below, from Mitchell & Cislo 2023). This is extremely helpful for both data visualization and data analysis, for example cell tracking.\nTissue cartography was proposed by Heemskerk & Streichan 2015 (ImSAnE software package). See also Mitchell & Cislo 2023 (TubULAR software package) and Herbert et al. 2021 (LocalZProjector software package).\nHowever, these existing tools for tissue cartography are either limited in the type of geometries they can handle, or difficult for non-experts to use and build upon. blender_tisssue_cartography is a set of Python tools, template analysis pipelines, and tutorials to do tissue cartography using the popular 3D creation software Blender. blender_tisssue_cartography makes tissue cartography user-friendly via simple, modular Python code, Blender’s graphical user interface, and powerful algorithms from the computer graphics community.\nYou can use blender_tissue_cartography in two forms:\nFor sophisticated use cases (dynamic datasets, custom computer vision operation), or automated pipeline, use the python library. Most users though can start with the blender add-on, which allows you to carry out almost all steps of the tissue cartography pipeline within blender’s graphical user interface.\nblender_tisssue_cartography also features a general-purpose pipeline for dynamic surfaces from timelapse microscopy in which the user graphically defines a cartographic projection for a single key frame, which is propagated to all other frames via robust surface-to-surface registration algorithms (tutorial 9, “Time-lapse imaging and dynamic surfaces”). A differential geometry module allows to analyze tissue flows and surface geometry dynamics (tutorial 10, “3D analysis and differential geometry”).",
    "crumbs": [
      "Tissue cartography with `blender_tisssue_cartography`"
    ]
  },
  {
    "objectID": "00_tissue_cartography_overview.html#tissue-cartography-workflow",
    "href": "00_tissue_cartography_overview.html#tissue-cartography-workflow",
    "title": "Tissue cartography with blender_tisssue_cartography",
    "section": "Tissue cartography workflow",
    "text": "Tissue cartography workflow\nThe TC workflow begins with a volumetric recording, for example, a \\(z\\)-stack recorded on a confocal microscope. Let’s consider first the case of a single recording with a single time point. TC comprises the following steps:\n\nSegmentation: volumetric image \\(\\mapsto\\) 3d segmentation. From the volumetric image - a .tif file , the user creates a 3d segmentation, also a .tif file with values from 0-1. This segmentation divides the image into “inside” and “outside”, and the SOI is defined as the boundary.\n\nNote: not all surfaces can be represented as the boundary of a volume. One can also directly segment out the SOI (and use a different algorithm below in step 2).\n\nMeshing: 3d segmentation \\(\\mapsto\\) surface mesh. A polygonal mesh of the segmentation boundary is computed, representing the surface of interest.\nUV mapping: Polygonal mesh \\(\\mapsto\\) cartographic plane. A map from the mesh to a cartographic plane is generated (parametrized by coordinates \\(u, v\\), to distinguish from the \\(x,y,z\\) 3d coordinates).\nProjection: volumetric image + polygonal mesh + UV map \\(\\mapsto\\) projected image. The volumetric image data is interpolated onto the UV cartographic plane using the 3d positions from the triangular mesh, creating a 2d projection of the 3d image data (“pullback”).\n\nMulti layers: to capture the whole thickness of a tissue sheet, one typically creates multiple projections using a version of the triangular mesh that is shifted along the local normal direction “inwards” and “outwards” (figure from Heemskerk & Streichan 2015):\n\n\n\n\n\nimage-3.png\n\n\n\nAnalysis and visualization: with the projected data in hand, we can visualize it in 3D, or analyse it in 2D (e.g. segment cells), computationally correcting for the distortion of the projection.\nBatch processing: batch process multiple 3D images, for example frames of a movie.\n\nThese steps will be explained in detail as you go through the following tutorials.",
    "crumbs": [
      "Tissue cartography with `blender_tisssue_cartography`"
    ]
  },
  {
    "objectID": "00_tissue_cartography_overview.html#design-principles-of-blender_tisssue_cartography",
    "href": "00_tissue_cartography_overview.html#design-principles-of-blender_tisssue_cartography",
    "title": "Tissue cartography with blender_tisssue_cartography",
    "section": "Design principles of blender_tisssue_cartography",
    "text": "Design principles of blender_tisssue_cartography\n\nBlender for graphical, interactive workflows - blender is a widely used, open-source program for 3D creation and contains advanced tools for mesh editing, UV mapping, and 3D visualization within a well-documented GUI. All of the main functionalities are available from within a Blender add-on - no programming knowledge necessary.\nFile-based encapsulation - each step in the pipeline terminates in a single data file (e.g. a .obj mesh, a .tif 3d segmentation), with all subsequent steps only depending on that data file. This means you can easily switch out the tools used for each step.\nPython library + template Jupyter notebooks. For sophisticated users, the blender_tissue_cartography python library allows creating custom and automatized pipelines. Template / tutorial jupyter notebooks are provided as starting points.\nUV transfer for dynamic data For dynamic data (i.e. movies), we provide general-purpose algorithms that transfer the user-defined UV map from one frame to all other frames. This allows non-experts to create consistent cartographic projections across time series.",
    "crumbs": [
      "Tissue cartography with `blender_tisssue_cartography`"
    ]
  },
  {
    "objectID": "00_tissue_cartography_overview.html#software-stack",
    "href": "00_tissue_cartography_overview.html#software-stack",
    "title": "Tissue cartography with blender_tisssue_cartography",
    "section": "Software stack",
    "text": "Software stack\n\nPython libraries:\n\njupyter\nNumpy / Matplotlib / Scipy\nskimage various image processing tools.\nh5py for reading/writing of .h5 files.\ntifffile for reading/writing of .tif files, including metadata.\nlibigl Triangular meshes.\n\nIlastik Image classification and segmentation,\nBlender Mesh editing and UV mapping.\n\n\nOptional\n\nMeshlab GUI and Python library with advanced surface reconstruction tools (required for some workflows). Not available for new Macs with ARM CPUs.\nPython libraries:\n\nPyMeshLab Python interface to MeshLab. Not available for new Macs with ARM CPUs\nnbdev for notebook-based development, if you want to add your code\n\nBlender plugins:\n\nMicroscopyNodes for rendering volumetric .tif files in blender\n\n\n\n\nWorkflow details\nWe briefly outline the tools and algorithms available for each step in the tissue cartography pipeline. They are implemented in both the blender_tissue_cartography library, as well as the Blender add-on. This section explains the required input and output data at each step in case you want to swap out the provided tools for something else.\n\n1. Segmentation\nTo create a segmentation, one first downsamples the data to reduce computational overhead. Next, we can use any of the following tools to create a 3d segmentation:\n\nIlastik - Tutorial 1.\nMorphsnakes - Tutorial 7\nCustom Python code\n\nFor the segmentation, there are two options:\n\nSegment out the object of which your SOI is the boundary (i.e. 1’s in the segmentation correspond to pixels that lie inside the volume of which your surface is the boundary). This assumes that our SOI is a closed/watertight surface (including the case where the SOI ends at the 3d image boundary).\nDirectly segment your SOI (i.e. 1’s in the segmentation correspond to pixels that lie on the surface). See the alternative workflow below. (Generally, option 1 is easier).\n\nOutput: 3d segmentation as .tif file\n\n\n2. Meshing\nWhen the surface is defined as by a volume segmentation (i.e. as a so-called “level set”), robust algorithms exist to compute the corresponding mesh, namely marching cubes and many other.\n\nMarching cubes is shipped with blender_tissue_cartography.\nMeshLab GUI and python library, contains marching cubes and many other tools for surface reconstruction\n\n\nAlternate workflow: segmentation and meshing for open surfaces\nIf we have an open surface that is not the boundary of some solid volume, or for some other reason we want to directly segment the surface, we need to use a different algorithm. From our surface segmentation, we extract a cloud of points on the surface. From the point cloud, one can create a triangular mesh using a variety of algorithms, notably Poisson surface reconstruction, as implemented by MeshLab. See tutorial 7.\nOutput: surface mesh as .obj file\n\n\n\n3. UV mapping and mesh processing\nTo create the map from the mesh to the cartographic \\(uv\\) plane, we use be blender and its UV mapping functionality. If the mesh looks very poor, we can step back and improve the segmentation and/or meshing. We can also edit the mesh here to correct segmentation/mesh generation errors.\n\nUV mapping conventions\nWe adopt the UV mapping conventions of the graphics community. \\(u,v\\) always range from 0-1 (with periodic boundary conditions). In general, blender will always map the entire mesh into the \\(u,v\\) plane.\nOutput: surface mesh with normals and texture coordinates as .obj file\n\n\n\n4. Projection\nUsing scipy, we interpolate the volumetric image data onto the \\(uv\\) plane. Using the vertex normals, we create a multilayer projection.\nOutput: two .tif \\(z\\)-stacks (interpolated data and 3d coordinates) and interpolated data as a series of .png images to use as blender textures\n\n\n5. Visualization and analysis\nWe can now load the projected data into blender as textures and see whether we are content with the result. If not, we iterate by editing the mesh and/or the \\(uv\\) mapping. Otherwise, we are done - we can proceed to the analysis of the projected images or use blender to create publication-quality 3d renders.\n\n\n6. Multiple recordings and movies\nIf we have multiple recordings of very similar structures or dynamic data, i.e. movies with multiple frames, we need to extract a surface mesh for each timepoint as above. However, we construct a UV map only once, for a suitably chosen keyframe or reference mesh. We then transfer that UV map to all other frames/recordings using surface t0 surface registration algorithms",
    "crumbs": [
      "Tissue cartography with `blender_tisssue_cartography`"
    ]
  },
  {
    "objectID": "Tutorials/08_multiple_recordings_and_reference_meshes.html",
    "href": "Tutorials/08_multiple_recordings_and_reference_meshes.html",
    "title": "8. Consistent cartographic projections across multiple recordings",
    "section": "",
    "text": "In previous tutorials, we saw how to do tissue cartography with a single volumetric image. But often, we have multiple images of very similarly shaped objects - either the successive frames of a movie or multiple recordings of biological structures with very consistent shapes, like the Drosophila egg. We want to use “the same” UV map/cartographic projection for all of the images - both so that we don’t need to redo the work of creating the UV map, and so that positions in our cartographic projections always correspond to the same anatomical position on the imaged object.\nTo do this, we use mesh to mesh mapping. The idea is that we have a reference mesh - for example, from the first frame of a movie - on which a UV map is defined. We then move and deform this mesh so that it fits our target mesh - which describes the surface we want to extract from the volumetric data, for example in subsequent frames - as well as possible. The deformed reference mesh now fits the volumetric data but still carries the UV map, and can now be used to create a cartographic projection.\nIf you have a consistently shaped object that you know you will image many times - in the Streichan lab, the early Drosophila embryo, of which we have hundreds of in toto recordings - it might make sense to make an idealized “prototypical” mesh with a nice mesh and UV map that you can use as a reference.\nMesh to mesh mapping is a well-studied problem and we can make use of many robust and already-implemented algorithms. We proceed in two steps:\nHere is an illustration - the green reference mesh is first registered to the orange target mesh:\nNext, the shrink-wrapping operation deforms the reference mesh to snuggly fit the target mesh:\nFor step 1, we provide a Python module and a button in the Blender add-on. We could also use the pymeshlab GUI. For step 2, we will use the shrinkwrap modifier in blender. In tutorial 9, we will see how to carry out shrinkwrapping automatically from within python.\nWe will explain the process using the dataset in nbs/Tutorials/wrapping_example.\nThis approach works if the reference mesh and the target mesh are roughly of similar shape. If this is no longer the case - for example in a movie where the surface of interest undergoes drastic deformations - more sophisticated approaches are required. These are described in full generality in tutorial 10.\nfrom blender_tissue_cartography import io as tcio\nfrom blender_tissue_cartography import mesh as tcmesh\nfrom blender_tissue_cartography import remesh as tcremesh\nfrom blender_tissue_cartography import interpolation as tcinterp\nfrom blender_tissue_cartography import registration as tcreg\nfrom blender_tissue_cartography import wrapping as tcwrap\n\nThe history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\nimport numpy as np\nfrom skimage import transform\nfrom scipy import stats, spatial, linalg\nimport os\nimport matplotlib.pyplot as plt\n# this module will not be available on new ARM apple computers\n\nimport pymeshlab\nfrom blender_tissue_cartography import remesh_pymeshlab as tcremesh_pymeshlab\n\nWarning:\nUnable to load the following plugins:\n\n    libio_e57.so: libio_e57.so does not seem to be a Qt Plugin.\n\nCannot load library /home/nikolas/Programs/miniconda3/envs/blender-tissue-cartography/lib/python3.11/site-packages/pymeshlab/lib/plugins/libio_e57.so: (/lib/x86_64-linux-gnu/libp11-kit.so.0: undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0)\nnp.set_printoptions(suppress=True)",
    "crumbs": [
      "Tutorials",
      "8. Consistent cartographic projections across multiple recordings"
    ]
  },
  {
    "objectID": "Tutorials/08_multiple_recordings_and_reference_meshes.html#pre-processing",
    "href": "Tutorials/08_multiple_recordings_and_reference_meshes.html#pre-processing",
    "title": "8. Consistent cartographic projections across multiple recordings",
    "section": "Pre-processing",
    "text": "Pre-processing\n\nLoading and segmenting the dataset\nWe will use the same dataset - a Drosophila example - as in tutorial 5.\n\nmetadata_dict = {'filename': 'wrapping_example/Drosophila_CAAX-mCherry',\n                 'resolution_in_microns': (1.05, 1.05, 1.05), # lightsheet data has isotropic resolution\n                 'subsampling_factors': (1/2, 1/2, 1/2),\n                }\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order\n\nimage shape: (1, 190, 509, 188)\n\n\n\nsubsampled_image = tcio.subsample_image(image, metadata_dict['subsampling_factors'],\n                                        use_block_averaging_if_possible=False)\nprint(\"subsampled image shape:\", subsampled_image.shape)\n\nsubsampled image shape: (1, 95, 254, 94)\n\n\n\n\nCreate 3d segmentation\nNow create a 3d segmentation, in this case using ilatik. We use ilastik binary pixel classification. We could post-process the ilastik output here, for example using morphsnakes. We then load the segmentation back into the jupyter notebook.\nThe bright dots outside the embryo are fluorescent beads necessary for sample registration in light-sheet microscopy. You can ignore them.\nAttention: when importing the .h5 into ilastik, make sure the dimension order is correct! In this case, czyx for both export and import.\n\n# we now save the subsampled image a .h5 file for input into ilastik for segmentation\n\ntcio.write_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image)\n\n\n# after creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n\nsegmentation = tcio.read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # select the first channel of the segmentation - it's the probablity a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (95, 254, 94)\n\n\n\n# look at the segmentation in a cross section\n\nplt.imshow(segmentation[:,:,50], vmin=0, vmax=1)\n\n\n\n\n\n\n\n\n\n\nMeshing\nWe convert the segmentation into a triangular mesh using the marching cubes method and save the mesh as a wavefront .obj file.\nImportant convention For sanity’s sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n\n# now we create a 3d mesh of using the marching cubes method\n\nvertices, faces = tcremesh.marching_cubes(segmentation, isovalue=0.5, sigma_smoothing=3)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n                                 /np.array(metadata_dict['subsampling_factors']))\n\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"Drosophila_CAAX-mCherry_mesh_marching_cubes\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")\n\n\n# improve mesh quality using meshlab - optional\n\nmesh_remeshed = tcremesh_pymeshlab.remesh_pymeshlab(mesh)\nmesh_remeshed.write_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\n\n\nmesh_remeshed.vertices.shape, mesh.vertices.shape\n\n((8434, 3), (81322, 3))",
    "crumbs": [
      "Tutorials",
      "8. Consistent cartographic projections across multiple recordings"
    ]
  },
  {
    "objectID": "Tutorials/08_multiple_recordings_and_reference_meshes.html#registration",
    "href": "Tutorials/08_multiple_recordings_and_reference_meshes.html#registration",
    "title": "8. Consistent cartographic projections across multiple recordings",
    "section": "Registration",
    "text": "Registration\n\nUsing the blender_tissue_cartography Python library\nIn the data folder, we have the mesh we just created, Drosophila_CAAX-mCherry_mesh_remeshed.obj, as well as out reference mesh Drosophila_reference.obj - an idealized Drosophila embryo with a standardized UV mapping, corresponding to a cylindrical projection. You can look at both meshes in the blender file Drosophila_CAAX-mCherry.blend.\nWe now register the reference mesh, i.e. bring it into approximate alignment with the data mesh. This is done in two steps (using a first, coarse alignment, and refining it using the Iterative Closest Point algorithm).\n\nmesh_data = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\nmesh_ref = tcmesh.ObjMesh.read_obj(f\"wrapping_example/Drosophila_reference.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\ntrafo_initial, _ = tcreg.align_by_centroid_and_intertia(mesh_ref.vertices,\n                                                        mesh_data.vertices,\n                                                        scale=True, shear=True)\n\n\ntrafo_icp, _, _ = tcreg.icp(mesh_ref.vertices, mesh_data.vertices,\n                            initial=trafo_initial, threshold=1e-4, max_iterations=100,\n                            include_scaling=True, n_samples=5000)\n\n\n# apply the computed transformation and save the result\nmesh_registered = mesh_ref.apply_affine_to_mesh(trafo_icp)\nmesh_registered.name = \"Drosophila_reference_preregistered\"\nmesh_registered.write_obj(f\"wrapping_example/Drosophila_reference_registered.obj\")\n\n\n\nUsing the blender_tissue_cartography add-on\nYou can also carry out the registration directly in Blender, using the add-on version of blender_tissue_cartography:\n\n\n\nimage.png\n\n\nSee tutorial 3.\n\n\nUsing MeshLab\nYou can also perform registration graphically in MeshLab - see this tutorial: https://www.youtube.com/watch?v=30bJcj6yA4c. Use this if you have problems with the automated method above.",
    "crumbs": [
      "Tutorials",
      "8. Consistent cartographic projections across multiple recordings"
    ]
  },
  {
    "objectID": "Tutorials/08_multiple_recordings_and_reference_meshes.html#wrapping",
    "href": "Tutorials/08_multiple_recordings_and_reference_meshes.html#wrapping",
    "title": "8. Consistent cartographic projections across multiple recordings",
    "section": "Wrapping",
    "text": "Wrapping\n\nUsing Blender\nNow that we have registered the mesh, we can wrap it! Let’s first do it using blender with the shrinkwrap modifier. Go to the “layout” tab and click the “blue wrench” on the right to add a modifier. Search for shrinkwrap, select the target, and use “Tangent Normal Project” for best results:\n\n\n\nimage.png\n\n\nIf things look good, click “Apply” to make the modifier permanent and export the mesh as Drosophila_reference_wrapped.obj.\n\n\nAutomated wrapping using the Python library\nWe can also carry out the shrink-wrapping operation in Python, optionally smoothing the mesh after to remove “creases”.\n\nmesh_wrapped = tcwrap.shrinkwrap_igl(mesh_registered, mesh_data,  n_iter_smooth_target=1, n_iter_smooth_wrapped=1)\nmesh_wrapped.write_obj(f\"wrapping_example/Drosophila_reference_automated_wrapped.obj\")\n\n\n\nNormals and normal-related problems\nThis may be a good point to note that if you have any problems with multilayer projections, your normals may be messed up. Some advice on how to visualize and if necessary, recalculate mesh normals.\nUseful tools: “Recalculate normals” (under “Mesh” in “Edit Mode”), and the modifier “Normals -&gt; Weighted Normal” (smoothes normals) and “Deform -&gt; Smooth”.\nLet’s visualize our mesh normals on top of our image data.\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\n#mesh = tcmesh.ObjMesh.read_obj(f\"wrapping_example/Drosophila_reference_wrapped.obj\")\nmesh = tcmesh.ObjMesh.read_obj(f\"wrapping_example/Drosophila_reference_automated_wrapped.obj\")\n\n#mesh = tcmesh.ObjMesh.read_obj(f\"wrapping_example/Drosophila_CAAX-mCherry_mesh_remeshed_sphere_uv.obj\")\n\nresolution = metadata_dict[\"resolution_in_microns\"]\n\n\nslice_image, slice_vertices, slice_normals = tcinterp.get_cross_section_vertices_normals(\n    1, 100, image, mesh, metadata_dict[\"resolution_in_microns\"], width=1.5)\n\n\nplt.scatter(*slice_vertices.T, s=5, c=\"tab:red\")\nplt.quiver(*slice_vertices.T, *slice_normals.T, color=\"tab:red\")\n\nplt.imshow(slice_image[0], vmax=10000, origin=\"lower\") \n# normal vectors are pointing \"out\"",
    "crumbs": [
      "Tutorials",
      "8. Consistent cartographic projections across multiple recordings"
    ]
  },
  {
    "objectID": "Tutorials/08_multiple_recordings_and_reference_meshes.html#uv-projection",
    "href": "Tutorials/08_multiple_recordings_and_reference_meshes.html#uv-projection",
    "title": "8. Consistent cartographic projections across multiple recordings",
    "section": "UV projection",
    "text": "UV projection\nTo see how well all of this has worked, let’s use the wrapped mesh to generate UV projections. We will compare it with the automatic sphere unwrap on the original data mesh.\n\nnormal_offsets = np.array([-4, -2, 0, 2])\nmetadata_dict[\"normal_offsets\"] = normal_offsets # add the info to the metadata\nuv_grid_steps = 512\n\n\ntcio.save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)\n\n\nprojected_data_wrapped, projected_coordinates_wrapped, projected_normals_wrapped = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=f\"wrapping_example/Drosophila_reference_wrapped.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_reference_wrapped_smoothed_normals\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python_code/jupyter_notebooks/blender-tissue-cartography/blender_tissue_cartography/interpolation.py:217: RuntimeWarning: UV map has self-intersections, 111104 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n\n\n\nprojected_data, projected_coordinates, projected_normals = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=\"wrapping_example/Drosophila_CAAX-mCherry_mesh_remeshed_sphere_uv.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_CAAX-mCherry_mesh_remeshed\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python_code/jupyter_notebooks/blender-tissue-cartography/blender_tissue_cartography/interpolation.py:217: RuntimeWarning: UV map has self-intersections, 8 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n\n\n\n# show the projected data\n\nfig, (ax1, ax2) = plt.subplots(figsize=(8,8), ncols=2)\nax1.imshow(projected_data_wrapped[0, 0], vmax=10000)\nax2.imshow(projected_data[0, 0][::-1,::-1].T, vmax=10000)\n\n\n\n\n\n\n\n\n\n# save images for visualization in blender\ntexture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\ntcio.save_stack_for_blender(projected_data_wrapped, texture_path, normalization=(0.01, 0.99))\n\n\n# save images as .tif stack for analysis\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data_wrapped, z_axis=1)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_3d_coordinates.tif\", projected_coordinates_wrapped)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_normals.tif\", projected_normals_wrapped)\n\nLet’s add a shader to check the texture looks good:\n\n\n\nimage.png",
    "crumbs": [
      "Tutorials",
      "8. Consistent cartographic projections across multiple recordings"
    ]
  },
  {
    "objectID": "Tutorials/09_movies_and_dynamic_surfaces.html",
    "href": "Tutorials/09_movies_and_dynamic_surfaces.html",
    "title": "9. Time-lapse imaging and dynamic surfaces",
    "section": "",
    "text": "In previous tutorials, we saw how to do tissue cartography with a single volumetric image. But often, we have multiple images of very similarly shaped objects - either the successive frames of a movie or multiple recordings of biological structures with very consistent shapes, like the Drosophila egg.\nFor each recording/frame, we have a mesh representing our surface of interest. We want to use “the same” UV map/cartographic projection for all of the meshes - both so that we don’t need to redo the work of creating the UV map, and to facilitate comparison across recordings/frames\nThere are two possible strategies for getting “the same” UV map:\nThis approach works well for simple shapes, for example mildly deformed planes (use an axial projection, “UV -&gt; Project from view” in Blender’s UV editor), or surfaces resembling spheres or cylinders ( “UV -&gt; Unwrap -&gt; Sphere/Cylinder Project”). Simply load all meshes of interest into Blender, and apply the projection (see Tutorial 3).\nHowever, we will not pursue this approach further here: for more complex shapes a simple axial or clyinder projection will no longer work. There are more sophisticated algorithms (see e.g. Mitchell & Cislo, 2023) - but using them can require a lot of expertise. Also using a fixed algorithm means that you cannot define your UV maps in a graphical and interactive way.\nThis allows you to graphically design the UV map for the reference mesh in whatever way you want. Here is a simple illustration, with reference mesh in green and target mesh from the data in orange:\nBelow, we explain the nuts and bolts of approach 2, also called texture tranfer or surface to surface registration in some detail, and showcase the tools provided by the blender_tissue_cartography python library, using the registration, wrapping and harmonic modules. Most of these tools are also available in the blender add-on (see tutorial 3).",
    "crumbs": [
      "Tutorials",
      "9. Time-lapse imaging and dynamic surfaces"
    ]
  },
  {
    "objectID": "Tutorials/09_movies_and_dynamic_surfaces.html#load-image-data",
    "href": "Tutorials/09_movies_and_dynamic_surfaces.html#load-image-data",
    "title": "9. Time-lapse imaging and dynamic surfaces",
    "section": "Load image data",
    "text": "Load image data\nThe image data shows fluorescently marked nuclei. The large “hole” in the center of the embryo is the developing midgut.\n\nimage = tcio.adjust_axis_order(tcio.imread(\"midgut_example/downsampled_3x/Time_000001_c1_stab-3x.tif\"))\nprint(\"image shape:\", image.shape)\n\nimage shape: (1, 247, 413, 200)\n\n\n\nslice_image, slice_vertices = tcinterp.get_cross_section_vertices_normals(0, 150, image,\n                            mesh_initial_UV, metadata_dict[\"resolution_in_microns\"], width=2, get_normals=False)\n\n\nplt.scatter(*slice_vertices.T, s=5, c=\"tab:red\")\nplt.imshow(slice_image[0], vmax=10000, origin=\"lower\")",
    "crumbs": [
      "Tutorials",
      "9. Time-lapse imaging and dynamic surfaces"
    ]
  },
  {
    "objectID": "Tutorials/09_movies_and_dynamic_surfaces.html#mesh-registration",
    "href": "Tutorials/09_movies_and_dynamic_surfaces.html#mesh-registration",
    "title": "9. Time-lapse imaging and dynamic surfaces",
    "section": "Mesh registration",
    "text": "Mesh registration\nLet’s first see how we can register two meshes using rigid body transformations.\n\nmesh_source = deepcopy(meshes_dict[1]) # the \"source\" mesh is the one we want to tranform to match the target\nmesh_target = deepcopy(meshes_dict[1]) # for the target mesh, we take a copy that we will rotate+scale+translate\n\nrandom_rotation = stats.special_ortho_group.rvs(3)\nmesh_target.vertices = 1.1 * mesh_target.vertices @ random_rotation + np.array([100, 0, 0])\n\n\n# registration proceeds in two steps: first, a coarse step gets an initial guess for the transformation\n\ntrafo_initial, _ = tcreg.align_by_centroid_and_intertia(mesh_source.vertices, mesh_target.vertices,\n                                                        scale=True)\n# and then the transformation is improved by the ICP algorithm (en.wikipedia.org/wiki/Iterative_closest_point)\n\ntrafo_icp, _, _ = tcreg.icp(mesh_source.vertices, mesh_target.vertices, initial=trafo_initial,\n                            threshold=1e-4, max_iterations=200, include_scaling=True, n_samples=10000)\n\n\n# a transformation is represented by a 4*4 matrix (en.wikipedia.org/wiki/Affine_transformation#Augmented_matrix)\ntrafo_icp\n\narray([[ -0.33481982,   0.5663229 ,   0.88130666, 100.12547619],\n       [ -0.92947084,   0.26932741,  -0.52252474,   0.03525151],\n       [ -0.48907168,  -0.90324193,   0.39543405,   0.21661484],\n       [  0.        ,   0.        ,   0.        ,   1.        ]])\n\n\n\n# let's apply the transformation and check we get the target mesh\n\nmesh_registered = mesh_source.apply_affine_to_mesh(trafo_icp)\n\n# indeed, the distance between the vertices is very small.\nnp.linalg.norm(mesh_registered.vertices-mesh_target.vertices, axis=-1).mean()\n\n0.12687255365401112",
    "crumbs": [
      "Tutorials",
      "9. Time-lapse imaging and dynamic surfaces"
    ]
  },
  {
    "objectID": "Tutorials/09_movies_and_dynamic_surfaces.html#shrink-wrapping-pipeline",
    "href": "Tutorials/09_movies_and_dynamic_surfaces.html#shrink-wrapping-pipeline",
    "title": "9. Time-lapse imaging and dynamic surfaces",
    "section": "Shrink-wrapping pipeline",
    "text": "Shrink-wrapping pipeline\nNow let’s try to serially shrink-wrap our initial mesh onto the subsequent timepoints, as described above. We use the function tcwrap.shrinkwrap_igl for this. It is useful to slightly smooth both the keyframe and the target of the shrinkwrap a little before shrinkwrapping, which you can control using the arguments n_iter_smooth_target, n_iter_smooth_wrapped.\nYou can also try out shrinkwrapping in blender, using the shrinkwrap modifier as explained in tutorial 7.\nAfter wrapping, we use on surface smoothing to remove creases potentially created by the shrink-wrapping.\nYou may need to experiment with the parameters a little.\n\n# we initialize the list of shrink-wrapped meshes with our keyframe\nmeshes_wrapped = {1: mesh_initial_UV}\n\n\nfor i in tqdm(range(2, 21)):\n    # we use the most recent deformed keyframe as input in the shrink-wrapping algorithm\n    mesh_source = meshes_wrapped[i-1]\n    mesh_target = meshes_dict[i]\n    # register using ICP\n    trafo_initial, _ = tcreg.align_by_centroid_and_intertia(mesh_source.vertices, mesh_target.vertices,\n                                                            scale=True, choose_minimal_rotation=True)\n    trafo_icp, _, _ = tcreg.icp(mesh_source.vertices, mesh_target.vertices, initial=trafo_initial,\n                                max_iterations=100, n_samples=5000)\n    mesh_registered = mesh_source.apply_affine_to_mesh(trafo_icp)\n    # shrink-wrap\n    mesh_wrapped = tcwrap.shrinkwrap_igl(mesh_registered, mesh_target,\n                                         n_iter_smooth_target=1, n_iter_smooth_wrapped=0)\n    # smooth out deformation on-surface - important!\n    mesh_wrapped = tcsmooth.smooth_laplacian_on_surface(mesh_wrapped, n_iter=5, lamb=0.5, n_iter_laplace=10)\n\n    # append to list\n    mesh_wrapped.write_obj(f\"midgut_example/meshes_wrapped/mesh_{str(i).zfill(2)}_wrapped.obj\")\n    meshes_wrapped[i] = mesh_wrapped\n\n\n\n\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/wrapping.py:206: RuntimeWarning: Warning: 2 normal(s) flipped during shrink-wrapping\n  warnings.warn(f\"Warning: {np.sum(dots&lt;0)} normal(s) flipped during shrink-wrapping\", RuntimeWarning)\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/wrapping.py:206: RuntimeWarning: Warning: 5 normal(s) flipped during shrink-wrapping\n  warnings.warn(f\"Warning: {np.sum(dots&lt;0)} normal(s) flipped during shrink-wrapping\", RuntimeWarning)\n\n\n\n# the wrapped meshes contain the same UV map information as the original mesh\n\nnp.allclose(meshes_wrapped[20].texture_vertices, mesh_initial_UV.texture_vertices)\n\nTrue",
    "crumbs": [
      "Tutorials",
      "9. Time-lapse imaging and dynamic surfaces"
    ]
  },
  {
    "objectID": "Tutorials/09_movies_and_dynamic_surfaces.html#inspect-in-blender",
    "href": "Tutorials/09_movies_and_dynamic_surfaces.html#inspect-in-blender",
    "title": "9. Time-lapse imaging and dynamic surfaces",
    "section": "Inspect in blender",
    "text": "Inspect in blender\nLet’s inspect the results in blender (midgut_example/wrapping.blend).\nFor the first, say 10 time points, this approach looks decent. After that, we encounter a problem: the surface if the gut deforms very strongly (by forming constrictions). If we do not use on-surface smoothing, we get poor results:\n\n\n\nimage-3.png\n\n\nOn-surface smoothing improves this a lot:\n\n\n\nimage-4.png\n\n\n\nOperate in reverse time\nHowever, the shrink-wrapped mesh fails to capture some sharp details of the target meshes (close to the constrictions, for example).\nAs described above, it is generally best to use your most complicated shape as a keyframe (here, the last timepoint), and deform it towards the simpler shapes. Hence, let us define a UV map for timepoint 20 and shrink-wrap reverse in time.\n\nt_final = 20\nmeshes_wrapped_reverse = {20: mesh_final_UV}\n\n\nfor i in tqdm(reversed(range(1, t_final))):\n    print(i)\n    mesh_source = meshes_wrapped_reverse[i+1]\n    mesh_target = meshes_dict[i]\n    # register using ICP\n    trafo_initial, _ = tcreg.align_by_centroid_and_intertia(mesh_source.vertices, mesh_target.vertices,\n                                                            shear=True, choose_minimal_rotation=True)\n    trafo_icp, _, _ = tcreg.icp(mesh_source.vertices, mesh_target.vertices, initial=trafo_initial,\n                                max_iterations=100, include_scaling=True, n_samples=5000)\n    mesh_registered = mesh_source.apply_affine_to_mesh(trafo_icp)\n    # shrink-wrap\n    mesh_wrapped = tcwrap.shrinkwrap_igl(mesh_registered, mesh_target,\n                                         n_iter_smooth_target=1, n_iter_smooth_wrapped=1)\n    # smooth out deformation on-surface\n    mesh_wrapped = tcsmooth.smooth_laplacian_on_surface(mesh_wrapped, n_iter=5, lamb=0.5, n_iter_laplace=10)\n    # append to list\n    mesh_wrapped.write_obj(f\"midgut_example/meshes_wrapped_reverse/mesh_{str(i).zfill(2)}_wrapped_reverse.obj\")\n    meshes_wrapped_reverse[i] = mesh_wrapped\n\n\n\n\n19\n\n\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/wrapping.py:206: RuntimeWarning: Warning: 3 normal(s) flipped during shrink-wrapping\n  warnings.warn(f\"Warning: {np.sum(dots&lt;0)} normal(s) flipped during shrink-wrapping\", RuntimeWarning)\n\n\n18\n17\n16\n15\n14\n13\n12\n11\n10\n\n\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/wrapping.py:206: RuntimeWarning: Warning: 1 normal(s) flipped during shrink-wrapping\n  warnings.warn(f\"Warning: {np.sum(dots&lt;0)} normal(s) flipped during shrink-wrapping\", RuntimeWarning)\n\n\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\n\n\n\nInspect in blender\nThe results look decent enough.\nLet’s visualize the UV maps on the wrapped meshes by adding an image texture in the shading workspace. We can use a color grid to visualize the correspondence between the meshes:\n\n\n\nimage.png\n\n\nLet’s show the keyframe mesh final_uv, and the shrink-wrapped one side by side:\n\n\n\nimage-2.png\n\n\nLooks good (except for the fact that I flipped the mesh orientations …)\n\n\nRemeshing\nAs noted above, most remeshing algorithms destroy UV information. However, there are two exceptions: 1. Subdivision - this type of algorithm refines meshes by adding new vertices and faces to a mesh according to a pre-determined rule (for example, place a new vertex at each triangle midpoint). Since there is a well-defined map between the topology before and after subdivision, the UV info is preserved. Note: subdivision will greatly increase the size of your mesh, and it may become too large computationally. A variant is adaptive subdivision (subdividing certain parts of the mesh only, e.g. deformed triangles). 2. Edge flips - this changes the configuration of faces whithout moving vertices. Only implemented for triangle meshes.\nThese algorithms may become very useful to improve the mesh quality of your keyframed mesh as you deform it, and are implemented in the remesh module. Adaptive subdivision requires pymeshlab, see remesh_pymeshlab module.\n\n# let's test this\nmesh_test = deepcopy(mesh_final_UV)\n\n\nmesh_subdiv = tcremesh.subdivide_igl(mesh_test, reglue=True)\n\n\n# as you can see, the mesh resolution is greatly increased\n\nfig, (ax1, ax2) = plt.subplots(figsize=(10, 5), ncols=2)\n\nax1.triplot(*mesh_test.texture_vertices.T, mesh_test.texture_tris, lw=0.2)\nax2.triplot(*mesh_subdiv.texture_vertices.T, mesh_subdiv.texture_tris, lw=0.2)\n\n\n\n\n\n\n\n\n\n# the second algorithm involves changing the faces of the mesh by \"edge flips\" without moving vertices\n# with the goal to make the mesh triangles more regularly sized (so-called Delaunay triangulation)\n\nmesh_flipped = tcremesh.make_delaunay(mesh_test)\n\n&lt;blender_tissue_cartography.io.ObjMesh&gt;\n\n\n\n\nLarge deformations\nFor very large deformations, shrink-wrapping does not work. As noted above, we provide an implementation of Moebius registration, which first maps the two meshes of interest to the sphere, and then aligns them via 3d rotations. This algorithm is more time-consuming but can handle large deformations. It is implemented in the harmonic module as harmonic.wrap_coords_via_sphere.\nFor meshes with disk or cylinder topology, similar algorithms are provided, harmonic.wrap_coords_via_disk and harmonic.wrap_coords_via_disk_cylinder.\nFor the individual components of the algorithms, namely (a) the map to sphere/disk and (b) rotational alignment, see the documentation in notebook 03d. Note that you can also use your own methods to map surfaces to the sphere or disk, or use an external tool like boundary-first-flattening.\nA general-purpose code for shrink-wrapping via a shared parametrization is also provided.\n\n# let's try to shrink-wrap the final onto the initial mesh\n\n#mesh_source = deepcopy(mesh_final_UV)\n#mesh_target = deepcopy(meshes_dict[1])\n\nmesh_source = deepcopy(mesh_initial_UV)\nmesh_target = deepcopy(meshes_dict[20])\n\n\n# compute the new coordinates for the keyframe mesh mesh_source.\n# for the meaning of the parameters, type help(wrap_coords_via_sphere)\n\nnew_coords, overlap = tcharmonic.wrap_coords_via_sphere(mesh_source, mesh_target, method=\"harmonic\")\n\n\n# 'overlap' measures the overlap between source and target geometry and gives an idea of how different the\n# two shapes are and how well the wrapping worked. 1 is perfect alignment\n\noverlap\n\n0.918174332653755\n\n\n\n# let's save the mesh and display it in blender\n\nmesh_wrapped = deepcopy(mesh_source)\nmesh_wrapped.vertices = new_coords\nmesh_wrapped.set_normals()\nmesh_wrapped.write_obj(\"midgut_example/wrapped_moebius_20.obj\")\n\n# we could combine this with some on-surface smoothing if desired\n\n\n\n\nimage.png\n\n\nWe can also overlay it with the mesh from timepoint 1 to see that they match perfectly.\n\n\nCartographic projections\nFinally, let’s use the results to make some cartographic projections\n\nnormal_offsets = [0,] #np.linspace(-5, 5, 11) # in microns\nmetadata_dict[\"normal_offsets\"] = normal_offsets # add the info to the metadata\nuv_grid_steps = 1024\n\n\nprojected_data_tpt_20, projected_coordinates_tpt_20, projected_normals_tpt_20 = tcinterp.create_cartographic_projections(\n    image=\"midgut_example/downsampled_3x/Time_000020_c1_stab-3x.tif\",\n    mesh=\"midgut_example/wrapped_moebius_20.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\n\nplt.imshow(projected_data_tpt_20[0][0])\n\n\n\n\n\n\n\n\n\nprojected_data_tpt_1, projected_coordinates_tpt_1, projected_normals_tpt_1 = tcinterp.create_cartographic_projections(\n    image=\"midgut_example/downsampled_3x/Time_000001_c1_stab-3x.tif\",\n    mesh=mesh_initial_UV,\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\n\nplt.imshow(projected_data_tpt_1[0][0])",
    "crumbs": [
      "Tutorials",
      "9. Time-lapse imaging and dynamic surfaces"
    ]
  },
  {
    "objectID": "Tutorials/02_blender_tutorial.html",
    "href": "Tutorials/02_blender_tutorial.html",
    "title": "2. Blender tutorial",
    "section": "",
    "text": "As it says on the tin, the 3d software blender is at the heart of blender_tissue_cartography. We will use it for visualization, mesh editing, and cartographic projection of surfaces (known as “UV mapping” in the graphics community).\nBlender’s user interface can be slightly overwhelming at first, but don’t despair, it is very easy to learn. There are many great blender tutorials available on the web, and the manual can be found here, from which most of the material in this tutorial is taken. Many people use blender, so when you have a question of the form “How do I do X in blender?”, a Google search will probably answer it.\nThis tutorial is a lightning overview of the part of blender’s user interface relevant to tissue cartography. We assume you are using blender 4.3. We advise you to check out the blender manual and/or look at one of the many excellent YouTube tutorials if you have never used blender. Follow along by launching blender and opening a new, empty file:\n\n\n\nimage.png\n\n\n\n\nBlender user interface\nYour blender interface has three main parts: - Top bar with the “File” menu for opening, saving, and exporting models, and the choice of the different “Workspaces” - “Areas” in the middle, your workspace - “Status Bar” at the bottom, shows relevant shortcuts and messages\n\n\n\nimage.png\n\n\n\n\nImport a mesh\nLet’s import a mesh, namely the one created from ilastik in the preceding tutorial (ilastik_example/basics_example_mesh_marching_cubes.obj). You can either drag and drop it into blender, or click “File -&gt; Import” on the top left.\n\n\n\nimage.png\n\n\nImportant: axis order when importing a mesh into blender, you have to choose how blender interprets the \\(x,y,z\\) axes. It is important to keep this choice consistent - please select “Forward axis: Y” and “Up axis: Z”\n\n\n\nimage.png\n\n\n\n\nPolygonal meshes\nBlender and blender_tissue_cartography represents surfaces in 3D as polygonal meshes, that is collections of points (vertices), and edges and faces connecting them. You can see the vertices and faces by selecting “Wireframe” display option:\n\n\n\nimage.png\n\n\n\n\nNavigation\nI strongly recommend using a full keyboard and mouse when operating blender. Use the scroll wheel to zoom in/out, click on an object to select it (it should be highlighted in orange), and use the Navigate gizmo to rotate the axes. Click the “hands” button to move your view.\n\n\n\nimage.png\n\n\n\n\nWorkspaces\nBlender has different “workspaces” for different 3d tasks, selected using the top bar. Right now, we are in “Layout”, a general-purpose workspace for viewing your blender scene. It has three main parts: - “3d viewport” (center) - “Outliner” (top right) - “Properties” (bottom right)\n\n\n\nimage.png\n\n\n\nOutliner and Viewport options\nIn the top right corner of the user interface, you control what is in your scene and how it is shown:\n\n\n\nimage-3.png\n\n\nThe outliner allows you to toggle the visibility of meshes and rename or delete them. The viewport options allow you to switch, for example, to wireframe rendering and toggle “x-ray”, i.e. whether you can see and select “through” the mesh. Click on the circle with the lines to activate wireframe rendering.\n\n\nImportant properties\nIn the properties section of the user interface, you can select several tabs. The following are the most important for us:\n\n“Object”: set position, scale, visibility, etc of your meshes. I recommend locking the mesh positions so you don’t accidentally move your mesh, messing up the connection between mesh coordinates, and the underlying 3d image:\n\n\n\n\nimage-2.png\n\n\n\n“Modifier”: apply filters/transformations to your mesh, for example smoothing or remeshing:\n\n\n\n\nimage-4.png\n\n\n\n\nImportant workspaces\nWe will mainly use the following three workspaces: - “Layout” - “UV editing” - “Shading”\n\n\n\nUV editor\nIn the UV editor, we “unwrap” our mesh into a 2d square. The axes in the square are called \\(u, v\\) to distinguish them from the 3d \\(x,y,z\\) axes - hence the name “UV mapping”. Go to the “UV editing” workspace now. You can see two main areas: the mesh in 3d on the right, and the UV square on the left. It is empty since we haven’t unwrapped our mesh yet. Notice also that we are now in “Edit Mode”, since we will edit a property of our mesh, the UV map:\n\n\n\nimage-3.png\n\n\nTo unwrap the mesh, several algorithms can be used. Take a look at the blender manual. In this example, our job is very simple, since the mesh is already basically flat.\nWe first select the part of the mesh we want to unwrap, in this case, the whole thing. Press “3” and “A” to select all faces of a mesh. Aside: a mesh is made out of vertices, edges, and faces. You can go into vertex, edge, and face select mode by pressing “1”, “2”, and “3”. The mesh should light up orange. Now press “UV-&gt; Unwrap”:\n\n\n\nimage-4.png\n\n\nCongratulations - you have mapped your surface to the plane. You can now see it on the left, in the UV square:\n\n\n\nimage.png\n\n\nThe UV editor has a lot of tools for post-processing UV maps created by an unwrapping algorithm, for example, the “Grab” brush. Try it:\n\n\n\nimage.png\n\n\nThe “relax” brush is also very useful to reduce distortion.\n\n\nShading\nTo visualize textures on the mesh, use the top bar to switch to the “Shading” workspace:\n\n\n\nimage-2.png\n\n\nWe will eventually use this to visualize the projected data extracted from our 3d .tif stack. But right now, let’s just look at how shading works. Add a new material using the central button on the bottom area.\nMaterials define what the surface of a mesh looks like. Thanks to our UV map, we can take any square picture, and project it onto the 3d mesh. Let’s go to our new material and add an empty image. Press “Shift-A” and search for “Image Texture”. Click “New” - we will generate an image since we don’t have any data yet. Select “Generated Type -&gt; UV grid” to make a checkerboard pattern. This checkerboard pattern is very helpful to visualize the distortion of your UV map!\nThe result should look like this: \nAs you can see, the “Material” contains a bunch of different blocks connected by wires. This is blender’s graphical programming language for defining materials. In our example, we have an input image, whose “color” output node is connected to the “base color” input node of a shader (Principled BSDF, which simulates how light reflects from our mesh). The shader output is connected to the material surface. You can edit and reconnect the wiring using your mouse. Try to see what happens if you connect the image output directly to the “Surface” input node:\n\n\n\nimage-2.png\n\n\n\n\nExporting meshes\nWe can now export our mesh together with its UV map. The exported mesh is the basis for the algorithm that carries out the cartographic projection of the 3d data, which you will see in the next tutorial notebook.\nClick on “File -&gt; Export” and export as .obj:\n\n\n\nimage.png\n\n\nImportant: export settings\n\nAlways export as .obj\nChoose “Forwad axis: Y” and “Up axis: Z”\nTick the box that says “Include Selection only” - otherwise you might export multiple meshes, and the subsequent algorithm does not know which one to use\nInclude “UV coordinates” and “Normals” - this data will be important in the following\nSelect Triangulated Mesh - the algorithms in this software package are designed to work with triangular meshes.\nUse a sensible name. You will need the filename in what comes next. I recommend using the name of the .tif image the mesh belongs to, plus some suffixes, like _mesh_with_UV.obj.\n\nSave the blender file as a .blend project for further use, and close blender.\n\n\nNext steps\nThat’s it! You now know the minimal amount of blender necessary to use blender_tissue_cartography. You can move on to the next tutorial to see an example of the complete workflow.\nKeep in mind: blender is a powerful tool, in particular its UV Editor. We have barely scratched the surface of what it can do. Look at tutorials and the blender manual to see its full capabilities. Think twice before implementing UV mapping algorithms yourself in Python/MATLAB/… . Blender probably has a better implementation, a graphical user interface, and can be scripted in Python, as we will see in a later tutorial.",
    "crumbs": [
      "Tutorials",
      "2. Blender tutorial"
    ]
  },
  {
    "objectID": "Tutorials/03_blender_addon_tutorial.html",
    "href": "Tutorials/03_blender_addon_tutorial.html",
    "title": "3. blender_tissue_cartography blender add-on",
    "section": "",
    "text": "You can use blender_tissue_cartography in two forms:\nFor sophisticated use cases (highly dynamic datasets, custom computer vision operation), or automated pipeline, use the python library. Most users though can start with the blender add-on, which allows you to carry out almost all steps of the tissue cartography pipeline within blender’s graphical user interface.\nThe add-on has been tested with blender version 4.3.2 - no guarantees for other versions!",
    "crumbs": [
      "Tutorials",
      "3. `blender_tissue_cartography` blender add-on"
    ]
  },
  {
    "objectID": "Tutorials/03_blender_addon_tutorial.html#workflow-for-a-single-dataset-single-timepoint",
    "href": "Tutorials/03_blender_addon_tutorial.html#workflow-for-a-single-dataset-single-timepoint",
    "title": "3. blender_tissue_cartography blender add-on",
    "section": "Workflow for a single dataset / single timepoint",
    "text": "Workflow for a single dataset / single timepoint\nLet’s see the functionality of the add-on at work on the example dataset in the nbs/Tutorials/addon_example folder.\n\nLoading a volumetric dataset\nWe start by importing a volumetric dataset, in this case nbs/Tutorials/addon_example/Drosophila_CAAX-mCherry.tif (download it here). This is a light-sheet recording of the gastrulating Drosophila embryo. You can inspect it in Fiji:\n\n\n\nimage.png\n\n\nTo load the dataset into Blender, click on “File Path” to select the .tif file, specify the resolution in microns/pixel for the \\(x,y,z\\) axes (in this case, \\(1.05 \\mu m\\) for all axes), and click “Load .tiff file”:\n\n\n\nimage.png\n\n\nThe loaded dataset is represented by a box mesh with dimensions equal to those of the image volume in microns. The add-on displays the shape (number of pixels along each axis) and the number of channels):\n\n\n\nimage.png\n\n\nMulti-channel data The add-on supports both single-channel and multi-channel data (so .tif files with 3 or 4 dimensions). For time-series data, you need to provide one .tif per timepoint (see section on “Batch Processing” below).\nAxis order The add-on attemps to automatically recognize which axis is the channel axis and which one is \\(x,y,z\\). If this goes wrong, you can use the “Axis order” input to specify the axis order of the .tif file. Input as xyz, zxy, … for single-channel and cxyz, zcyx, … for multichannel data.\n\n\nLoading a mesh\nNext, we need to obtain a mesh to represent the surface of interest (SOI) onto which we want to project our volumetric data. You have two options:\n\nLoading a pre-computed mesh\nIf you already have a mesh, you can load it into blender via drag-and-drop. Try it with nbs/Tutorials/addon_example/Drosophila_CAAX-mCherry_mesh_premade.obj\nMesh units Importantly, the units of your mesh coordinates need to be in physical units, i.e. microns (and not pixels)!\nMesh axis order When importing a mesh into blender, you have to choose how blender interprets the \\(x,y,z\\) axes. It is important to keep this choice consistent - please select “Forward axis: Y” and “Up axis: Z”\n\n\nCreating a mesh from a segmentation\nAlternatively, you can create a mesh from a 3D segmentation, created for example by Ilastik (see Tutorial 1). This should be a single-channel volumetric .tif-file, with values close to 1 representing the inside, and close to 0 the outside of your sample.\nNote: If your surface of interest cannot be represented by the boundary of a volume (e.g. an open sheete floating around in free space), or if you have a segmentation of a hallow shell instead of a solid object, please see Tutorial 7.\nAn example segmentation is provided by nbs/Tutorials/addon_example/Drosophila_CAAX-mCherry_subsampled-image_Probabilities.tif Here is what is looks like in Fiji:\n\n\n\nimage.png\n\n\nClick on “Segmentation File Path” to select this file, and specify the resolution (here \\(2.1\\mu m\\) for all axes). The “Segmentation smoothing” option allows creating a smooth mesh from the blocky/pixelized segmentation data:\n\n\n\nimage.png\n\n\nSegmentation resolution The resolution of your segmentation .tif file can be different from the image data. This is to allow you to downsample your image when you segment it (e.g. in Ilastik), which makes the segmentation often much faster.\nAfter clicking “Get mesh from binary segmentation .tiff file”, you should see a new mesh object in your scene:\n\n\n\nimage.png\n\n\nAs you can see, the mesh of our surface lies nicely inside the bounding box of the image, indicating that they are correctly aligned.\nBatch processing Selecting a folder instead of a file under “Segmentation File Path” batch processes all files in folder. Selecting a multi-channel .tif file creates one mesh per channel (for example, if you have a segmentation with multiple labels for multiple objects).\n\n\n\nData storage and representation in the blender_tissue_cartography add-on\nBoth 3D image data and the projected 2D image data we will compute below are always associated with a mesh. For example, the 3D image data we loaded is associated with the “BoundingBox” mesh Drosophila_CAAX-mCherry_BoundingBox. What dataset the blender_tissue_cartography operations are applied to is determineed by which mesh you have selected (orange outlines) in blender.\n\n\nVisualize 3D data using orthogonal slices\nTo visualize the 3D data, we can create slices of it along the \\(x,y,z\\) axes and load them into blender. To do this, select the bounding box representing the 3D data (it should be outlined in orange), chose an axis, a position along the axis, and a channel (here channel 0, since this is a single-channel dataset):\n\n\n\nimage.png\n\n\nClick “create slice plane”, and select “Material preview” to see the image texture on the slice:\n\n\n\nimage.png\n\n\nWhen toggling the visibility of the SOI mesh back on, we can see that it fits nicely along the contours of the 3D data:\n\n\n\nimage.png\n\n\n\nVolumetric rendering using MicroscopyNodes\n.tiff files can also be rendered “volumetrically” in blderm using the MicroscopyNodes plugin. This means each voxel will emit (or absorb) light, showing the full 3D-data instead of just a slice. However, this can be computationally expensive and may not give you a “helpful” view of the data if the 3D data is complex or shows a large object.\n\n*MicroscopyNodes coordinates MicroscopyNodes scales and translates the .tiff data (scale 0.02 + centering in \\(x,y\\)). To overlay the volumetric object created by MicroscopyNodes with the blender_tissue_cartography representation, you need to undo this - go to “Object properties -&gt; Transform”:\n\n\n\nimage.png\n\n\n\n\n\nOptional - remeshing\nBy selecting “Wireframe” shading, we can see that the mesh has a lot more vertices/detail than necessary to represent the shape of the embryo. For many downstream applications, it is useful to have a mesh with a lower resolution. Go to the “Sclupting workspace” and click on “Remesh”, using whatever voxel-size you think is reasonable to control the mesh resolution:\n\n\n\nimage.png\n\n\n\n\nVertex shading\nThe add-on features two ways of projecting the 3D image data onto the SOI mesh. The first is called “Vertex shading”, which looks up the image intensity at each vertex of the mesh and uses it to color the mesh. This does not require any UV map (cartographic projection of the mesh to the plane).\nSelect both the bounding box representing the volumetric image, and the mesh onto which you want to project the data. Select the channel you want. You can use the “Normal offset” button to look up the image intensity at the position of the vertex shifted inwards or outwards along the surface normal. After clicking “Initialize vertex shading” you should see the image projected onto the surface:\n\n\n\nimage.png\n\n\nUse case Use this to get a quick idea of the image intensity of your mesh before creating your cartographic projection, or if you want to interactively sculpt the mesh (e.g. fixing holes, or deforming the mesh so it matches the 3D data better). Use the “Refresh vertex shading” button to update the vertex colors every time you sculpt.\nResolution You will need a relatively high-resolution mesh (with many vertices), or the vertex shader will look blurry. The resolution of cartographic projections (see below) on the other hand is independent of the mesh resolution - you can get a high-resolution projection with a coarse mesh.\n\nFine-tuning the vertex shading\nThe new texture of the mesh is controlled by a “Material” which you can edit in the “Shading” workspace for fine-tuning (e.g. adjusting brightness or contrast):\n\n\n\nimage.png\n\n\n\n\n\nCartographic projection\nVertex shading can give you a first idea of the image intensity projected onto the mesh, but it has several disadvantages:\n\nThe projected data is saved as one intensity per vertex, instead of as continuous 2D image, which makes quantitative image analysis (cell segmentation, for example) very difficult\nThe mesh resoution controls the image resolution so you need a very dense mesh (which is inefficient)\nYou can only see one part of the 3D shape at a time, which makes visualization harder\n\nInstead, we now compute a cartographic projection of the data, which unwraps the mesh to a plane (much like a map of the globe) and projects the 3D image data to a genuine 2D image.\n\nUnwrap mesh using UV editor\nWe first need to unwrap our mesh. This can be done in the “UV Editing” workspace (see previous tutorial). Here, we use the fully automatic “Smart UV Project” algorithm (“UV -&gt; Unwrap -&gt; Smart UV project”):\n\n\n\nimage.png\n\n\nBlender has many tools for creating cartographic projections, from cylindrical and spherical projections to more sophisticated algorithms that minimize distortion. You can also choose where your mesh is cut when it is mapped into the plane (“seams”). See tutorials 2 and 5, and 6.\nSelf-intersections It is important to try to unwrap the mesh while avoiding self-intersection (where the mesh is folded onto itself in 2D). This is bad because the multiple 3D positions are mapped to the same cartographic location. For example, if you use the “project from view on the example UV cube:\n\n\n\nimage.png\n\n\n\n\nCompute a cartographic projection\nNow we are ready to compute a cartographic projection using the add-on. The add-on can compute multilayer projections where each layer shows the image intensity at a given distance from the surface of interest inwards or outwards along the surface normal (a bit like the layers of an onion). You specify the layers you want as a comma-separated list using the “Normal offsets” field. Positive and negative values represent an outwards, respectively inwards shift (in microns).\nThe projected data will always be a square image, covering the UV square (the 2D region into which we unwrapped our mesh in the preceding step). The number of pixels of the image is controlled by the “Projection Format” field. Select both the mesh and the 3D data set, and click “Create Projection”:\n\n\n\nimage.png\n\n\nThe projected data just created is now associated with the mesh, and can be visualized in blender thanks to a newly created “Material”. You can check it out in the shading tab. You can look at the 2D projected image on the left. On the bottom, you can modify the connections of the nodes to determine, for instance, which layer of the multi-layer projection is shown:\n\n\n\nimage.png\n\n\nRegions of the UV square not covered by the unwrapped mesh are black.\n\n\nSave a cartographic projection\nFinally, we can save the cartographic projection to disk. Select the mesh of which you want to save the projection, and click “Save projection”:\n\n\n\nimage.png\n\n\nThis creates three .tiff files: test_projection_BakedData.tif (the projected 3D data), test_projection_BakedPositions.tif, and test_projection_BakedNormals.tif. The latter two are the 3D positions and surface normals (as RGB images, with red=\\(x\\) and so on), which are important for downstream analysis.\nCrucially, the projected image data is not rescaled, normalized, or distorted in any way - it reflects the numerical values of the voxel intensities in the original 3D data. Let’s open this in Fiji:\n\n\n\nimage.png\n\n\nFor the projected data, the \\(z\\)-axis of the stack represents the different layers we chose when projecting the data (-5 and 0 microns from the surface).\n\n\n\nAnnotating data in blender\nYou can annotate the projected data in Blender using the “Texture Paint” workspace. This allows you to paint on the 3D surface (for example, highlighting some cells you are interested in), and having the data saved to 2D on top of the projected image data. You can save the results to disk using teh “Image” button. Here is a simple example:\n\n\n\nimage.png\n\n\n\n\nVisualizing UV distortion\nTo visualize the distortion created by unwrapping a mesh, you can create a new material that projects a colored grid onto the mesh surface. Go to the shading editor and add a new material, with an image input (“Shift+A” -&gt; “Texture” -&gt; “Image Texture”), and select new image with generated type “Color Grid” or “UV Grid”:\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nYou can now see which region in UV space corresponds to which region in 3D, and based on the size and the shape of the squares gauge how much distortion there is.",
    "crumbs": [
      "Tutorials",
      "3. `blender_tissue_cartography` blender add-on"
    ]
  },
  {
    "objectID": "Tutorials/03_blender_addon_tutorial.html#workflow-for-multiple-datasets-multiple-timepoints",
    "href": "Tutorials/03_blender_addon_tutorial.html#workflow-for-multiple-datasets-multiple-timepoints",
    "title": "3. blender_tissue_cartography blender add-on",
    "section": "Workflow for multiple datasets / multiple timepoints",
    "text": "Workflow for multiple datasets / multiple timepoints\nSo far, we have looked at a single volumetric dataset in isolation. But often, such a dataset is part of a larger collection - for example, frames/timepoints of a movie or different recordings of Drosophila embryos, whose eggs have very consistent shapes. This raises two new issues:\n\nHow to batch-process multiple datasets?\nHow to obtain “the same” cartographic projection across multiple datasets?\n\nWe now explain how to deal with these issues using tools provided by the blender_tissue_cartography add-on and native blender. Please see also tutorials 8 and 9.\n\nRepresentation of time-lapse data\nIn blender_tissue_cartography, time-lapse datasets (i.e. movies) are represented frame-by-frame: for each timepoint, you need to provide a volumetric .tif file, and a mesh. Note: it is a good idea to give your meshes and .tif files indicative names, e.g. Time_000001_mesh.obj.\n\n\nBatch processing\nThe blender_tissue_cartography add-on allows you to batch-process multiple 3D images (for example, the frames of a movie) using the Batch processing panel:\n\n\n\nimage.png\n\n\nIt works as follows:\n\nLoad the meshes for all images you want to process into Blender. Note: each mesh needs to have a UV map. See the next section for how to obtain “consistent” UV maps across multiple meshes.\nLoad at least one of the 3D images you want to process into Blender using the “Load .tiff file” button. This file (and its BoundingBox) are used to specify image resolution and the relative position of 3D data and meshes.\nPlace the 3D images you want to process into a single directory (one .tiff file per timepoint), and select this directory via “Batch Process Input Directory”. 3D images are matched to meshes based on their file name - e.g. a mesh named “timepoint_2” in Blender will look for a .tif file called “timepoint_2.tif” or similar. Note: all 3D datasets must have the same resolution and axis order as the one you loaded in step 2.\nSpecify Normal offsets and projection resolution as above.\nSpecify an output directory via “Batch Process Output Directory”.\nSelect the BoundingBox from step 2, and all meshes you want to process, and click “Batch Process And Save”. The results are written to the output directory. Warning if you are processing a large number of files, adding projected textures to blender can result in a very large .blend file. You can deactive it using “Create materials”\n\nYou can try this out using the data provided in nbs/Tutorials/addon_example/batch_processing. It shows 3 timepoints of a lightsheet movie of the Drosophila midgut from Mitchell et al., 2022. The meshes are alreay equipped with a UV map.\n\n\nConsistent cartographic projections across multiple datasets / multiple timepoints\nIt is highly desirable to use the “same” cartographic projection across a collection of related datasets: this faciliates comparison and analysis across datasets, and avoids having to manually re-designing the UV map for each mesh in the collection. This is particularly true for time-lapse data.\nThere are, generally speaking, two types of approaches to this question:\n\nDefine a UV map separately for each mesh in the collection, but using the same procedure/algorithm each time.\nDefine a UV map for a single reference mesh, and then deform/warp the reference mesh to “fit” each mesh in the collection. The deformed mesh now fits the surface we want to extract from the 3D data but also has a UV map, allowing you to carry out cartographic projections. This approach is also known as texture transfer or mesh to mesh mapping in the literature.\n\nGetting a consistent UV map across multiple different meshes is a complicated problem (in fact, an area of active research). Which approach you should use depends on your problem. See Tutorial 9 for an in depth-explanation and sophisticated approaches. In the following, we focus on the concrete implementation and the tools provided by the add-on.\n\nAlgorithmically batch-computing UV maps\nFor surfaces with relatively simple geometries, approach 1 can work well. One very common example is a surface that can be defined by a height function over a 2D plane, without any “overhangs” (so the mesh coordinates would be something like \\((x,y, h(x,y))\\) for a height function \\(h\\)). For instance, consider the meshes in the folder nbs/Tutorials/addon_example/batch_processing/planar_meshes:\n\n\n\nimage.png\n\n\n. We can UV-project them using the “Project from view” option. Go to the UV editor, select all meshes you want to process, and navigate to a view that looks straight down on the meshes. Now click “UV-&gt;Project from view (bounds)” to get a projection. Done!\n\n\n\nimage.png\n\n\nThis approach works great for mildly curved surfaces extracted from a confocal microscope \\(z\\)-stack (see tutorial 4). You can also batch-process using blender’s Cylindrical, Cube, or Spherical UV projection operators.\n\n\nUsing reference meshes\nThe advantage of the second, reference mesh approach is that it allows you to use any custom UV map you want, potentially hand-crafted. Let’s see this in action. To “load” a custom cartographic projection and apply it to your surface of interest, you provide a reference mesh - an “idealized” version of your sample with a pre-made UV map. The mesh nbs/Tutorials/addon_example/Drosophila_reference.obj is an idealized (smothed, symmetric) Drosophila embryo with a cylindrical cartographic projection. Let’s load it into Blender:\n\n\n\nimage.png\n\n\nThe idea is now to align the reference mesh with the mesh extracted from the 3D data. Then we can use the reference mesh with its pre-made UV map to compute the cartographic projection. Alignment is done in two steps:\n\nRigid alignment. Select the reference mesh and the mesh you want to align to, and click “Align meshes”.\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nThe reference mesh will be rotated, scaled, and translated to match the data mesh:\n\n\n\nimage.png\n\n\nIf we carry out a cartographic projection using the reference mesh now, it won’t look great: the reference mesh is not a great fit to the 3D data, and so the projection has “holes”:\n\n\n\nimage.png\n\n\n\nShrink wrapping. To fix this, we now project each point on the reference mesh to the closest point on the data mesh. This is called shrink-wrapping. Click on “Modifiers” and add a “Shrink-wrap” modifier with the correct target to the reference mesh:\n\n\n\n\nimage.png\n\n\nAfter clicking “Apply” (Ctr+A), the reference mesh is wrapped and we can re-compute the cartographic projection:\n\n\n\nimage.png\n\n\nThe projected image is now uses the clean cylindrical projection defined for the reference mesh, and we can easily compare it with different recordings:\n\n\n\nimage.png\n\n\n\n\nReference mesh approach to time lapse data\nWe can also apply the reference mesh approach to time-lapse data. The blender_tissue_cartography add-on features a button to automatize the align+shrink-wrap process, which is particularly useful for time-lapse imaging. Proceed as follows:\n\nIf you don’t already have a reference mesh, designate one of your time-points as reference timepoint and create a UV map for it.\nRegister + shrink-wrap it to the remaining time-points:\n\n\n\n\nimage.png\n\n\nYou can try this with the example meshes in nbs/Tutorials/addon_example/shrinkwrap_meshes.\nThe “Shrinkwrap Corrective Smooth” option allows you to compensate for mesh distortion induced by the shrink-wrapping process. The “Shrink-wrap” mode allows you to proceed iteratively:\n\nFirst shrink-wrap the reference mesh to the first timepoint\nDuplicate the wrapped mesh, and shrink-wrap it to the second timepoint\n…\n\nThe meshes are processes in alpha-numerical order, depending on their name (so make sure you name them something like mesh_timepoint_001, mesh_timepoint_002). The “Forward” and “Backward” options allow you to start with the first or last timepoint.\nChoosing a “good” reference timepoint is important. Choose the mesh with the most complicated and/or most representative shape! You may need to define several reference timepoints if there is a lot of distortion.\nClicking the shrink-wrap button will produce two results:\n\nA mesh called XXX_wrapped. This is your reference mesh, deformed to match the shape of the target mesh and will have the exact same topology (vertices and faces) and UV map as the reference mesh\nYour target meshes will get a UV map, tranfered over from the wrapped reference mesh. The target mesh will not be altered, but the quality of the UV map (in particular its seams) may be degraded\n\nYou can then use both of these outputs for further processing.\n\nAutomatization & advanced algorithms\nIf you would like to automatize this process, or use more powerful shrink-wrapping algorithms, you will need to use the blender_tissue_cartography library. Please see tutorial 9.",
    "crumbs": [
      "Tutorials",
      "3. `blender_tissue_cartography` blender add-on"
    ]
  },
  {
    "objectID": "Tutorials/03_blender_addon_tutorial.html#next-steps",
    "href": "Tutorials/03_blender_addon_tutorial.html#next-steps",
    "title": "3. blender_tissue_cartography blender add-on",
    "section": "Next steps",
    "text": "Next steps\nTutorial 4 introduces the Python library version of blender_tissue_cartography which is useful for creating custom or automatized tissue cartography pipelines. Tutorials 5 and 6 teach you more about designing UV maps. Tutorials 8 and 9 give more information on dynamic datasets.",
    "crumbs": [
      "Tutorials",
      "3. `blender_tissue_cartography` blender add-on"
    ]
  },
  {
    "objectID": "Tutorials/04_btc_python_library.html",
    "href": "Tutorials/04_btc_python_library.html",
    "title": "4. blender_tissue_cartography Python library",
    "section": "",
    "text": "In this notebook, we go through a basic example of tissue cartography using the Python library version of blender_tissue_cartogrpahy - extracting the mildly curved surface of an epithelium from a confocal \\(z\\)-stack.\nThis example data is taken from Lye et al. 2024, available at https://www.ebi.ac.uk/biostudies/studies/S-BIAD1271.\nWe start by loading the required Python modules. We don’t need to manually copy the functions we need into each jupyter notebook.\n\n\n# these are the modules with the tissue cartography code\nfrom blender_tissue_cartography import io as tcio # for file reading and saving\nfrom blender_tissue_cartography import mesh as tcmesh # for mesh handling\nfrom blender_tissue_cartography import remesh as tcremesh # for mesh creationg\nfrom blender_tissue_cartography import interpolation as tcinterp # for cartographic projection\n\n\nimport igl\nimport numpy as np\nfrom skimage import transform\nimport os\nimport matplotlib.pyplot as plt\n\n\n# Only run this if you have installed pymeshlab\nimport pymeshlab\nfrom blender_tissue_cartography import remesh_pymeshlab as tcremesh_pymeshlab\n\nWarning:\nUnable to load the following plugins:\n\n    libio_e57.so: libio_e57.so does not seem to be a Qt Plugin.\n\nCannot load library /home/nikolas/Programs/miniconda3/envs/blender-tissue-cartography/lib/python3.11/site-packages/pymeshlab/lib/plugins/libio_e57.so: (/lib/x86_64-linux-gnu/libp11-kit.so.0: undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0)\n\n\n\n\nImportant conventions\n\nImage axis 0 is always the channel. All other axes are not permuted\nMesh coordinates are always saved in microns.\nThe UV map (the map of our surface mesh to a cartographic plane) always maps into the unit square, \\(u\\in[0,1], \\; v\\in[0,1]\\). All of our projections will be square images (with transparent regions for parts of the UV square not covered by the unwrapped mesh)\n\n\n\nLoad and subsample data for segmentation\nData description myosin + membrane ventral view of Drosophila embryo during germband extension, from Lye et al. 2024.\nWe begin by creating a directory for our project where we’ll save all related files (and normally, the jupyter notebook used to generate them!).\nLet’s load the dataset. We then enter the relevant metadata - the filename, resolution in microns, and how much we want to subsample for segmentation purposes.\n\n# start by entering the filename   \nmetadata_dict = {'filename': 'basics_example/basics_example'}\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape)\n\nimage shape: (2, 26, 454, 511)\n\n\n\nResolution info and subsampling\nFrom the image shape, we can see that the spatial axes are in \\(z-x-y\\) order. We use this information to correctly enter the resolution in microns/pixel for each axis. You can typically get this info from the .tif metadata. We then subsample the image for rapid segmentation. It is usually a good idea to make the subsampled image approximately isotropic.\n\nmetadata_dict['resolution_in_microns'] = (1, 0.36, 0.36)\nmetadata_dict['subsampling_factors'] = (1, 1/3, 1/3)\n\nWe now downscale the image. For very large volumetric images, you should use the option use_block_averaging_if_possible=True and choose subsampling factors which (a) are inverse integers like 1/2, 1/3, and (b) if possible, divide the number of pixels along each axis (so for instance 1/2 would be good for an axis with 1000 pixels, but not ideal for 1001 pixels).\n\nsubsampled_image = tcio.subsample_image(image, metadata_dict['subsampling_factors'], \n                                        use_block_averaging_if_possible=False)\nprint(\"subsampled image shape:\", subsampled_image.shape)\n\nsubsampled image shape: (2, 26, 151, 170)\n\n\n\n\n\nCreate 3d segmentation\nNow create a 3d segmentation, in this case using ilatik. We use ilastik binary pixel classification. We could post-process the ilastik output here, for example using morphsnakes. We then load the segmentation back into the jupyter notebook.\nAttention: when importing the .h5 into ilastik, make sure the dimension order is correct! In this case, CZYX for both export and import.\n\n# we now save the subsampled image a .h5 file for input into ilastik for segmentation\n\ntcio.write_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image)\n\n\n# after creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n\nsegmentation = tcio.read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # select the first channel of the segmentation - it's the probablity a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (26, 151, 170)\n\n\n\n# look at the segmentation in a cross section\n\nplt.imshow(segmentation[:,:,50], vmin=0, vmax=1)\n\n\n\n\n\n\n\n\n\n\nMeshing\nWe convert the segmentation into a triangular mesh using the marching cubes method and save the mesh. We save all meshes as wavefront .obj files (see wikipedia). In Python, we represent missing entries (such as a vertex that doesn’t have a normal by np.nan.\nImportant convention For sanity’s sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n\n# now we create a 3d mesh of using the marching cubes method\n\nvertices, faces = tcremesh.marching_cubes(segmentation, isovalue=0.5, sigma_smoothing=3)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n                                 /np.array(metadata_dict['subsampling_factors']))\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"basics_example_mesh_marching_cubes\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")\n\n\nOptional - mesh compression using igl\nThe mesh returned by the marching cubes method is normally much denser than necessary. You can automatically reduce its size here in Python, or later in blender.\n\n_, vertices_remeshed, faces_remeshed, _, _ = igl.qslim(vertices_in_microns, faces, max_m=int(faces.shape[0]/2))\n\nmesh = tcmesh.ObjMesh(vertices_remeshed, faces_remeshed)\nmesh.name = \"basics_example_mesh_marching_cubes_compressed\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes_compressed.obj\")\n\n\n\nOptional - improve mesh quality using MeshLab\nWe can remesh the output of the marching cubes algorithm to obtain an improved mesh, i.e. with more uniform triangle shapes. In this example, we first remesh to make the mesh more uniform. You can also try this out in the MeshLab GUI and export your workflow as a Python script. Be careful not to move the mesh or it will mess up the correspondence with the pixel coordinates!\nSee List of MeshLab filers\n\nmesh_remeshed = tcremesh_pymeshlab.remesh_pymeshlab(mesh)\nmesh_remeshed.write_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\n\nTo check all went well, let’s overlay the mesh coordinates over a cross section of the image. To do so, we use a simple helper function.\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\n\n\nslice_image, slice_vertices = tcinterp.get_cross_section_vertices_normals(2, 100,\n                                image, mesh, metadata_dict[\"resolution_in_microns\"],\n                                get_normals=False)\n\n\n# we need to specify the \"aspect\" because our image resolution is not isotropic\n# and use some extra arguments (like `origin=\"lower\"`) to get our plot to be correctly\n# oriented\n\nfig = plt.figure(figsize=(10,10))\nplt.scatter(*slice_vertices[:,::-1].T, s=5, c=\"tab:red\")\nplt.imshow(slice_image[0].T, origin=\"lower\",\n           aspect=(metadata_dict[\"resolution_in_microns\"][0]\n                   /metadata_dict[\"resolution_in_microns\"][2]))\n\n\n\n\n\n\n\n\n\n\n\nAutomated UV mapping by axis projection\nWe are now ready to compute a UV map for our mesh. You can do this interactively in Blender (see below) and export the results back to python, or do this in an automated fashion in Python.\nblender_tissue_cartography provides a small set of tools for automated UV mapping (see also tutorial 9). The dataset we are looking at here is an example of a simple, but very frequent case: a surface that can be represented by a “height function” without any “overhang”. This means we can create a UV map by simply projecting the vertex coordinates along one axis. The result is an improved version of a basic “z-projection” you could do in Fiji.\nThis automated UV mapping can for example be directly applied to all frames of a movie (see also tutorial 10).\n\n# first select the two axes along which you want to project (here, x and y) \naxis1 = np.array([0,1,0])\naxis2 = np.array([0,0,1])\n\n# the UV coordinates need to lie between 0 and 1, so we need to scale the projected 3D coordinates.\n# we can compute this scale from the shape of the volumetric image.\n\nscale = tcmesh.compute_project_from_axis_scale(image_shape=(26, 454, 511), resolution=metadata_dict[\"resolution_in_microns\"],\n                                               axis1=axis1, axis2=axis2)\n\nmesh_axis_projected = tcmesh.project_from_axis(mesh, axis1=axis1, axis2=axis2, scale=scale, translate=0)\nmesh_axis_projected.set_normals()\n\nmesh_axis_projected.write_obj(f\"{metadata_dict['filename']}_mesh_axis_projected.obj\")\n\n\n\nUV-mapping in blender\nWe now switch to blender and create a new empty project, which we will call f\"{metadata_dict['filename']}.blend\". We import the mesh just generated (File-&gt;Import).\n\n\n\nimage-3.png\n\n\nI recommend using the “object” tab (orange square on the right toolbar) to lock mesh position and rotation so we don’t accidentally move it.\nLet’s try to move forward and get a UV map of the mesh. To do so, we go to the “UV Editing” tab on the top toolbar, and press “3” then “A” to select all faces (“1” selects vertices, “2” edges, and “3” faces). Click “UV-&gt;unwrap” on the top panel.  For more complicated meshes (e.g. a sphere), we will need to use extra steps, e.g. define seams.\n\nSelf-intersections\nEspecially for complicated geometries, it is easy to accidentally create UV maps that self-intersect: some triangles will overlap with others. This can either happen if your UV map has multiple patches (see next tutorial), or if some triangles are flipped. For example, if you use the “project from view on the example UV cube:\n\n\n\nimage.png\n\n\nSelf-intersections are bad for cartography because they mean that the map from the plane to 3D is no longer well-defined where the self-intersection occurs. The algorithms here can handle self-intersections, but will give you a warning.\n\n\nBlender export\nWe then click on “File-&gt;Export” and save as .obj with UV and normals:\n\n\n\nimage.png\n\n\nA few things are important: - Always include UV and normals. Otherwise, cartographic projection will fail! - Only export selected items! With a more complicated blender project, you might end up exporting multiple meshes. This will trip up the cartographic projection algorithm. - Save as a triangulated mesh! The algorithms in this software package require mesh faces to be triangles. This option will subdivide any quads/polygons your mesh may have.\nThe new mesh file basics_example_mesh_uv.obj now contains vertex normals and UV coordinates.\n\n\n\nCartographic projection\nWe now read in the new .obj file to interpolate the image data onto the 3d mesh. This .obj mesh contains both the information about where the surface is located in 3d, and how it will be mapped into the plane (into the UV square). The UV grid always covers the unit square \\([0,1]^2\\).\nWe additionally specify a series of offsets in the surface normal direction to make a multilayer projection. We also get the 3d coordinates and the vertex normals interpolated onto the UV grid from this step.\n\nmesh_uv = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_uv.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n\n\n\nnormal_offsets = np.linspace(-2, 2, 5) # in microns\n# let's add the normal offset we want to our metadata - it will be important for analysis!\nmetadata_dict[\"normal_offsets\"] = normal_offsets\n\n\n# compute the cartographic projections\nprojected_data, projected_coordinates, projected_normals = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=f\"{metadata_dict['filename']}_mesh_uv.obj\", # you can either specify a path to an .obj file\n    #mesh=mesh_axis_projected, # or a ObjMesh object\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=256)\nprint(\"Image shape:\", projected_data.shape) # 0th axis is the channel, 1st axis the layer\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n\n\nImage shape: (2, 5, 256, 256)\n\n\n\n# show projected image - here let's look at channel 1 (membrane marker)\nplt.imshow(projected_data[1, 0])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\n# let's check that there are no self-intersections\n\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_uv.obj\")\n\nlen(igl.flipped_triangles(mesh.texture_vertices, mesh.texture_tris)) # looks good\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n\n\n0\n\n\n\n\nSaving the results\nWe can now save the cartographic projections both as .tif stack for quantitative analysis and as .png’s for visualization as mesh texture in blender. We will also save the metadata to a .json file\n\n# save metadatat o a human and computer readable file\ntcio.save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)\n\n\n# save projected data as .tif's for quantitative analysis\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data, z_axis=1)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_3d_coordinates.tif\", projected_coordinates)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_normals.tif\", projected_normals)\n\n\ntexture_path = f\"{metadata_dict['filename']}_textures\"\ntcio.save_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))\n\n\n# let's make a max projection of each channel and save them also\n\nmax_projected_ch_0, max_projected_ch_1 = projected_data.max(axis=1)\ntcio.imsave(f'{texture_path}/max_channel_0.png', tcio.normalize_quantiles_for_png(max_projected_ch_0))\ntcio.imsave(f'{texture_path}/max_channel_1.png', tcio.normalize_quantiles_for_png(max_projected_ch_1))\n\n\n\nVisualization in blender\nYou can set up textures in the “shading” tab:\n\n\n\nimage-3.png\n\n\nI find it helpful to remove the distracting “world” background, which you can do by either enabling “scene world” or setting “world opacity” to 0 in the viewport shading settings (arrow above “options” in the top right corner)\nGo to the bottom panel, add a new material (center top of bottom panel), and press “shift+A” to add a new shader element (the search bar is very helpful). Here is an example configuration mixing two channels as red and green:\n\n\n\nimage-4.png\n\n\nAnd there we go! Many further options exist to make more sophisticated renderings.\n\n\nNext steps\nIn the next tutorial, we will see how to UV map a more complicated shape (not topologically a disk) via seams.",
    "crumbs": [
      "Tutorials",
      "4. `blender_tissue_cartography` Python library"
    ]
  },
  {
    "objectID": "01_blender_addon.html",
    "href": "01_blender_addon.html",
    "title": "Blender add-on",
    "section": "",
    "text": "Blender add-on for loading and visualizing volumetric data into blender and projecting image intensities onto a mesh surface\n\nNote This module is not for use in a standard python environment, but must be run as an add-on within Blender. For documentation of the add-on user interface, see tutorial 3. This page documents the add-on code.\n\n\nAdd-on design\nThe add-on code comprises three parts:\n\nFunctions for carrying out key tissue cartography operations. These are based on the blender_tissue_cartography python library, edited to reflect the constraints of python scripting in blender (e.g. the igl library is not available).\nA class (inherting from bpy.types.Operator) defining each add-on button, with an execute function defining what happens when you click it.\nThe TissueCartographyPanel(Panel) class and the register function defining all user input fields and how user input fields and buttons from part 2 are laid out in the Tissue Cartography Panel.\n\nAll functions and classes are documented below.\nTo allow the user to load multiple 3D datasets and meshes into the same blender file, image data is associated with mesh objects. Which data any operation is applied to is determined by the currently selected mesh.\nThe bpy library allows the add-on to interact with blender. It is only available within blender’s python scripting interface, which is why you cannot run the add-on in a normal python interpreter. See this tutorial for an introduction into scripting Blender: https://docs.blender.org/manual/en/latest/advanced/scripting/addon_tutorial.html\nIf you want to edit/extend the add-on, be aware of the following hacks used:\n\nAssociating data with meshes: In the add-on, tissue cartography data is associated with blender meshes. For example, loaded volumetric image data (represented as a numpy array) is associated with a BoundingBox rectangular cuboid showing the volume covered by the image data (see tutorial 3). Unfortunately, blender does not allow adding arbitrary attributes to meshes. The functions set_numpy_attribute/get_numpy_attribute circumvent this by representing array data as binary buffer + shape. To associate functions with a mesh (e.g. interpolators), a global dictionary is used.\nUV layout: To obtain the part of the UV square covered by an unwrapped mesh, a .png of the layout is exported to disk and re-read.\n\nThe add-on makes use of the following libraries which are included with the add-ons using wheels: numpy, tifffile, scipy, skimage.\n\n\nAdd-on code\nThe code below is shown for completeness of the documentation webpage. Please download the add-on code from GitHub here.\n\nbl_info = {\n    \"name\": \"Tissue Cartography\",\n    \"blender\": (4, 3, 0),\n    \"category\": \"Scene\",\n}\n\nimport bpy\nfrom bpy.props import StringProperty, FloatProperty, FloatVectorProperty, IntProperty, IntVectorProperty, BoolProperty, EnumProperty\nfrom bpy.types import Operator, Panel\nimport mathutils\nimport bmesh\n\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport difflib\nimport itertools\nimport subprocess\nimport sys\n\nimport tifffile\nfrom scipy import interpolate, ndimage, spatial, stats, linalg\nfrom skimage import measure\n\n\n### Installing dependencies\n\ndef install_dependencies():\n    try:\n        import scipy\n        import skimage\n    except ImportError:\n        python_executable = sys.executable\n        subprocess.check_call([python_executable, \"-m\", \"pip\", \"install\", \"scipy\", \"scikit-image\", \"tifffile\"])\n\n\n### I/O and image handling\n\n\ndef load_png(image_path):\n    \"\"\"Load .png into numpy array.\"\"\"\n    image = bpy.data.images.load(image_path)\n    width, height = image.size\n    pixels = np.array(image.pixels[:], dtype=np.float32)\n    return pixels.reshape((height, width, -1))\n\n\ndef normalize_quantiles(image, quantiles=(0.01, 0.99), channel_axis=None, clip=False,\n                        data_type=None):\n    \"\"\"\n    Normalize a multi-dimensional image by setting given quantiles to 0 and 1.\n    \n    Parameters\n    ----------\n    image : np.array\n        Multi-dimensional image.\n    quantiles : tuple\n        Image quantile to set to 0 and 1.\n    channel_axis : int or None\n        If None, the image is assumed to have only a single channel.\n        If int, indicates the position of the channel axis. \n        Each channel is normalized separately.\n    clip : bool\n        Whether to clip image to 0-1. Automatically enabled if converting to int dtype.\n    data_type : None, np.unit8 or np.uint16\n        If not None, image is converted to give data type.\n    \n    Returns\n    -------\n    image_normalized : np.array\n        Normalized image, the same shape as input\n    \"\"\"\n    if channel_axis is None:\n        image_normalized = image - np.nanquantile(image, quantiles[0])\n        image_normalized /= np.nanquantile(image_normalized, quantiles[1])\n        image_normalized = np.nan_to_num(image_normalized)\n    else:\n        image_normalized = np.moveaxis(image, channel_axis, 0)\n        image_normalized = np.stack([ch - np.nanquantile(ch, quantiles[0]) for ch in image_normalized])\n        image_normalized = np.stack([ch / np.nanquantile(ch, quantiles[1]) for ch in image_normalized])\n        image_normalized = np.moveaxis(np.nan_to_num(image_normalized), 0, channel_axis)\n    if clip or (data_type is not None):\n        image_normalized = np.clip(image_normalized, 0, 1)\n    if data_type is np.uint8:\n        image_normalized = np.round((2**8-1)*image_normalized).astype(np.uint8)\n    if data_type is np.uint16:\n        image_normalized = np.round((2**16-1)*image_normalized).astype(np.uint16)\n    return image_normalized\n\n\n\ndef axis_order_to_transpose(axis_order_string):\n    \"\"\"Convert string describing axis order into tuple for use in np.transpose.\"\"\"\n    assert ''.join(sorted(axis_order_string)) in ['xyz', 'cxyz'], \"Must be xyz, cxyz, or permutation thereof\"\n    if 'c' in axis_order_string:\n        transpose = [axis_order_string.index(k) for k in 'cxyz']\n    else:\n        transpose = [axis_order_string.index(k) for k in 'xyz']\n    return transpose\n\n\n### Tissue cartography - projecting 3d images to UV textures\n\n\ndef get_uv_layout(obj, uv_layout_path, image_resolution):\n    \"\"\"Get UV layout mask for obj object as a np.array. As a side effect, saves layout to disk and deselects everything except obj.\"\"\"\n    if os.path.exists(uv_layout_path):\n        os.remove(uv_layout_path)\n\n    bpy.ops.object.select_all(action='DESELECT')  # Deselect all objects\n    obj.select_set(True)  # Select the specific object\n    bpy.context.view_layer.objects.active = obj\n    bpy.ops.object.mode_set(mode='EDIT')\n    \n    # Set all faces to selected for the UV layout\n    mesh = bmesh.from_edit_mesh(obj.data)\n    for face in mesh.faces:\n        face.select = True\n    bmesh.update_edit_mesh(obj.data)\n    \n    bpy.ops.uv.export_layout(filepath=uv_layout_path, size=(image_resolution, image_resolution), opacity=1, export_all=False, check_existing=False)\n    bpy.ops.object.mode_set(mode='OBJECT')\n    UV_layout = load_png(uv_layout_path)\n    \n    return (UV_layout.sum(axis=-1) &gt; 0)[::-1]\n\n\ndef get_uv_normal_world_per_loop(mesh_obj, filter_unique=False):\n    \"\"\"\n    Get UV, normals, and world and normal for each loop (half-edge) as np.array.\n    \n    If filter_unique, remove \"duplicate\" loops (for which UV, normals and position\n    are identical).\n    \"\"\"\n    if not mesh_obj:\n        raise TypeError(\"No object selected\")\n    if mesh_obj.type != 'MESH':\n        raise TypeError(\"Selected object is not a mesh\")\n    world_matrix = mesh_obj.matrix_world\n    uv_layer = mesh_obj.data.uv_layers.active\n    if not uv_layer:\n        raise RuntimeError(\"Mesh does not have an active UV map\")\n    loop_uvs = np.zeros((len(mesh_obj.data.loops), 2), dtype=np.float32)\n    loop_normals = np.zeros((len(mesh_obj.data.loops), 3), dtype=np.float32)\n    loop_world_positions = np.zeros((len(mesh_obj.data.loops), 3), dtype=np.float32)\n    for loop in mesh_obj.data.loops:\n        loop_uvs[loop.index] = uv_layer.data[loop.index].uv\n        loop_normals[loop.index] = world_matrix.to_3x3() @ mesh_obj.data.vertices[loop.vertex_index].normal\n        loop_world_positions[loop.index] = world_matrix @ mesh_obj.data.vertices[loop.vertex_index].co\n    if filter_unique:\n        unqiue_loops = np.unique(np.hstack([loop_uvs, loop_normals, loop_world_positions]), axis=0)\n        loop_uvs, loop_normals, loop_world_positions = (unqiue_loops[:,:2], unqiue_loops[:,2:5], unqiue_loops[:,5:])\n    loop_normals = np.round((loop_normals.T/np.linalg.norm(loop_normals, axis=1)).T, decimals=4)\n    return loop_uvs, loop_normals, loop_world_positions\n\n\ndef bake_per_loop_values_to_uv(loop_uvs, loop_values, image_resolution):\n    \"\"\"\n    Bake (interpolate) values (normals or world position) defined per loop into the UV square.\n    \n    UV coordinates outside [0,1] are ignored.\n    \n    Parameters\n    ----------\n    loop_uvs : np.array of shape (n_loops, 2)\n        UV coordinates of loop.\n    loop_values : np.array of shape (n_loops, ...)\n        Input field. Can be an array with any number of axes (e.g. scalar or vector field).\n    image_resolution : int, default 256\n        Size of UV grid. Determines resolution of result.\n\n    Returns\n    -------\n    \n    interpolated : np.array of shape (uv_grid_steps, uv_grid_steps, ...)\n        Field across [0,1]**2 UV grid, with a uniform step size. UV positions that don't\n        correspond to any value are set to np.nan.\n            \n    \"\"\"\n    U, V = np.meshgrid(*(2*(np.linspace(0,1, image_resolution),)))\n    interpolated = interpolate.griddata(loop_uvs, loop_values, (U, V), method='linear')[::-1]\n    return interpolated\n\n\ndef bake_volumetric_data_to_uv(image, baked_world_positions, resolution, baked_normals, normal_offsets=(0,), affine_matrix=None):\n    \"\"\" \n    Interpolate volumetric image data onto UV coordinate grid.\n    \n    Uses baked 3d world positions corresponding to each UV grid point (see bake_per_loop_values_to_UV).\n    3d coordinates (in microns) are converted into image coordinates via the resolution scaling factor.\n    The resolution of the bake (number of pixels) is determined by the shape of baked_world_positions.\n    \n    normal_offsets moves the 3d positions whose volumetric voxel values will be baked inwards or outwards\n    along the surface normal. Providing a list of offsets results in a multi-layer pullback\n    \n    Parameters\n    ----------\n    image : 4d np.array\n        Image, axis 0  is assumed to be the channel axis\n    baked_world_positions : np.array of shape (image_resolution, image_resolution, uv_grid_steps, 3)\n        3d world positions baked to UV grid, with uniform step size. UV positions that don't correspond to \n        any value are set to np.nan.\n    resolution : np.array of shape (3,)\n        Resolution in pixels/microns for each of the three spatial axes.\n    baked_normals : np.array of shape (image_resolution, image_resolution, uv_grid_steps, 3)\n        3d world normals baked to UV grid, with uniform step size. UV positions that don't correspond to \n        any value are set to np.nan.\n    normal_offsets : np.array of shape (n_layers,), default (0,)\n        Offsets along normal direction, in same units as interpolated_3d_positions (i.e. microns).\n        0 corresponds to no shift.\n    affine_matrix : np.array of shape (4, 4) or None\n        If not None, transform coordinates by affine trafor before calling interpolator\n        \n    Returns\n    -------\n    aked_data : np.array of shape (n_channels, n_layers, image_resolution, image_resolution)\n        Multi-layer 3d volumetric data baked onto UV.\n    \"\"\"\n    x, y, z = [np.arange(ni) for ni in image.shape[1:]]\n    baked_data = []\n    for o in normal_offsets:\n        position = (baked_world_positions+o*baked_normals)\n        if affine_matrix is not None:\n            position = position @ affine_matrix[:3, :3].T + affine_matrix[:3,3]\n        position =  position/resolution\n        baked_layer_data = np.stack([interpolate.interpn((x, y, z), channel, position,\n                                     method=\"linear\", bounds_error=False) for channel in image])\n        baked_data.append(baked_layer_data)\n    baked_data = np.stack(baked_data, axis=1)\n    return baked_data\n\n\n### Bounding box and orthoslices for visualizing the 3d data\n\n\ndef create_box(length, width, height, name=\"RectangularBox\", hide=True):\n    \"\"\"\n    Creates a rectangular box using Blender's default cube.\n    One corner is positioned at the origin, and the box lies in the positive x/y/z quadrant.\n\n    Args:\n        length (float): Length of the box along the X-axis.\n        width (float): Width of the box along the Y-axis.\n        height (float): Height of the box along the Z-axis.\n    \"\"\"\n    # Store the current active object\n    current_active = bpy.context.active_object\n\n    bpy.ops.mesh.primitive_cube_add(size=2, location=(0, 0, 0))\n    obj = bpy.context.active_object\n    obj.name = name\n    obj.scale = (length / 2, width / 2, height / 2)\n    obj.location = (length / 2, width / 2, height / 2)\n    bpy.ops.object.transform_apply(location=True, scale=True)\n    obj.hide_set(hide)\n    # re-select the currently active object\n    if current_active:\n        bpy.context.view_layer.objects.active = current_active\n    return obj\n\n\ndef create_slice_plane(length, width, height, axis='z', position=0.0):\n    \"\"\"\n    Creates a 2D plane as a slice of a rectangular box along a specified axis.\n    The plane lies within the bounds of the box.\n\n    Args:\n        length (float): Length of the box along the X-axis.\n        width (float): Width of the box along the Y-axis.\n        height (float): Height of the box along the Z-axis.\n        axis (str): Axis along which to slice ('x', 'y', or 'z').\n        position (float): Position along the chosen axis for the slice plane.\n                          Should be within the range of the box dimensions.\n    \"\"\"\n    current_active = bpy.context.active_object\n    # Validate axis and position\n    if axis not in {'x', 'y', 'z'}:\n        raise ValueError(\"Axis must be 'x', 'y', or 'z'.\")\n    \n    axis_limits = {'x': length, 'y': width, 'z': height}\n    if not (0.0 &lt;= position &lt;= axis_limits[axis]):\n        raise ValueError(f\"Position must be within [0, {axis_limits[axis]}] for axis {axis}.\")\n\n    # Create the plane's dimensions based on the slicing axis\n    if axis == 'x':\n        plane_size = (height, width) #(width, height)\n        location =  (position, width / 2, height / 2)\n        rotation = (0, 1.5708, 0)  # Rotate to align with the YZ-plane\n    elif axis == 'y':\n        plane_size = (length, height)\n        location = (length / 2, position, height / 2)\n        rotation = (1.5708, 0, 0)  # Rotate to align with the XZ-plane\n    else:  # 'z'\n        plane_size = (length, width)\n        location = (length / 2, width / 2, position)\n        rotation = (0, 0, 0)  # No rotation needed for the XY-plane\n\n    # Add a plane\n    bpy.ops.mesh.primitive_plane_add(size=2, location=(0, 0, 0))\n    plane = bpy.context.active_object\n    plane.name = f\"SlicePlane_{axis.upper()}_{position:.2f}\"\n\n    # Scale and position the plane\n    plane.scale = (plane_size[0] / 2, plane_size[1] / 2, 1)\n    plane.location = location\n    plane.rotation_euler = rotation\n\n    # Apply transformations (scale, location, rotation)\n    bpy.ops.object.transform_apply(location=True, scale=True, rotation=True)\n\n    # Restore the previously active object\n    if current_active:\n        bpy.context.view_layer.objects.active = current_active\n\n    return plane\n\n\ndef get_slice_image(image_3d, resolution, axis='z', position=0.0):\n    \"\"\"Get slice of 3d image along axis for ortho-slice visualization.\n    image_3d must be a 4d array (channels, x, y, z). Position in microns.\"\"\"\n    if axis == 'x':\n        ind = int(np.round(position / resolution[0]))\n        slice_img = image_3d[:,ind,:,::-1]\n    elif axis == 'y':\n        ind = int(np.round(position / resolution[1]))\n        slice_img = image_3d[:,:,ind,:].transpose((0,2,1))\n    elif axis == 'z': \n        ind = int(np.round(position / resolution[0]))\n        slice_img = image_3d[:,:,:,ind].transpose((0,2,1))\n    return slice_img\n\n\ndef create_material_from_array(slice_plane, array, material_name=\"SliceMaterial\"):\n    \"\"\"\n    Creates a material for a ortho-slice plane using a 2D numpy array as a texture.\n\n    Args:\n        slice_plane (bpy.types.Object): The plane object to which the material will be applied.\n        array (numpy.ndarray): 2D array representing grayscale values (0-1), or 3D array representing RGBA values (0-1).\n        material_name (str): Name of the new material.\n    \"\"\"\n    # Validate input array\n    if not len(array.shape) in [2,3]:\n        raise ValueError(\"Input array must be 2D.\")\n    \n    # Normalize array to range [0, 1] and convert to a flat list\n    image_height, image_width = array.shape[:2]\n    pixel_data = np.zeros((image_height, image_width, 4), dtype=np.float32)  # RGBA\n    if len(array.shape) == 2:\n        pixel_data[..., 0] =  pixel_data[..., 1] = pixel_data[..., 2] = array\n        pixel_data[..., 3] = 1.0  # Alpha\n    else:\n        pixel_data[...] = array\n    pixel_data = pixel_data.flatten()\n\n    # Create a new image in Blender\n    image = bpy.data.images.new(name=\"SliceTexture\", width=image_width, height=image_height)\n    image.pixels = pixel_data.tolist()\n\n    # Create a new material\n    material = bpy.data.materials.new(name=material_name)\n    material.use_nodes = True\n    nodes = material.node_tree.nodes\n    links = material.node_tree.links\n\n    # Clear default nodesx\n    for node in nodes:\n        nodes.remove(node)\n\n    # Add required nodes\n    texture_node = nodes.new(type=\"ShaderNodeTexImage\")\n    texture_node.image = image\n    bsdf_node = nodes.new(type=\"ShaderNodeBsdfPrincipled\")\n    output_node = nodes.new(type=\"ShaderNodeOutputMaterial\")\n\n    # Arrange nodes\n    texture_node.location = (-400, 0)\n    bsdf_node.location = (0, 0)\n    output_node.location = (400, 0)\n\n    # Connect nodes\n    links.new(texture_node.outputs[\"Color\"], bsdf_node.inputs[\"Base Color\"])\n    links.new(bsdf_node.outputs[\"BSDF\"], output_node.inputs[\"Surface\"])\n\n    # Assign the material to the plane\n    slice_plane.active_material = material\n    return None\n\n\n### Pullback shading\n\n\ndef create_material_from_multilayer_array(mesh, array, material_name=\"ProjectedMaterial\"):\n    \"\"\"\n    Creates a material for a mesh using multi-channel, multi-layer projection.\n\n    Args:\n        obj (bpy.types.Object): The mesh object to which the material will be applied.\n        array (numpy.ndarray): 4D array of shape (channels, layers, U, V)\n        material_name (str): Name of the new material.\n    \"\"\"\n    # Validate and normalize input array\n    if not len(array.shape) == 4:\n        raise ValueError(\"Input array must have 4 axes.\")\n    array_normalized = normalize_quantiles(array, quantiles=(0.01, 0.99), channel_axis=0,\n                                           clip=True, data_type=None)\n    # Create a new image in Blender for each layer and channel\n    image_height, image_width = array.shape[-2:]\n    n_channels, n_layers = array.shape[:2]\n    images = {}\n    for ic, chanel in enumerate(array_normalized):\n        for il, layer in enumerate(chanel):\n            pixel_data = np.zeros((image_height, image_width, 4), dtype=np.float32)\n            pixel_data[..., 0] =  pixel_data[..., 1] = pixel_data[..., 2] = layer[::-1]\n            pixel_data[..., 3] = 1.0  # Alpha\n            pixel_data = pixel_data.flatten()\n            images[(ic, il)] = bpy.data.images.new(name=f\"Channel_{ic}_Layer_{il}\",\n                                                   width=image_width, height=image_height)\n            images[(ic, il)].pixels = pixel_data.tolist()\n    # Create a new material\n    material = bpy.data.materials.new(name=material_name)\n    material.use_nodes = True\n    nodes = material.node_tree.nodes\n    links = material.node_tree.links\n    # Clear default nodesx\n    for node in nodes:\n        nodes.remove(node)\n    # Add required nodes\n    texture_nodes = {}\n    for (ic, il), image in images.items():\n        texture_nodes[(ic, il)] = nodes.new(type=\"ShaderNodeTexImage\")\n        texture_nodes[(ic, il)].image = image\n        texture_nodes[(ic, il)].location = (-400, ic*400 + il*300)\n    \n    bsdf_node = nodes.new(type=\"ShaderNodeBsdfPrincipled\")\n    output_node = nodes.new(type=\"ShaderNodeOutputMaterial\")\n\n    # Arrange nodes\n    bsdf_node.location = (0, 0)\n    output_node.location = (400, 0)\n\n    # Connect nodes\n    links.new(texture_nodes[(0,0)].outputs[\"Color\"], bsdf_node.inputs[\"Base Color\"])\n    links.new(bsdf_node.outputs[\"BSDF\"], output_node.inputs[\"Surface\"])\n\n    # Assign the material to the mesh\n    mesh.active_material = material\n    return None\n\n\n### Vertex shading\n\n\ndef compute_edge_lengths(obj):\n    \"\"\"\n    Computes the lengths of all edges in a mesh object as a numpy array.\n\n    Args:\n        obj (bpy.types.Object): The mesh object to compute edge lengths for.\n\n    Returns:\n        numpy.ndarray: A 1D array containing the lengths of all edges in the mesh.\n    \"\"\"\n    # Ensure the object is a mesh\n    if obj.type != 'MESH':\n        raise ValueError(\"The selected object is not a mesh.\")\n    # Ensure the mesh is in edit mode for accurate vertex data\n    bpy.context.view_layer.objects.active = obj\n    if obj.mode != 'OBJECT':\n        bpy.ops.object.mode_set(mode='OBJECT')\n    edge_lengths = []\n    for edge in obj.data.edges:\n        v1 = obj.data.vertices[edge.vertices[0]].co\n        v2 = obj.data.vertices[edge.vertices[1]].co\n        edge_lengths.append((v1 - v2).length)\n    return np.array(edge_lengths)\n\n\ndef get_image_to_vertex_interpolator(obj, image_3d, resolution_array, quantiles=(0.01, 0.99)):\n    \"\"\"\n    Get interpolator that maps vertex position -&gt; image intensity.\n    \n    Returns a list of interpolators, one for each channel.\n    To avoid aliasing, the 3d image is smoothed with\n    sigma=median edge length /2. The image data is also normalized to\n    range from 0-1 using the provided quantiles.\n    \"\"\"\n    anti_aliasing_scale = np.median(compute_edge_lengths(obj))/2\n    image_3d_smoothed = np.stack([ndimage.gaussian_filter(ch, anti_aliasing_scale/resolution_array)\n                                  for ch in image_3d])\n    image_3d_smoothed = normalize_quantiles(image_3d_smoothed,\n                                            quantiles=quantiles, clip=True, data_type=None)\n    x, y, z = [np.arange(ni)*resolution_array[i]\n               for i, ni in enumerate(image_3d.shape[1:])]\n    \n    return [interpolate.RegularGridInterpolator((x,y,z), ch, method='linear', bounds_error=False)\n            for ch in image_3d_smoothed]\n\n\ndef assign_vertex_colors(obj, colors):\n    \"\"\"\n    Assigns an RGB color to each vertex in the given object.\n    Args:\n        obj: The mesh object.\n        colors: A list or dict of (R, G, B) tuples for each vertex.\n    \"\"\"\n    if obj.type != 'MESH':\n        print(\"Object is not a mesh!\")\n        return\n    if not obj.data.vertex_colors:\n        obj.data.vertex_colors.new()\n    color_layer = obj.data.vertex_colors.active\n    # Assign colors to each loop (face corner)\n    for loop in obj.data.loops:    \n        color_layer.data[loop.index].color = (*colors[loop.vertex_index], 1.0)  # RGBA\n    return None\n\n\ndef create_vertex_color_material(object, material_name=\"VertexColorMaterial\"):\n    \"\"\"\n    Creates a material for an object that uses vertex colors.\n    The R, G, and B channels are processed through separate \"Map Range\" nodes\n    to edit their brightness, and then combined into a Principled BSDF.\n\n    Args:\n        object (bpy.types.Object): The object to which the material will be applied.\n        material_name (str): Name of the new material.\n    \"\"\"\n    # Ensure the object has a vertex color layer\n    if not object.data.vertex_colors:\n        raise ValueError(\"The object has no vertex color layers.\")\n\n    # Create a new material\n    material = bpy.data.materials.new(name=material_name)\n    material.use_nodes = True\n    nodes = material.node_tree.nodes\n    links = material.node_tree.links\n\n    # Clear default nodes\n    for node in nodes:\n        nodes.remove(node)\n\n    # Add nodes\n    vertex_color_node = nodes.new(type=\"ShaderNodeVertexColor\")\n    vertex_color_node.layer_name = object.data.vertex_colors[0].name\n    vertex_color_node.location = (-1000, 0)\n\n    separate_color_node = nodes.new(type=\"ShaderNodeSeparateRGB\")\n    separate_color_node.location = (-800, 0)\n\n    map_range_r = nodes.new(type=\"ShaderNodeMapRange\")\n    map_range_r.label = \"Map Range R\"\n    map_range_r.location = (-600, 300)\n\n    map_range_g = nodes.new(type=\"ShaderNodeMapRange\")\n    map_range_g.label = \"Map Range G\"\n    map_range_g.location = (-600, 0)\n\n    map_range_b = nodes.new(type=\"ShaderNodeMapRange\")\n    map_range_b.label = \"Map Range B\"\n    map_range_b.location = (-600, -300)\n\n    combine_rgb = nodes.new(type=\"ShaderNodeCombineRGB\")\n    combine_rgb.location = (-200, 0)\n\n    bsdf_node = nodes.new(type=\"ShaderNodeBsdfPrincipled\")\n    bsdf_node.location = (000, 0)\n\n    output_node = nodes.new(type=\"ShaderNodeOutputMaterial\")\n    output_node.location = (400, 0)\n\n    # Connect nodes\n    links.new(vertex_color_node.outputs[\"Color\"], separate_color_node.inputs[\"Image\"])\n    links.new(separate_color_node.outputs[\"R\"], map_range_r.inputs[\"Value\"])\n    links.new(separate_color_node.outputs[\"G\"], map_range_g.inputs[\"Value\"])\n    links.new(separate_color_node.outputs[\"B\"], map_range_b.inputs[\"Value\"])\n\n    links.new(map_range_r.outputs[\"Result\"], combine_rgb.inputs[\"R\"])\n    links.new(map_range_g.outputs[\"Result\"], combine_rgb.inputs[\"G\"])\n    links.new(map_range_b.outputs[\"Result\"], combine_rgb.inputs[\"B\"])\n\n    links.new(combine_rgb.outputs[\"Image\"], bsdf_node.inputs[\"Base Color\"])\n    links.new(bsdf_node.outputs[\"BSDF\"], output_node.inputs[\"Surface\"])\n\n    # Set default map range values for each channel\n    for map_range_node in [map_range_r, map_range_g, map_range_b]:\n        map_range_node.inputs[\"From Min\"].default_value = 0.0\n        map_range_node.inputs[\"From Max\"].default_value = 1.0\n        map_range_node.inputs[\"To Min\"].default_value = 0.0\n        map_range_node.inputs[\"To Max\"].default_value = 1.0\n\n    # Assign the material to the object\n    object.active_material = material\n    return None\n\n\n### Marching cubes\n\n\ndef create_mesh_from_numpy(name, verts, faces):\n    \"\"\"\n    Creates a Blender mesh object from NumPy arrays of vertices and faces.\n    \n    :param name: Name of the new mesh object.\n    :param verts: NumPy array of shape (n, 3) containing vertex coordinates.\n    :param faces: NumPy array of shape (m, 3 or 4) containing face indices.\n    :return: The created mesh object.\n    \"\"\"\n    mesh = bpy.data.meshes.new(name)\n    obj = bpy.data.objects.new(name, mesh)\n    # Link the object to the scene\n    bpy.context.collection.objects.link(obj)\n    mesh.from_pydata(verts.tolist(), [], faces.tolist())\n    mesh.update()\n    return obj\n\n\n### Iterative closest point alignment\n\n\ndef package_affine_transformation(matrix, vector):\n    \"\"\"Package matrix transformation & translation into (d+1,d+1) matrix representation of affine transformation.\"\"\"\n    matrix_rep = np.hstack([matrix, vector[:, np.newaxis]])\n    matrix_rep = np.pad(matrix_rep, ((0,1),(0,0)), constant_values=0)\n    matrix_rep[-1,-1] = 1\n    return matrix_rep\n\n\ndef get_inertia(pts):\n    \"\"\"Get inertia tensor of 3d point cloud.\"\"\"\n    pts_nomean = pts - np.mean(pts, axis=0)\n    x, y, z = pts_nomean.T\n    Ixx = np.mean(x**2)\n    Ixy = np.mean(x*y)\n    Ixz = np.mean(x*z)\n    Iyy = np.mean(y**2)\n    Iyz = np.mean(y*z)\n    Izz = np.mean(z*z)\n    return np.array([[Ixx, Ixy, Ixz], [Ixy,Iyy, Iyz], [Ixz, Iyz, Izz]])\n\n\ndef align_by_centroid_and_intertia(source, target, scale=True, shear=True, improper=False, n_samples=10000):\n    \"\"\"\n    Align source point cloud to target point cloud using affine transformation.\n    \n    Align by matching centroids and axes of inertia tensor. Since the inertia tensor is invariant\n    under reflections along its principal axes, all 2^3 reflections are tried and the one leading\n    to the best agreement with the target is chosen.\n    \n    Parameters\n    ----------\n    source : np.array of shape (n_source, 3)\n        Point cloud to be aligned.\n    target : np.array of shape (n_target, 3)\n        Point cloud to align to.\n    scale : bool, default True\n        Whether to allow scale transformation (True) or rotations only (False)\n    shear : bool, default False\n        Whether to allow shear transformation (True) or rotations/scale only (False)\n    improper : bool, default False\n        Whether to allow transfomations with determinant -1\n    n_samples : int, optional\n        Number of samples of source to use when estimating distances.\n\n\n    Returns\n    -------\n    np.array, np.array\n        affine_matrix_rep : np.array of shape (4, 4)\n            Affine transformation source -&gt; target\n        aligned : np.array of shape (n_source, 3)\n            Aligned coordinates\n    \"\"\"\n    target_centroid = np.mean(target, axis=0)\n    target_inertia = get_inertia(target)\n    target_eig = np.linalg.eigh(target_inertia)\n\n    source_centroid = np.mean(source, axis=0)\n    source_inertia = get_inertia(source)\n    source_eig = np.linalg.eigh(source_inertia)\n\n    flips = [np.diag([i,j,k]) for i, j, k in itertools.product(*(3*[[-1,1]]))]\n    trafo_matrix_candidates = []\n    tree = spatial.cKDTree(target)\n    samples = source[np.random.randint(low=0, high=source.shape[0], size=min([n_samples, source.shape[0]])),:]\n    distances = []\n    for flip in flips:\n        if shear:\n            trafo_matrix = (source_eig.eigenvectors\n                            @ np.diag(np.sqrt(target_eig.eigenvalues/source_eig.eigenvalues))\n                            @ flip @ target_eig.eigenvectors.T)\n        elif scale and not shear:\n            scale_fact = np.sqrt(stats.gmean(target_eig.eigenvalues)/stats.gmean(source_eig.eigenvalues))\n            trafo_matrix = scale_fact*source_eig.eigenvectors@flip@target_eig.eigenvectors.T\n        elif not scale and not shear:\n            trafo_matrix = source_eig.eigenvectors@flip@target_eig.eigenvectors.T\n        if not improper and np.linalg.det(trafo_matrix) &lt; 0:\n            continue\n        trafo_matrix = trafo_matrix.T\n        trafo_matrix_candidates.append(trafo_matrix)\n        trafo_translate = target_centroid - trafo_matrix@source_centroid\n        aligned = samples@trafo_matrix.T + trafo_translate\n        distances.append(np.mean(tree.query(aligned)[0]))\n    trafo_matrix = trafo_matrix_candidates[np.argmin(distances)]\n    print('inferred rotation/scale', trafo_matrix)\n    trafo_translate = target_centroid - trafo_matrix@source_centroid\n    aligned = source@trafo_matrix.T + trafo_translate\n    affine_matrix_rep = package_affine_transformation(trafo_matrix, trafo_translate)\n    \n    print('inferred translation', trafo_translate)\n    return affine_matrix_rep, aligned\n\n\ndef procrustes(source, target, scale=True):\n    \"\"\"\n    Procrustes analysis, a similarity test for two data sets.\n\n    Copied from scipy.spatial.procrustes, modified to return the transform\n    as an affine matrix, and return the transformed source data in the original,\n    non-normalized coordinates.\n\n    Each input matrix is a set of points or vectors (the rows of the matrix).\n    The dimension of the space is the number of columns of each matrix. Given\n    two identically sized matrices, procrustes standardizes both such that:\n\n    - tr(AA^T) = 1.\n    - Both sets of points are centered around the origin.\n\n    Procrustes then applies the optimal transform to the source matrix\n    (including scaling/dilation, rotations, and reflections) to minimize the\n    sum of the squares of the pointwise differences between the two input datasets.\n\n    This function is not designed to handle datasets with different numbers of\n    datapoints (rows).  If two data sets have different dimensionality\n    (different number of columns), simply add columns of zeros to the smaller\n    of the two.\n    \n\n    Parameters\n    ----------\n    source : array_like\n        Matrix, n rows represent points in k (columns) space. The data from\n        source will be transformed to fit the pattern in target.\n    target : array_like\n        Maxtrix, n rows represent points in k (columns) space. \n        target is the reference data. \n    scale : bool, default True\n        Whether to allow scaling transformations\n\n    Returns\n    -------\n    trafo_affine : array_like\n        (4,4) array representing the affine transformation from source to target.\n    aligned : array_like\n        The orientation of source that best fits target.\n    disparity : float\n        np.linalg.norm(aligned-target, axis=1).mean()\n    \"\"\"\n    mtx1 = np.array(target, dtype=np.float64, copy=True)\n    mtx2 = np.array(source, dtype=np.float64, copy=True)\n\n    if mtx1.ndim != 2 or mtx2.ndim != 2:\n        raise ValueError(\"Input matrices must be two-dimensional\")\n    if mtx1.shape != mtx2.shape:\n        raise ValueError(\"Input matrices must be of same shape\")\n    if mtx1.size == 0:\n        raise ValueError(\"Input matrices must be &gt;0 rows and &gt;0 cols\")\n\n    # translate all the data to the origin\n    centroid1, centroid2 = (np.mean(mtx1, 0), np.mean(mtx2, 0))\n    mtx1 -= centroid1\n    mtx2 -= centroid2\n\n    # change scaling of data (in rows) such that trace(mtx*mtx') = 1\n    norm1 = np.linalg.norm(mtx1)\n    norm2 = np.linalg.norm(mtx2)\n    if norm1 == 0 or norm2 == 0:\n        raise ValueError(\"Input matrices must contain &gt;1 unique points\")\n    mtx1 /= norm1\n    mtx2 /= norm2\n    # transform mtx2 to minimize disparity\n    R, s = linalg.orthogonal_procrustes(mtx1, mtx2)\n    mtx2 = np.dot(mtx2, R.T) * s\n\n    # retranslate and scale\n    aligned = norm1 * mtx2 + centroid1\n\n    # measure the dissimilarity between the two datasets\n    disparity = np.mean(np.linalg.norm(aligned-target, axis=1))\n\n    # assemble the linear transformation\n    if scale:\n        trafo_matrix = (norm1/norm2)*s*R\n    else:\n        trafo_matrix = (norm1/norm2)*R\n    trafo_translate = centroid1 - trafo_matrix@centroid2\n    trafo_affine = package_affine_transformation(trafo_matrix, trafo_translate)\n    return trafo_affine, aligned, disparity\n\n\ndef icp(source, target, initial=None, threshold=1e-4, max_iterations=20, scale=True, n_samples=1000):\n    \"\"\"\n    Apply the iterative closest point algorithm to align point cloud a with\n    point cloud b. Will only produce reasonable results if the\n    initial transformation is roughly correct. Initial transformation can be\n    found by applying Procrustes' analysis to a suitable set of landmark\n    points (often picked manually), or by inertia+centroid based alignment,\n    implemented in align_by_centroid_and_intertia.\n\n    Parameters\n    ----------\n    source : (n,3) float\n      Source points in space.\n    target : (m,3) float or Trimesh\n      Target points in space or mesh.\n    initial : (4,4) float\n      Initial transformation.\n    threshold : float\n      Stop when change in cost is less than threshold\n    max_iterations : int\n      Maximum number of iterations\n    scale : bool, optional\n      Whether to allow dilations. If False, orthogonal procrustes is used\n    n_samples : int or None\n        If not None, n_samples sample points are randomly chosen from source array for distance computation\n    \n    Returns\n    ----------\n    matrix : (4,4) float\n      The transformation matrix sending a to b\n    transformed : (n,3) float\n      The image of a under the transformation\n    cost : float\n      The cost of the transformation\n    \"\"\"\n    # initialize transform matrix\n    total_matrix = np.eye(4) if initial is None else initial\n    tree = spatial.cKDTree(target)\n    # subsample and apply initial transformation\n    samples = (source[np.random.randint(low=0, high=source.shape[0],\n                                        size=min([n_samples, source.shape[0]])),:]\n               if n_samples is not None else source[:])\n    samples = samples@total_matrix[:3,:3].T + total_matrix[:3,-1]\n    # start with infinite cost\n    old_cost = np.inf\n    # avoid looping forever by capping iterations\n    for i in range(max_iterations):\n        print('iteration', i, 'cost', old_cost) \n        # Find closest point in target to each point in sample and align\n        closest = target[tree.query(samples, 1)[1]]\n        matrix, samples, cost = procrustes(samples, closest, scale=scale)\n        # update a with our new transformed points\n        total_matrix = np.dot(matrix, total_matrix)\n        if old_cost - cost &lt; threshold:\n            break\n        else:\n            old_cost = cost\n    aligned = source@total_matrix[:3,:3].T + total_matrix[:3,-1]\n    return total_matrix, aligned, cost\n\n\ndef combined_alignment(source, target, pre_align=True, shear=False, iterations=100):\n    \"\"\"Align source to target by combination of moment-of-intertia based aligment + ICP\"\"\"\n    if pre_align:\n        trafo_initial, _ = align_by_centroid_and_intertia(source, target,\n                                                          scale=True, shear=shear, improper=False)\n    else:\n        trafo_initial = None\n    trafo_icp, _, _ = icp(source, target, initial=trafo_initial,\n                          threshold=1e-4, max_iterations=iterations,\n                          scale=True, n_samples=5000)\n    return trafo_icp\n\n\n### Shrink-wrapping\n\n\ndef shrinkwrap_and_smooth(source_obj, target_obj, corrective_smooth_iter=0):\n    \"\"\"\n    Applies a shrinkwrap modifier with target_obj to source_obj, \n    optionally adds a corrective smooth modifier, and applies all modifiers.\n\n    Parameters:\n    - source_obj: The source mesh object to be modified.\n    - target_obj: The target mesh object for the shrinkwrap modifier.\n    - corrective_smooth_iter: (Optional) Number of iterations for the corrective smooth modifier. \n      If 0, no corrective smooth is applied.\n\n    Returns:\n    - bpy.types.Object: The new modified mesh object.\n    \"\"\"\n    # Ensure the objects are valid\n    if source_obj.type != 'MESH' or target_obj.type != 'MESH':\n        raise ValueError(\"Both source_obj and target_obj must be mesh objects.\")\n\n    # Store the currently active object\n    original_active_obj = bpy.context.view_layer.objects.active\n\n    # Add the first shrinkwrap modifier\n    shrinkwrap_1 = source_obj.modifiers.new(name=\"Shrinkwrap\", type='SHRINKWRAP')\n    shrinkwrap_1.target = target_obj\n    shrinkwrap_1.wrap_method = 'TARGET_PROJECT'\n\n    # Add a corrective smooth modifier if requested\n    for i in range(0, corrective_smooth_iter):\n        corrective_smooth = source_obj.modifiers.new(name=f\"Corrective Smooth {i}\", type='CORRECTIVE_SMOOTH')\n        corrective_smooth.iterations = 5\n        corrective_smooth.scale = 0\n        # Add a second shrinkwrap modifier after the corrective smooth\n        shrinkwrap_2 = source_obj.modifiers.new(name=f\"Shrinkwrap {i}\", type='SHRINKWRAP')\n        shrinkwrap_2.target = target_obj\n        shrinkwrap_2.wrap_method = 'TARGET_PROJECT'\n\n    # Apply all modifiers\n    bpy.context.view_layer.objects.active = source_obj\n    for modifier in source_obj.modifiers:\n        bpy.ops.object.modifier_apply(modifier=modifier.name)\n    # Restore the original active object\n    bpy.context.view_layer.objects.active = original_active_obj\n    return source_obj\n\n\n### Handling of mesh-associated array-data\n\n\ndef set_numpy_attribute(mesh, name, array):\n    \"\"\"Sets mesh[name] = array.\n    \n    Since Blender does not support adding arbitrary objects as attributes to meshes,\n    the array is flattened, converted to a binary buffer, and saved as a tuple together with its shape.\n    All arrays are converted to np.float32.\n    \"\"\"\n    bytes, shape = (array.astype(np.float32).flatten().tobytes(), array.shape)\n    mesh[name] = (bytes, shape)\n    return None\n\n\ndef get_numpy_attribute(mesh, name):\n    \"\"\"Get array = mesh[name].\n    \n    Since Blender does not support adding arbitrary objects as attributes to meshes,\n    the array is flattened, converted to a binary buffer, and saved as a tuple together with its shape.\n    All arrays are converted to np.float32.\n    \"\"\"\n    assert name in mesh, \"Attribute not found\"\n    return np.frombuffer(mesh[name][0], dtype=np.float32).reshape(mesh[name][1])\n\n\ndef separate_selected_into_mesh_and_box(self, context):\n    \"\"\"\n    Separate selected objects into mesh and box, representing 3D image data.\n    \n    If not exactly one mesh and one box (with attribute \"3D_data\") are selected,\n    an error is raised.\n    \"\"\"\n    n_data_selected = len([x for x in context.selected_objects if \"3D_data\" in x])\n    n_mesh_selected = len([x for x in context.selected_objects if not \"3D_data\" in x])\n    if not ((n_data_selected==1) and (n_mesh_selected==1)):\n        self.report({'ERROR'}, \"Select exactly one mesh and one 3D image (BoundingBox)!\")\n        return None, None\n    box = [x for x in context.selected_objects if \"3D_data\" in x][0]\n    obj = [x for x in context.selected_objects if not \"3D_data\" in x][0]\n    if not obj or obj.type != 'MESH':\n        self.report({'ERROR'}, \"No mesh object selected!\")\n        return None, None\n    return box, obj\n\n\n### Operators defining the user interface of the add-on\n\n\nclass LoadTIFFOperator(Operator):\n    \"\"\"Load .tif file and resolution. Also creates a bounding box object.\"\"\"\n    bl_idname = \"scene.load_tiff\"\n    bl_label = \"Load TIFF File\"\n\n    def execute(self, context):\n        file_path = bpy.path.abspath(context.scene.tissue_cartography_file)\n        resolution = np.array(context.scene.tissue_cartography_resolution)\n        self.report({'INFO'}, f\"Resolution loaded: {resolution}\")\n\n        # Load TIFF file as a NumPy array\n        if not (file_path.lower().endswith(\".tiff\") or file_path.lower().endswith(\".tif\")):\n            self.report({'ERROR'}, \"Selected file is not a TIFF\")\n            return {'CANCELLED'}\n        try:\n            data = tifffile.imread(file_path)\n            if not len(data.shape) in [3,4]:\n                self.report({'ERROR'}, \"Selected TIFF must have 3 or 4 axes.\")\n                return {'CANCELLED'}\n            # sort out axis order\n            axis_order_string = context.scene.tissue_cartography_axis_order\n            if not ''.join(sorted(axis_order_string)) in ['', 'xyz', 'cxyz']:\n                self.report({'ERROR'}, \"Must be empty, xyz, cxyz, or permutation thereof\")\n                return {'CANCELLED'}\n            if not len(axis_order_string) in [0, len(data.shape)]:\n                self.report({'ERROR'}, \"Number of axes in axis order does not match tiff data.\")\n                return {'CANCELLED'}\n            \n            if axis_order_string == '' and len(data.shape) == 4:\n                # ensure channel axis (assumed shortest axis) is 1st if no axis order provided.\n                channel_axis = np.argmin(data.shape)\n                data = np.moveaxis(data, channel_axis, 0)\n            if axis_order_string != '':\n                data = data.transpose(axis_order_to_transpose(axis_order_string))\n            if len(data.shape) == 3: # add singleton channel axis to single channel-data \n                data = data[np.newaxis]\n            # display image shape in add-on\n            context.scene.tissue_cartography_image_shape = str(data.shape[1:])\n            context.scene.tissue_cartography_image_channels = data.shape[0]\n            self.report({'INFO'}, f\"TIFF file loaded with shape {data.shape}\")\n            # create a bounding box mesh to represent the data\n            box = create_box(*(np.array(data.shape[1:])*resolution),\n                             name=f\"{Path(file_path).stem}_BoundingBox\",\n                             hide=False)\n            box.display_type = 'WIRE'\n            # attach the data to the box\n            set_numpy_attribute(box, \"resolution\", resolution)\n            set_numpy_attribute(box, \"3D_data\", data)\n            \n        except Exception as e:\n            self.report({'ERROR'}, f\"Failed to load TIFF file: {e}\")\n            return {'CANCELLED'}\n\n        return {'FINISHED'}\n\n\nclass LoadSegmentationTIFFOperator(Operator):\n    \"\"\"\n    Load segmentation .tif file and resolution, and create a mesh from binary segmentation.\n    \n    Selecting a folder instead of a file batch processes all files in folder.\n    \"\"\"\n    bl_idname = \"scene.load_segmentation\"\n    bl_label = \"Load Segmentation TIFF File\"\n\n    def execute(self, context):\n        # Load resolution as a NumPy array\n        resolution_array = np.array(context.scene.tissue_cartography_segmentation_resolution)\n        input_path = Path(bpy.path.abspath(context.scene.tissue_cartography_segmentation_file))\n        if input_path.is_dir():\n            files_to_process = [f for f in input_path.iterdir() if f.is_file() and f.suffix in [\".tif\", \".tiff\"]]\n        elif input_path.is_file():\n            files_to_process = [input_path]\n        else:\n            self.report({'ERROR'}, \"Select a valid file or directory\")\n            return {'CANCELLED'}\n        for file_path in files_to_process:\n            if not file_path.suffix in [\".tif\", \".tiff\"]:\n                self.report({'ERROR'}, \"Selected file is not a TIFF\")\n                return {'CANCELLED'}\n            try:\n                data = tifffile.imread(file_path)\n                # sort out axis order\n                if not len(data.shape) in [3,4]:\n                    self.report({'ERROR'}, \"Selected TIFF must have 3 or 4 axes.\")\n                    return {'CANCELLED'}\n                axis_order_string = context.scene.tissue_cartography_segmentation_axis_order\n                if not ''.join(sorted(axis_order_string)) in ['', 'xyz', 'cxyz']:\n                    self.report({'ERROR'}, \"Must be empty, xyz, cxyz, or permutation thereof\")\n                    return {'CANCELLED'}\n                if not len(axis_order_string) in [0, len(data.shape)]:\n                    self.report({'ERROR'}, \"Number of axes in axis order does not match tiff data.\")\n                    return {'CANCELLED'}\n                if axis_order_string == '' and len(data.shape) == 4:\n                    # ensure channel axis (assumed shortest axis) is 1st if no axis order provided.\n                    channel_axis = np.argmin(data.shape)\n                    data = np.moveaxis(data, channel_axis, 0)\n                if axis_order_string != '':\n                    data = data.transpose(axis_order_to_transpose(axis_order_string))\n                if len(data.shape) == 3: # add singleton channel axis to single channel-data \n                    data = data[np.newaxis]\n                self.report({'INFO'}, f\"TIFF file loaded with shape {data.shape}\")\n                context.scene.tissue_cartography_segmentation_shape = str(data.shape[1:])\n                context.scene.tissue_cartography_segmentation_channels = data.shape[0]\n                # iterate over channels. each channel is one label\n                for ic, channel in enumerate(data):\n                    # smooth and normalize the segmentation\n                    channel = (channel-channel.min())/(channel.max()-channel.min())\n                    sigma = context.scene.tissue_cartography_segmentation_sigma\n                    channel = ndimage.gaussian_filter(channel, sigma=sigma/resolution_array)\n                    # compute mesh using marching cubes, and convert to mesh\n                    verts, faces, _, _ = measure.marching_cubes(channel, level=0.5, spacing=(1.0,1.0,1.0))\n                    verts = verts * resolution_array\n                    create_mesh_from_numpy(f\"{Path(file_path).stem}_c{ic}\", verts, faces)\n                \n            except Exception as e:\n                self.report({'ERROR'}, f\"Failed to load segmentation: {e}\")\n                return {'CANCELLED'}\n        return {'FINISHED'}\n    \n\nclass CreateProjectionOperator(Operator):\n    \"\"\"\n    Create a cartographic projection.\n    \n    Select one mesh and one 3d-image ([...]_BoundingBox) to project 3d image data\n    onto mesh surface.\n    \"\"\"\n    bl_idname = \"scene.create_projection\"\n    bl_label = \"Create Projection\"\n\n    def execute(self, context):\n        # Validate selected object and UV map\n        box, obj = separate_selected_into_mesh_and_box(self, context)\n        if box is None or obj is None:\n            return {'CANCELLED'}\n        # Ensure the object has a UV map\n        if not obj.data.uv_layers:\n            self.report({'ERROR'}, \"The selected mesh does not have a UV map!\")\n            return {'CANCELLED'}\n        # Parse offsets into a NumPy array\n        offsets_str = context.scene.tissue_cartography_offsets\n        try:\n            offsets_array = np.array([float(x) for x in offsets_str.split(\",\") if x.strip()])\n            if offsets_array.size == 0:\n                offsets_array = np.array([0])\n            self.report({'INFO'}, f\"Offsets loaded: {offsets_array}\")\n        except ValueError as e:\n            self.report({'ERROR'}, f\"Invalid offsets input: {e}\")\n            return {'CANCELLED'}\n        # set offsets as property\n        set_numpy_attribute(obj, \"projection_offsets\", offsets_array)\n        \n        # Parse projection resolution\n        projection_resolution = context.scene.tissue_cartography_projection_resolution\n        self.report({'INFO'}, f\"Using projection resolution: {projection_resolution}\")\n\n        # texture bake normals and world positions\n        loop_uvs, loop_normals, loop_world_positions = get_uv_normal_world_per_loop(obj, filter_unique=True)\n        \n        baked_normals = bake_per_loop_values_to_uv(loop_uvs, loop_normals, \n                                                   image_resolution=projection_resolution)\n        baked_normals = (baked_normals.T/np.linalg.norm(baked_normals.T, axis=0)).T\n        baked_world_positions = bake_per_loop_values_to_uv(loop_uvs, loop_world_positions,\n                                                           image_resolution=projection_resolution)\n        # obtain UV layout and use it to get a mask\n        uv_layout_path = str(Path(bpy.path.abspath(\"//\")).joinpath(f'{obj.name}_UV_layout.png'))\n        mask = get_uv_layout(obj, uv_layout_path, projection_resolution)\n        baked_normals[~mask] = np.nan\n        baked_world_positions[~mask] = np.nan\n        \n        # create a pullback\n        box_world_inv = np.linalg.inv(np.array(box.matrix_world))\n        baked_data = bake_volumetric_data_to_uv(get_numpy_attribute(box, \"3D_data\"),\n                                                baked_world_positions, \n                                                get_numpy_attribute(box, \"resolution\"),\n                                                baked_normals, normal_offsets=offsets_array,\n                                                affine_matrix=box_world_inv)\n        # set results as attributes of the mesh\n        set_numpy_attribute(obj, \"baked_data\", baked_data)\n        set_numpy_attribute(obj, \"baked_normals\", baked_normals)\n        set_numpy_attribute(obj, \"baked_world_positions\", baked_world_positions)\n        # create texture\n        create_material_from_multilayer_array(obj, baked_data, material_name=f\"ProjectedMaterial_{obj.name}\")\n\n        return {'FINISHED'}\n\n\nclass SaveProjectionOperator(Operator):\n    \"\"\"Save cartographic projection to disk\"\"\"\n    bl_idname = \"scene.save_projection\"\n    bl_label = \"Save Projection\"\n    \n    filepath: bpy.props.StringProperty(subtype=\"FILE_PATH\")\n    \n    def invoke(self, context, event):\n        # Open file browser to choose the save location\n        context.window_manager.fileselect_add(self)\n        return {'RUNNING_MODAL'}\n    \n    def execute(self, context):\n        obj = context.active_object\n        if not obj or \"baked_data\" not in obj:\n            self.report({'ERROR'}, \"No baked data found on the active object!\")\n            return {'CANCELLED'}\n        \n        # Get the baked data\n        baked_data = get_numpy_attribute(obj, \"baked_data\")\n        baked_normals = get_numpy_attribute(obj, \"baked_normals\")\n        baked_world_positions = get_numpy_attribute(obj, \"baked_world_positions\")\n        # Save the data to the chosen filepath\n        try:\n            tifffile.imwrite(self.filepath + \"_BakedNormals.tif\", baked_normals)\n            tifffile.imwrite(self.filepath + \"_BakedPositions.tif\", baked_world_positions)\n            tifffile.imwrite(self.filepath + \"_BakedData.tif\", baked_data.astype(np.float32),\n                             metadata={'axes': 'ZCYX'}, imagej=True)\n            self.report({'INFO'}, f\"Cartographic projection saved to {self.filepath}\")\n        except Exception as e:\n            self.report({'ERROR'}, f\"Failed to save data: {str(e)}\")\n            return {'CANCELLED'}\n        \n        return {'FINISHED'}\n\n\nclass BatchProjectionOperator(Operator):\n    \"\"\"\n    Batch-process cartographic projections.\n    \n    Select all meshes to process (in blender) and one 3d-image ([...]_BoundingBox)\n    for resolution and relative position information. Further 3d .tiff files are read from\n    Batch Process Input directory. Mesh names should match .tiff file names.\n    \n    \"\"\"\n    bl_idname = \"scene.batch_projection\"\n    bl_label = \"Create Projections (Batch Mode)\"\n\n    def execute(self, context):\n        try:\n            box = [x for x in context.selected_objects if \"3D_data\" in x][0]\n        except IndexError:\n            self.report({'ERROR'}, \"Select one 3D image (BoundingBox) for resolution and position information!\")\n            return\n        # get list of files\n        batch_path = Path(bpy.path.abspath(context.scene.tissue_cartography_batch_directory))\n        batch_out_path = Path(bpy.path.abspath(context.scene.tissue_cartography_batch_output_directory))\n        batch_files = {f.stem: f for f in list(batch_path.iterdir()) if ((f.suffix in [\".tif\", \".tiff\"]) and not \"Baked\" in f.stem)}\n        # match files to selected meshes\n        meshes_to_process = [obj for obj in context.selected_objects if obj != box]\n        mesh_names = [obj.name for obj in meshes_to_process]\n        matched = {obj.name: difflib.get_close_matches(obj.name, batch_files.keys(), n=1, cutoff=0.1)\n                   for obj in context.selected_objects if obj != box}\n        # parse axis order\n        axis_order = list(context.scene.tissue_cartography_axis_order)\n        if not sorted(axis_order) == [0,1,2,3]:\n            self.report({'ERROR'}, \"Axis order must be a permutation of [0,1,2,3] (e.g. [3,0,1,2])\")\n            return {'CANCELLED'}\n        # parse offsets into a NumPy array\n        offsets_str = context.scene.tissue_cartography_offsets\n        try:\n            offsets_array = np.array([float(x) for x in offsets_str.split(\",\") if x.strip()])\n            if offsets_array.size == 0:\n                offsets_array = np.array([0])\n            self.report({'INFO'}, f\"Offsets loaded: {offsets_array}\")\n        except ValueError as e:\n            self.report({'ERROR'}, f\"Invalid offsets input: {e}\")\n            return {'CANCELLED'}\n        # Parse projection resolution\n        projection_resolution = context.scene.tissue_cartography_projection_resolution\n        self.report({'INFO'}, f\"Using projection resolution: {projection_resolution}\")\n        # find box for position and resolution info\n        \n        for iobj, obj in enumerate(meshes_to_process):\n            self.report({'INFO'}, f\"Processing {iobj}/{len(meshes_to_process)}\")\n            if not obj.data.uv_layers:\n                self.report({'ERROR'}, f\"Mesh {obj.name} does not have a UV map!\")\n                return {'CANCELLED'}\n            # set offsets as property\n            set_numpy_attribute(obj, \"projection_offsets\", offsets_array)\n            # find the matching file\n            if len(matched[obj.name]) == 0:\n                self.report({'ERROR'}, \"No matching file found for {obj.name}!\")\n                return {'CANCELLED'}\n            file_path = batch_files[matched[obj.name][0]]\n            # load the 3D data\n            try:\n                data = tifffile.imread(file_path)\n                if not len(data.shape) in [3,4]:\n                    self.report({'INFO'}, f\"Selected TIFF for {obj.name} must have 3 or 4 axes.\")\n                    return {'CANCELLED'}\n                if len(data.shape) == 3: # add singleton channel axis to single channel-data \n                    data = data[np.newaxis]\n                # ensure channel axis (assumed shortest axis) is 1st\n                channel_axis = np.argmin(data.shape)\n                data = np.moveaxis(data, channel_axis, 0)\n                data = data.transpose(axis_order)\n            except:\n                self.report({'ERROR'}, f\"Failed loading TIFF for {obj.name}\")\n                return {'CANCELLED'}\n            # texture bake normals and world positions\n            loop_uvs, loop_normals, loop_world_positions = get_uv_normal_world_per_loop(obj, filter_unique=True)\n            \n            baked_normals = bake_per_loop_values_to_uv(loop_uvs, loop_normals, \n                                                       image_resolution=projection_resolution)\n            baked_normals = (baked_normals.T/np.linalg.norm(baked_normals.T, axis=0)).T\n            baked_world_positions = bake_per_loop_values_to_uv(loop_uvs, loop_world_positions,\n                                                               image_resolution=projection_resolution)\n            # obtain UV layout and use it to get a mask\n            uv_layout_path = str(Path(batch_out_path).joinpath(f'{obj.name}_UV_layout.png'))\n            mask = get_uv_layout(obj, uv_layout_path, projection_resolution)\n            baked_normals[~mask] = np.nan\n            baked_world_positions[~mask] = np.nan\n            # create a pullback\n            box_world_inv = np.linalg.inv(np.array(box.matrix_world))\n            baked_data = bake_volumetric_data_to_uv(data,\n                                                    baked_world_positions, \n                                                    get_numpy_attribute(box, \"resolution\"),\n                                                    baked_normals, normal_offsets=offsets_array,\n                                                    affine_matrix=box_world_inv)\n            # Save the data to the chosen filepath\n            try:\n                tifffile.imwrite(batch_out_path.joinpath(f\"{obj.name}_BakedNormals.tif\"), baked_normals)\n                tifffile.imwrite(batch_out_path.joinpath(f\"{obj.name}_BakedPositions.tif\"), baked_world_positions)\n                tifffile.imwrite(batch_out_path.joinpath(f\"{obj.name}_BakedData.tif\"), baked_data.astype(np.float32),\n                                 metadata={'axes': 'ZCYX'}, imagej=True)\n                self.report({'INFO'}, f\"Cartographic projection saved for {obj.name}\")\n            except Exception as e:\n                self.report({'ERROR'}, f\"Failed to save data for {obj.name}: {str(e)}\")\n                return {'CANCELLED'}\n            if bpy.context.scene.tissue_cartography_batch_create_materials:\n                # set results as attributes of the mesh\n                set_numpy_attribute(obj, \"baked_data\", baked_data)\n                set_numpy_attribute(obj, \"baked_normals\", baked_normals)\n                set_numpy_attribute(obj, \"baked_world_positions\", baked_world_positions)\n                # create texture\n                create_material_from_multilayer_array(obj, baked_data, material_name=f\"ProjectedMaterial_{obj.name}\")\n        return {'FINISHED'}\n    \n\nclass SlicePlaneOperator(Operator):\n    \"\"\"Create a slice plane along the selected axis with texture from 3D data\"\"\"\n    bl_idname = \"scene.create_slice_plane\"\n    bl_label = \"Create Slice Plane\"\n    bl_options = {'REGISTER', 'UNDO'}\n\n    def execute(self, context):\n        # Get the 3D data array from the selected box\n        box = context.active_object\n        if not box or not \"3D_data\" in box:\n            self.report({'ERROR'}, \"Select exactly a 3D image (BoundingBox)!\")\n            return {'CANCELLED'}\n        data = get_numpy_attribute(box, \"3D_data\")\n        \n        resolution = get_numpy_attribute(box, \"resolution\")\n        if not isinstance(data, np.ndarray) or data.ndim != 4:\n            self.report({'ERROR'}, \"Invalid 3D data array.\")\n            return {'CANCELLED'}\n        if context.scene.tissue_cartography_slice_channel &gt;= data.shape[0]:\n            self.report({'ERROR'}, f\"Channel {context.scene.tissue_cartography_slice_channel} is out of bounds for the data array.\")\n            return {'CANCELLED'}\n\n        length, width, height = (np.array(data.shape[1:]) * resolution)\n        slice_plane = create_slice_plane(length, width, height, axis=context.scene.tissue_cartography_slice_axis,\n                                         position=context.scene.tissue_cartography_slice_position)\n        slice_plane.name = f\"{slice_plane.name}_{box.name}\"\n        # set matrix world\n        slice_plane.matrix_world = box.matrix_world\n                                         \n        slice_img = get_slice_image(data, resolution, axis=context.scene.tissue_cartography_slice_axis,\n                                    position=context.scene.tissue_cartography_slice_position)\n        slice_img = normalize_quantiles(slice_img, quantiles=(0.01, 0.99),\n                                        channel_axis=0, clip=True, data_type=None)     \n        create_material_from_array(slice_plane, slice_img[context.scene.tissue_cartography_slice_channel], material_name=f\"SliceMaterial_{box.name}_{context.scene.tissue_cartography_slice_axis}_{context.scene.tissue_cartography_slice_position}\")  \n        return {'FINISHED'}\n\n\nclass VertexShaderInitializeOperator(Operator):\n    \"\"\"Initialize vertex shader for a selected mesh. Colors mesh vertices according to \n    3D image intensity from selected BoundingBox.\"\"\"\n    bl_idname = \"scene.initialize_vertex_shader\"\n    bl_label = \"Initialize Vertex Shader\"\n    bl_options = {'REGISTER', 'UNDO'}\n\n    def execute(self, context):\n        # create global dict to hold interpolator objects\n        if not hasattr(bpy.types.Scene, \"tissue_cartography_interpolators\"):\n            bpy.types.Scene.tissue_cartography_interpolators = dict()\n        # get the selected mesh and bounding box\n        box, obj = separate_selected_into_mesh_and_box(self, context)\n        if box is None or obj is None:\n            return {'CANCELLED'}\n        # Get the 3D data array from the box object\n        data = get_numpy_attribute(box, \"3D_data\")\n        resolution = get_numpy_attribute(box, \"resolution\")\n     \n        if not isinstance(data, np.ndarray) or data.ndim != 4:\n            self.report({'ERROR'}, \"Invalid 3D data array.\")\n            return {'CANCELLED'}\n        if not obj or obj.type != 'MESH':\n            self.report({'ERROR'}, \"No mesh object selected!\")\n            return {'CANCELLED'}\n        if context.scene.tissue_cartography_vertex_shader_channel &gt;= data.shape[0]:\n            self.report({'ERROR'}, f\"Channel {context.scene.tissue_cartography_vertex_shader_channel} is out of bounds for the data array.\")\n            return {'CANCELLED'}\n        # need to compute coordinates relative to matrix_world of box I think\n        set_numpy_attribute(obj, \"box_world_inv_vertex_shader\",\n                            np.array(box.matrix_world.inverted()))\n        bpy.types.Scene.tissue_cartography_interpolators[obj.name] = get_image_to_vertex_interpolator(obj, data, resolution)\n        box_inv = mathutils.Matrix(get_numpy_attribute(obj, \n                                   \"box_world_inv_vertex_shader\"))\n        positions = np.array([box_inv@obj.matrix_world@(v.co + context.scene.tissue_cartography_vertex_shader_offset*v.normal)\n                              for v in obj.data.vertices])\n        intensities = bpy.types.Scene.tissue_cartography_interpolators[obj.name][context.scene.tissue_cartography_vertex_shader_channel](positions)\n        colors = np.stack(3*[intensities,], axis=1)\n        \n        assign_vertex_colors(obj, colors)\n        create_vertex_color_material(obj, material_name=f\"VertexColorMaterial_{obj.name}\")\n\n        return {'FINISHED'}\n\n\nclass VertexShaderRefreshOperator(Operator):\n    \"\"\"Refresh vertex colors for a selected mesh. Colors mesh vertices according to \n    3D image intensity.\"\"\"\n    bl_idname = \"scene.refresh_vertex_shader\"\n    bl_label = \"Refresh Vertex Shader\"\n    bl_options = {'REGISTER', 'UNDO'}\n\n    def execute(self, context):\n        obj = context.active_object\n        interpolator_dict = getattr(context.scene, \"tissue_cartography_interpolators\")\n        if not obj or obj.type != 'MESH':\n            self.report({'ERROR'}, \"No mesh object selected!\")\n            return {'CANCELLED'}\n        if interpolator_dict is None or obj.name not in interpolator_dict:\n            self.report({'ERROR'}, f\"Vertex shader not initialized.\")\n            return {'CANCELLED'}\n        if context.scene.tissue_cartography_vertex_shader_channel &gt;= len(interpolator_dict[obj.name]):\n            self.report({'ERROR'}, f\"Channel {context.scene.tissue_cartography_vertex_shader_channel} is out of bounds for the data array.\")\n        box_inv = mathutils.Matrix(get_numpy_attribute(obj, \"box_world_inv_vertex_shader\"))\n        positions = np.array([box_inv@obj.matrix_world@(v.co + context.scene.tissue_cartography_vertex_shader_offset*v.normal)\n                              for v in obj.data.vertices])\n        intensities = interpolator_dict[obj.name][context.scene.tissue_cartography_vertex_shader_channel](positions)\n        colors = np.stack(3*[intensities,], axis=1)\n        assign_vertex_colors(obj, colors)\n\n        return {'FINISHED'}\n\n\nclass AlignOperator(Operator):\n    \"\"\"Align active and selected meshes by rotation, translation, and scaling.\"\"\"\n    bl_idname = \"scene.align\"\n    bl_label = \"Align Selected To Active Mesh\"\n    bl_options = {'REGISTER', 'UNDO'}\n\n    def execute(self, context):\n        \n        if context.scene.tissue_cartography_align_type == \"selected\":\n            target_mesh = context.active_object\n            for source_mesh in [x for x in context.selected_objects if not x==target_mesh]:\n                self.report({'INFO'}, f\"Aligning: {source_mesh.name} to {target_mesh.name}\")\n                if target_mesh.type != 'MESH'  or source_mesh.type != 'MESH':\n                    self.report({'ERROR'}, \"Selected object(s) is not a mesh.\")\n                    return {'CANCELLED'}\n                # Get the 3D coordinates from the meshes\n                target = np.array([target_mesh.matrix_world@v.co for v in target_mesh.data.vertices])\n                source = np.array([source_mesh.matrix_world@v.co for v in source_mesh.data.vertices])\n                trafo_matrix = combined_alignment(source, target,\n                                                  pre_align=context.scene.tissue_cartography_prealign,\n                                                  shear=context.scene.tissue_cartography_prealign_shear,\n                                                  iterations=context.scene.tissue_cartography_align_iter)\n                source_mesh.matrix_world = mathutils.Matrix(trafo_matrix)@ source_mesh.matrix_world\n        elif context.scene.tissue_cartography_align_type == \"active\":\n            source_mesh = context.active_object\n            for target_mesh in [x for x in context.selected_objects if not x==source_mesh]:\n                self.report({'INFO'}, f\"Aligning: {source_mesh.name} to {target_mesh.name}\")\n                if target_mesh.type != 'MESH'  or source_mesh.type != 'MESH':\n                    self.report({'ERROR'}, \"Selected object(s) is not a mesh.\")\n                    return {'CANCELLED'}\n                # Get the 3D coordinates from the meshes and compute alignment\n                target = np.array([target_mesh.matrix_world@v.co for v in target_mesh.data.vertices])\n                source = np.array([source_mesh.matrix_world@v.co for v in source_mesh.data.vertices])\n                trafo_matrix = combined_alignment(source, target,\n                                                  pre_align=context.scene.tissue_cartography_prealign,\n                                                  shear=context.scene.tissue_cartography_prealign_shear,\n                                                  iterations=context.scene.tissue_cartography_align_iter)\n                # copy source mesh\n                source_mesh_copied = source_mesh.copy()\n                source_mesh_copied.data = source_mesh.data.copy()\n                bpy.context.collection.objects.link(source_mesh_copied)\n                source_mesh_copied.name = f\"{target_mesh.name}_aligned\" \n                source_mesh_copied.matrix_world = mathutils.Matrix(trafo_matrix)@ source_mesh.matrix_world\n        return {'FINISHED'}\n\n\nclass ShrinkwrapOperator(Operator):\n    \"\"\"Copy and shrink-wrap active mesh to selected meshes.\"\"\"\n    bl_idname = \"scene.shrinkwrap\"\n    bl_label = \"Shrink-Wrap Active to Selected\"\n    bl_options = {'REGISTER', 'UNDO'}\n\n    def execute(self, context):\n        mode = context.scene.tissue_cartography_shrinkwarp_iterative\n        source_mesh = context.active_object\n        targets = sorted([x for x in context.selected_objects if not x==source_mesh], key=lambda x: x.name)\n        if mode == \"backward\":\n            targets = targets[::-1]\n        for target_mesh in targets:\n            self.report({'INFO'}, f\"Aligning: {source_mesh.name} to {target_mesh.name}\")\n            if target_mesh.type != 'MESH'  or source_mesh.type != 'MESH':\n                self.report({'ERROR'}, \"Selected object(s) is not a mesh.\")\n                return {'CANCELLED'}\n            # rigid alignment\n            target = np.array([target_mesh.matrix_world@v.co for v in target_mesh.data.vertices])\n            source = np.array([source_mesh.matrix_world@v.co for v in source_mesh.data.vertices])\n            trafo_matrix = combined_alignment(source, target,\n                                              pre_align=context.scene.tissue_cartography_prealign,\n                                              shear=context.scene.tissue_cartography_prealign_shear,\n                                              iterations=context.scene.tissue_cartography_align_iter)\n            # copy source mesh\n            source_mesh_copied = source_mesh.copy()\n            source_mesh_copied.data = source_mesh.data.copy()\n            bpy.context.collection.objects.link(source_mesh_copied)\n            source_mesh_copied.matrix_world = mathutils.Matrix(trafo_matrix)@ source_mesh.matrix_world\n            source_mesh_copied.name = f\"{target_mesh.name}_wrapped\" \n            # shrink-wrap\n            shrinkwrap_and_smooth(source_mesh_copied, target_mesh,\n                                  corrective_smooth_iter=context.scene.tissue_cartography_shrinkwarp_smooth)\n            # data transfer modifier to copy UV map from wrapped to target\n            data_transfer = target_mesh.modifiers.new(name=\"DataTransfer\", type='DATA_TRANSFER')\n            data_transfer.object = source_mesh_copied\n            data_transfer.use_loop_data = True\n            data_transfer.data_types_loops = {'UV'}\n            data_transfer.loop_mapping = 'POLYINTERP_NEAREST'\n            # apply\n            original_active_obj = bpy.context.view_layer.objects.active\n            bpy.context.view_layer.objects.active = target_mesh\n            bpy.ops.object.datalayout_transfer(modifier=\"DataTransfer\")\n            bpy.ops.object.modifier_apply(modifier=\"DataTransfer\")\n            bpy.context.view_layer.objects.active = original_active_obj\n                                  \n            if mode in [\"forward\", \"backward\"]:\n                source_mesh = source_mesh_copied\n        return {'FINISHED'}\n\n\nclass HelpPopupOperator(Operator):\n    \"\"\"Open help window.\"\"\"\n    bl_idname = \"scene.help_popup\"\n    bl_label = \"Tissue Cartography Help\"\n\n    def execute(self, context):\n        url = \"https://nikolas-claussen.github.io/blender-tissue-cartography/Tutorials/03_blender_addon_tutorial.html\"\n        bpy.ops.wm.url_open(url=url)\n        return {'FINISHED'}\n        \n        \nclass TissueCartographyPanel(Panel):\n    \"\"\"Class defining layout of user interface (buttons, inputs, etc.)\"\"\"\n    bl_label = \"Tissue Cartography\"\n    bl_idname = \"SCENE_PT_tissue_cartography\"\n    bl_space_type = 'PROPERTIES'\n    bl_region_type = 'WINDOW'\n    bl_context = \"scene\"\n\n    def draw(self, context):\n        layout = self.layout\n        scene = context.scene\n\n        layout.prop(scene, \"tissue_cartography_file\")\n        row_tiff = layout.row()\n        row_tiff.prop(scene, \"tissue_cartography_resolution\")\n        row_tiff.prop(scene, \"tissue_cartography_axis_order\")\n        layout.operator(\"scene.load_tiff\", text=\"Load .tiff file\")\n        layout.label(text=f\"Loaded Image Shape: {scene.tissue_cartography_image_shape}. Loaded Image Channels: {scene.tissue_cartography_image_channels}\")\n        layout.separator()\n        \n        layout.prop(scene, \"tissue_cartography_segmentation_file\")\n        row_segmentation = layout.row()\n        row_segmentation.prop(scene, \"tissue_cartography_segmentation_resolution\")\n        row_segmentation.prop(scene, \"tissue_cartography_segmentation_axis_order\")\n        row_segmentation.prop(scene, \"tissue_cartography_segmentation_sigma\")\n        layout.operator(\"scene.load_segmentation\", text=\"Get mesh(es) from binary segmentation .tiff file(s)\")\n        layout.label(text=f\"Loaded Segmentation Shape: {scene.tissue_cartography_segmentation_shape}. Loaded Segmentation Channels: {scene.tissue_cartography_segmentation_channels}\")\n        layout.separator()\n        \n        row_slice = layout.row()\n        row_slice.prop(scene, \"tissue_cartography_slice_axis\")\n        row_slice.prop(scene, \"tissue_cartography_slice_position\")\n        row_slice.prop(scene, \"tissue_cartography_slice_channel\")\n        layout.operator(\"scene.create_slice_plane\", text=\"Create slice plane\")\n        layout.separator()\n        \n        row_vertex = layout.row()\n        row_vertex.prop(scene, \"tissue_cartography_vertex_shader_offset\")\n        row_vertex.prop(scene, \"tissue_cartography_vertex_shader_channel\")\n        row_vertex2 = layout.row()\n        row_vertex2.operator(\"scene.initialize_vertex_shader\", text=\"Initialize vertex shading\")\n        row_vertex2.operator(\"scene.refresh_vertex_shader\", text=\"Refresh vertex shading\")\n        layout.separator()\n        \n        row_projection = layout.row()\n        row_projection.prop(scene, \"tissue_cartography_offsets\")\n        row_projection.prop(scene, \"tissue_cartography_projection_resolution\")\n        row_projection2 = layout.row()\n        row_projection2.operator(\"scene.create_projection\", text=\"Create Projection\")\n        row_projection2.operator(\"scene.save_projection\", text=\"Save Projection\")\n        layout.separator()\n        \n        row_batch = layout.row()\n        row_batch.prop(scene, \"tissue_cartography_batch_directory\")\n        row_batch.prop(scene, \"tissue_cartography_batch_output_directory\")\n        row_batch.prop(scene, \"tissue_cartography_batch_create_materials\")\n        layout.operator(\"scene.batch_projection\", text=\"Batch Process And Save\")\n        layout.separator()\n        \n        row_align = layout.row()\n        row_align.prop(scene, \"tissue_cartography_prealign\")\n        row_align.prop(scene, \"tissue_cartography_prealign_shear\")\n        row_align.prop(scene, \"tissue_cartography_align_type\")\n        row_align.prop(scene, \"tissue_cartography_align_iter\")\n        layout.operator(\"scene.align\", text=\"Align Meshes\")\n        layout.separator()\n        \n        row_shrinkwrap = layout.row()\n        row_shrinkwrap.prop(scene, \"tissue_cartography_shrinkwarp_smooth\")\n        row_shrinkwrap.prop(scene, \"tissue_cartography_shrinkwarp_iterative\")\n        layout.operator(\"scene.shrinkwrap\", text=\"Shrinkwrap Meshes (Active To Selected)\")\n        layout.separator()\n        \n        layout.operator(\"scene.help_popup\", text=\"Show help\", icon='HELP')\n    \n\n### Add the add-on to the user interface\n\n\ndef register():\n    \"\"\"Add the add-on to the blender user interface\"\"\"\n    bpy.utils.register_class(TissueCartographyPanel)\n    bpy.utils.register_class(LoadTIFFOperator)\n    bpy.utils.register_class(LoadSegmentationTIFFOperator)\n    bpy.utils.register_class(CreateProjectionOperator)\n    bpy.utils.register_class(SaveProjectionOperator)\n    bpy.utils.register_class(BatchProjectionOperator)\n    bpy.utils.register_class(SlicePlaneOperator)\n    bpy.utils.register_class(VertexShaderInitializeOperator)\n    bpy.utils.register_class(VertexShaderRefreshOperator)\n    bpy.utils.register_class(AlignOperator)\n    bpy.utils.register_class(ShrinkwrapOperator)\n    bpy.utils.register_class(HelpPopupOperator)\n    \n    bpy.types.Scene.tissue_cartography_file = StringProperty(\n        name=\"File Path\",\n        description=\"Path to the TIFF file\",\n        subtype='FILE_PATH',\n    )\n    bpy.types.Scene.tissue_cartography_resolution = FloatVectorProperty(\n        name=\"x/y/z Resolution (µm)\",\n        description=\"Resolution in microns along x, y, z axes\",\n        size=3,\n        default=(1.0, 1.0, 1.0),\n    )\n    bpy.types.Scene.tissue_cartography_axis_order= StringProperty(\n        name=\"Axis order\",\n        description=\"Axis order, either xyz + permutations or xyz + permutations (multichannel data). Dynamic data should be loaded as one .tiff per timepoint. If not provided, inferred automatically.\",\n        default=\"\",\n    )\n    bpy.types.Scene.tissue_cartography_image_channels = IntProperty(\n        name=\"Image Channels\",\n        description=\"Channels of the loaded image (read-only)\",\n        default=0,\n        min=0,\n    )\n    bpy.types.Scene.tissue_cartography_image_shape = StringProperty(\n        name=\"Image Shape\",\n        description=\"Shape of the loaded image (read-only)\",\n        default=\"Not loaded\"\n    )\n    bpy.types.Scene.tissue_cartography_segmentation_file = StringProperty(\n        name=\"Segmentation File Path\",\n        description=\"Path to the segmentation TIFF file. Should have values between 0-1. Selecting a folder instead of a single file will batch-process the full folder.\",\n        subtype='FILE_PATH',\n    )\n    bpy.types.Scene.tissue_cartography_segmentation_resolution = FloatVectorProperty(\n        name=\"Segmentation x/y/z Resolution (µm)\",\n        description=\"Resolution of segmentation in microns along x, y, z axes\",\n        size=3,\n        default=(1.0, 1.0, 1.0),\n    )\n    bpy.types.Scene.tissue_cartography_segmentation_axis_order= StringProperty(\n        name=\"Axis order segmentation\",\n        description=\"Axis order of segmentation, either xyz + permutations or cxyz + permutations (multichannel data). Different channels for a segmentation mean different labels. Dynamic data should be loaded as one .tiff per timepoint. If not provided, inferred automatically.\",\n        default=\"\",\n    )\n    bpy.types.Scene.tissue_cartography_segmentation_sigma = FloatProperty(\n        name=\"Smoothing (µm)\",\n        description=\"Smothing kernel for extracting mesh from segmentation, in µm\",\n        default=0,\n        min=0\n    )\n    bpy.types.Scene.tissue_cartography_segmentation_channels = IntProperty(\n        name=\"Segmentation Channels\",\n        description=\"Channels of the segmentation (read-only)\",\n        default=0,\n        min=0,\n    )\n    bpy.types.Scene.tissue_cartography_segmentation_shape = StringProperty(\n        name=\"Segmentation Shape\",\n        description=\"Shape of the loaded segmentation (read-only)\",\n        default=\"Not loaded\"\n    )\n    bpy.types.Scene.tissue_cartography_slice_axis = EnumProperty(\n        name=\"Slice Axis\",\n        description=\"Choose an axis\",\n        items=[('x', \"X-Axis\", \"Align to the X axis\"),\n               ('y', \"Y-Axis\", \"Align to the Y axis\"),\n               ('z', \"Z-Axis\", \"Align to the Z axis\")],\n        default='x'\n    )\n    bpy.types.Scene.tissue_cartography_slice_position = FloatProperty(\n        name=\"Slice Position (µm)\",\n        description=\"Position along the selected axis in µm\",\n        default=0\n    )\n    bpy.types.Scene.tissue_cartography_slice_channel = IntProperty(\n        name=\"Slice Channel\",\n        description=\"Channel for slice plane\",\n        default=0,\n        min=0,\n    )\n    bpy.types.Scene.tissue_cartography_vertex_shader_offset = FloatProperty(\n        name=\"Vertex Shader Normal Offset (µm)\",\n        description=\"Normal offse for vertex shading.\",\n        default=0,\n    )\n    bpy.types.Scene.tissue_cartography_vertex_shader_channel = IntProperty(\n        name=\"Vertex Shader Channel\",\n        description=\"Channel for vertex shading.\",\n        default=0,\n        min=0,\n    )\n    bpy.types.Scene.tissue_cartography_offsets = StringProperty(\n        name=\"Normal Offsets (µm)\",\n        description=\"Comma-separated list of floats for multilayer projection offsets\",\n        default=\"0\",\n    )\n    bpy.types.Scene.tissue_cartography_projection_resolution = IntProperty(\n        name=\"Projection Format (Pixels)\",\n        description=\"Resolution for the projection (e.g., 1024 for 1024x1024 pixels)\",\n        default=1024,\n        min=1,\n    )\n    \n    bpy.types.Scene.tissue_cartography_batch_directory = StringProperty(\n        name=\"Batch Process Input Directory\",\n        description=\"Path to TIFF files directory\",\n        subtype='DIR_PATH',\n    )\n    bpy.types.Scene.tissue_cartography_batch_output_directory = StringProperty(\n        name=\"Batch Process Output Directory\",\n        description=\"Path to TIFF files directory\",\n        subtype='DIR_PATH',\n    )\n    bpy.types.Scene.tissue_cartography_batch_create_materials = BoolProperty(\n        name=\"Create materials\",\n        description=\"Enable or disable creating materials with projected texture in batch mode. Enabling can result in large .blend files.\",\n        default=True\n    )\n    bpy.types.Scene.tissue_cartography_prealign = BoolProperty(\n        name=\"Pre-align?\",\n        description=\"Enable or disable pre-alignment. Do not use if the two meshes are already closely aligned.\",\n        default=True\n    )\n    bpy.types.Scene.tissue_cartography_prealign_shear = BoolProperty(\n        name=\"Allow shear\",\n        description=\"Allow shear transformation during alignment.\",\n        default=True\n    )\n    bpy.types.Scene.tissue_cartography_align_type = EnumProperty(\n        name=\"Align Mode\",\n        description=\"Choose an axis\",\n        items=[('selected', \"Selected to Active\", \"Align selected meshes to active mesh.\"),\n               ('active', \"Active to Selected\", \"Align active mesh to selected meshe (creates copies of active mesh).\")],\n        default='selected'\n    )\n    bpy.types.Scene.tissue_cartography_align_iter = IntProperty(\n        name=\"Iterations\",\n        description=\"ICP iterations during alignment.\",\n        default=100,\n        min=1,\n    )\n    bpy.types.Scene.tissue_cartography_shrinkwarp_smooth = IntProperty(\n        name=\"Shrinkwrap Corrective Smooth\",\n        description=\"Corrective smooth iterations during shrink-wrapping.\",\n        default=10,\n        min=0,\n    )\n    bpy.types.Scene.tissue_cartography_shrinkwarp_iterative = EnumProperty(\n        name=\"Shrinkwrap Mode\",\n        description=\"Choose an axis\",\n        items=[('one-to-all', \"One-To-All\", \"Shrink-wrap active mesh to each selected individually\"),\n               ('forward', \"Iterative Forward\", \"Shrink-wrap active mesh to selected meshes iteratively, starting with alpha-numerically first\"),\n               ('backward', \"Iterative Backward\", \"Shrink-wrap active mesh to selected meshes iteratively, starting with alpha-numerically last\")],\n        default='one-to-all'\n    )\n\ndef unregister():\n    bpy.utils.unregister_class(TissueCartographyPanel)\n    bpy.utils.unregister_class(LoadTIFFOperator)\n    bpy.utils.unregister_class(LoadSegmentationTIFFOperator)\n    bpy.utils.unregister_class(CreateProjectionOperator)\n    bpy.utils.unregister_class(BatchProjectionOperator)\n    bpy.utils.unregister_class(SaveProjectionOperator)\n    bpy.utils.unregister_class(SlicePlaneOperator)\n    bpy.utils.unregister_class(VertexShaderInitializeOperator)\n    bpy.utils.unregister_class(VertexShaderRefreshOperator)\n    bpy.utils.unregister_class(AlignOperator)\n    bpy.utils.unregister_class(ShrinkwrapOperator)\n    bpy.utils.unregister_class(HelpPopupOperator)\n\n    del bpy.types.Scene.tissue_cartography_file \n    del bpy.types.Scene.tissue_cartography_resolution\n    del bpy.types.Scene.tissue_cartography_axis_order\n    del bpy.types.Scene.tissue_cartography_image_channels\n    del bpy.types.Scene.tissue_cartography_image_shape\n    del bpy.types.Scene.tissue_cartography_segmentation_file\n    del bpy.types.Scene.tissue_cartography_segmentation_resolution\n    del bpy.types.Scene.tissue_cartography_segmentation_axis_order\n    del bpy.types.Scene.tissue_cartography_segmentation_sigma\n    del bpy.types.Scene.tissue_cartography_segmentation_channels\n    del bpy.types.Scene.tissue_cartography_segmentation_shape\n    del bpy.types.Scene.tissue_cartography_offsets \n    del bpy.types.Scene.tissue_cartography_projection_resolution \n    del bpy.types.Scene.tissue_cartography_slice_axis \n    del bpy.types.Scene.tissue_cartography_slice_position \n    del bpy.types.Scene.tissue_cartography_slice_channel \n    del bpy.types.Scene.tissue_cartography_vertex_shader_offset \n    del bpy.types.Scene.tissue_cartography_vertex_shader_channel\n    del bpy.types.Scene.tissue_cartography_prealign \n    del bpy.types.Scene.tissue_cartography_prealign_shear\n    del bpy.types.Scene.tissue_cartography_align_iter\n    del bpy.types.Scene.tissue_cartography_align_type\n    del bpy.types.Scene.tissue_cartography_batch_directory\n    del bpy.types.Scene.tissue_cartography_batch_output_directory\n    del bpy.types.Scene.tissue_cartography_batch_create_materials\n    del bpy.types.Scene.tissue_cartography_shrinkwarp_smooth\n    del bpy.types.Scene.tissue_cartography_shrinkwarp_iterative\n    \n    if bpy.types.Scene.tissue_cartography_interpolators in globals():\n        del bpy.types.Scene.tissue_cartography_interpolators\n\n### Run the add-on\n\n\nif __name__ == \"__main__\":\n    register()",
    "crumbs": [
      "Blender add-on"
    ]
  },
  {
    "objectID": "Python library/io.html",
    "href": "Python library/io.html",
    "title": "Image I/O",
    "section": "",
    "text": "In this notebook, we define several functions for image and metadata loading and saving, as well as image normalization, and show how to use them with the data from the basics_example folder.\nsource",
    "crumbs": [
      "Python library",
      "Image I/O"
    ]
  },
  {
    "objectID": "Python library/io.html#image-reading-and-normalization",
    "href": "Python library/io.html#image-reading-and-normalization",
    "title": "Image I/O",
    "section": "Image reading and normalization",
    "text": "Image reading and normalization\n\nsource\n\nadjust_axis_order\n\n adjust_axis_order (image, channel_axis=None)\n\n*Adjust axis order of image (numpy array) so that the channel axis is axis 0.\nIf the channel axis is not specified, it is inferred as the axis with the smallest number of entries. this function adds a singleton dimension if the image contains a single channel. Axis order is otherwise left unchanged. Image must have 3 axes (single channel volumetric) or four axes (multichannel volumetric).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\n\n\n\n\n\nchannel_axis\nNoneType\nNone\n\n\n\nReturns\ntransposed image: np.ndarray with 4 axes\n\nInput image, channel now axis 0.\n\n\n\n\n\nLoad and subsample data for segmentation\nLet’s load an example dataset. We then enter the relevant metadata - the filename, resolution in microns, and how much we want to subsample for segmentation purposes.\n\nmetadata_dict = {'filename': 'datasets/basics_example/basics_example',\n                 'resolution_in_microns': (1, 0.36, 0.36), # you can typically get this from the .tif metadata\n                 'subsampling_factors': (1, 1/3, 1/3),\n                 'normal_offsets':np.linspace(-2, 2, 5) # normal offsets for map projection, in microns\n                }\n\n\nimage = adjust_axis_order(imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order\n\nimage shape: (2, 26, 454, 511)\n\n\n\nsource\n\n\nsubsample_image\n\n subsample_image (image, subsampling_factors,\n                  use_block_averaging_if_possible=True)\n\n*Subsample (downscale) image by given factors.\nReduce image size by given factors along each dimension. The subsampling_factors need to be smaller than 1. If the image is large, subsampling can be performed by block averaging, which is much faster. In this case, you need to use inverse integer rescaling factors (e.g. 1/2, 1/3). If the number of pixels is not divisible by those factors, the subsampled image is padded by 0.\nImportant: the chanel axis must be axis 0 (automatically done by adjust_axis_order)!*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\n\n\n\n\n\nsubsampling_factors\nlist or tuple of float or int\n\nSubsampling factors along each axis. A factor of 1/2 will reduce image size by 2x along that axis.\n\n\nuse_block_averaging_if_possible\nbool\nTrue\nUse fast block averaging if subsampling_factors are inverses of integers.\n\n\nReturns\nsubsampled_image: np.ndarray\n\nSubsampled imaged\n\n\n\n\nsource\n\n\nnormalize_mean_std\n\n normalize_mean_std (image, channel_axis=None)\n\nNormalize a multi-dimensional image by setting mean to 0 and std dev to 1.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\nnp.array\n\nMulti-dimensional image. The first axis needs to be the channel axis.\n\n\nchannel_axis\nNoneType\nNone\nIf None, the image is assumed to have only a single channel.If int, indicates the position of the channel axis. Each channel is normalized separately.\n\n\nReturns\nnp.array\n\nNormalized image, the same shape as input\n\n\n\n\nsource\n\n\nnormalize_quantiles\n\n normalize_quantiles (image, quantiles=(0.01, 0.99), channel_axis=None)\n\nNormalize a multi-dimensional image by setting given quantiles to 0 and 1.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\nnp.array\n\nMulti-dimensional image.\n\n\nquantiles\ntuple\n(0.01, 0.99)\nImage quantile to set to 0 and 1.\n\n\nchannel_axis\nNoneType\nNone\nIf None, the image is assumed to have only a single channel.If int, indicates the position of the channel axis. Each channel is normalized separately.\n\n\nReturns\nnp.array\n\nNormalized image, the same shape as input\n\n\n\n\nsource\n\n\nread_h5\n\n read_h5 (filename)\n\nRead .h5 file (e.g. ilastik output) into numpy array. Loads alphabetically first entry in .h5.\n\nsource\n\n\nwrite_h5\n\n write_h5 (filename, image, h5_dataset_name='image', axis_order='CZYX')\n\nWrite image (numpy array) as .h5 file (e.g. as input for ilastik).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilename\nstr\n\n\n\n\nimage\nnp.array\n\nMulti-dimensional array\n\n\nh5_dataset_name\nstr\nimage\n\n\n\naxis_order\nstr\nCZYX\n“Meaning” of each array axis. ‘C’=channel, ‘X’,‘Y’,‘Z’= spatial axis,‘T’=time. This is important if you want to use the .h5 as input foran ilastik model.\n\n\nReturns\nNone\n\n\n\n\n\n\n\nCreating a 3d segmentation\nNow create a 3d segmentation, in this case using ilastik. ilastik works best with input saved as .h5 data sets. We first subsample the data, and the save it as a .h5.\n\nsubsampled_image = subsample_image(image, metadata_dict['subsampling_factors'],\n                                  use_block_averaging_if_possible=False)\nprint(\"subsampled image shape:\", subsampled_image.shape)\n\nsubsampled image shape: (2, 26, 151, 170)\n\n\n\n# We now save the subsampled image as a .h5 file for input into ilastik for segmentation\n\nwrite_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image)\n\n\n# After creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n\nsegmentation = read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # Select the first channel of the segmentation - it's the probability that a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (26, 151, 170)\n\n\n\n# look at the segmentation in a cross-section\n\nplt.imshow(segmentation[:,:,50], vmin=0, vmax=1)",
    "crumbs": [
      "Python library",
      "Image I/O"
    ]
  },
  {
    "objectID": "Python library/io.html#image-and-metadata-writing",
    "href": "Python library/io.html#image-and-metadata-writing",
    "title": "Image I/O",
    "section": "Image and metadata writing",
    "text": "Image and metadata writing\n\nsource\n\nsave_dict_to_json\n\n save_dict_to_json (filename, dictionary)\n\n*Save dictionary to .json file.\nWill automatically convert numpy arrays to lists for saving. If you get an error like “XXX is not JSON serializable”, you need to ensure all your dictionary items are things that can be saved to text by json (strings, numbers, lists).*\n\n\n\n\nType\nDetails\n\n\n\n\nfilename\nstr\nFilename to save to\n\n\ndictionary\ndict\nDictionary to save\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nsave_stack_for_blender\n\n save_stack_for_blender (image, directory, normalization=(0.01, 0.99))\n\n*Save multichannel volumetric image as a series of grayscale .png images. Can normalize data if desired.\nThis function necessarily converts the image to 8 bit. Use a suitable normalization to ensure nothing is lost.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\n4d np.array\n\nAxis 0 is assumed to be the channel axis, axis 1 is the slicing axes, i.e. images will correspond toslices along axis 1.\n\n\ndirectory\nstr\n\nPath to save data to. Will create a directory if it doesn’t exist\n\n\nnormalization\ntuple\n(0.01, 0.99)\nWhether to normalize the image before saving it. If None, no normalization is performed. If atuple is given, it will be interpreted as quantiles to set to 0 and 255, respectively (over thewhole channel, not each slice). If a callable is provided, it will be applied to each channel.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nnormalize_quantiles_for_png\n\n normalize_quantiles_for_png (image, quantiles=(0.01, 0.99))\n\n*Normalize an image by setting given quantiles to 0 and 255 and converting to 8-bit, for saving as .png\nAlso replaces nan by 0.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\nnp.array\n\nImage (should be single-channel)\n\n\nquantiles\ntuple\n(0.01, 0.99)\nImage quantile to set to 0 and 255.\n\n\nReturns\nnp.array\n\nNormalized image, datatype np.uint8\n\n\n\n\nsource\n\n\nsave_for_imageJ\n\n save_for_imageJ (filename, image, z_axis=None, channel_axis=None)\n\n*Save image as 32bit ImageJ compatible .tif file\nIf channel_axis is not provided, it is inferred as the shortest axis. If z_axis is provided for a 4d array, it will be set as the default z-axis for ImageJ.*\n\n\nSaving results\nWe want to save the cartographic projections we will create both as .tif stack for quantitative analysis and as .png’s for visualization as mesh texture in blender. We will also save the metadata to a .json file\nAnnoyingly, we have to normalize our data and convert it to 8-bit to save it as png.\n\n# save metadata\nsave_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)\n\n\n# read some data so we can check the saving function\nprojected_data = adjust_axis_order(imread(f\"{metadata_dict['filename']}_projected.tif\"))\nprint(\"Image shape\", projected_data.shape)\n\nImage shape (2, 5, 256, 256)\n\n\n\nsave_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data, z_axis=1)\n\n\ntexture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\nsave_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))",
    "crumbs": [
      "Python library",
      "Image I/O"
    ]
  },
  {
    "objectID": "Python library/02_cartographic_interpolation.html",
    "href": "Python library/02_cartographic_interpolation.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Project volumetric image onto a surface and map it to 2d using UV map.\n\nThis module is the “heart” of blender_tissue_cartography - the rest is just add ons. We illustrate it with a basic example of tissue cartography workflow.\nIn this notebook, we go through a basic example of tissue cartography - extracting the mildly curved surface of an epithelium from a confocal \\(z\\)-stack. This example data is taken from Lye et al. 2024, available here.\nWe introduce the functions of our python module for cartographic interpolation one by one.\n\n\n\nImage axis 0 is always the channel. All other axes are not permuted\nMesh coordinates are always saved in microns.\nThe UV map (the map of our surface mesh to a cartographic plane) always maps into the unit square, \\(u\\in[0,1], \\; v\\in[0,1]\\). All of our projections will be square images (with transparent regions for parts of the UV square not covered by the unwrapped mesh)",
    "crumbs": [
      "Python library",
      "Cartographic interpolation"
    ]
  },
  {
    "objectID": "Python library/02_cartographic_interpolation.html#cartographic-interpolation",
    "href": "Python library/02_cartographic_interpolation.html#cartographic-interpolation",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Project volumetric image onto a surface and map it to 2d using UV map.\n\nThis module is the “heart” of blender_tissue_cartography - the rest is just add ons. We illustrate it with a basic example of tissue cartography workflow.\nIn this notebook, we go through a basic example of tissue cartography - extracting the mildly curved surface of an epithelium from a confocal \\(z\\)-stack. This example data is taken from Lye et al. 2024, available here.\nWe introduce the functions of our python module for cartographic interpolation one by one.\n\n\n\nImage axis 0 is always the channel. All other axes are not permuted\nMesh coordinates are always saved in microns.\nThe UV map (the map of our surface mesh to a cartographic plane) always maps into the unit square, \\(u\\in[0,1], \\; v\\in[0,1]\\). All of our projections will be square images (with transparent regions for parts of the UV square not covered by the unwrapped mesh)",
    "crumbs": [
      "Python library",
      "Cartographic interpolation"
    ]
  },
  {
    "objectID": "Python library/02_cartographic_interpolation.html#example-dataset",
    "href": "Python library/02_cartographic_interpolation.html#example-dataset",
    "title": "blender-tissue-cartography",
    "section": "Example dataset",
    "text": "Example dataset\n\nLoad and subsample data for segmentation\nData description myosin + membrane ventral view of Drosophila embryo during germband extension, from Lye et al. 2024.\nWe begin by creating a directory for our project where we’ll save all related files (and normally, the jupyter notebook used to generate them!).\nLet’s load the dataset. We then enter the relevant metadata - the filename, resolution in microns, and how much we want to subsample for segmentation purposes.\n\nmetadata_dict = {'filename': 'datasets/basics_example/basics_example',\n                 'resolution_in_microns': (1, 0.36, 0.36), # you can typically get this from the .tif metadata\n                 'subsampling_factors': (1, 1/3, 1/3)}\n\n\nmetadata_dict['subsampling_factors']\n\n(1, 0.3333333333333333, 0.3333333333333333)\n\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order\n\nimage shape: (2, 26, 454, 511)\n\n\n\nsubsampled_image = tcio.subsample_image(image, metadata_dict['subsampling_factors'],\n                                        use_block_averaging_if_possible=False)\nprint(\"subsampled image shape:\", subsampled_image.shape)\n\nsubsampled image shape: (2, 26, 151, 170)\n\n\n\n\nCreate 3d segmentation\nNow create a 3d segmentation, in this case using ilatik. We use ilastik binary pixel classification. We could post-process the ilastik output here, for example using morphsnakes. We then load the segmentation back into the jupyter notebook.\nAttention: when importing the .h5 into ilastik, make sure the dimension order is correct! In this case, CZYX (where C=channel), for both export and import. You can include this metadata in the .h5:\n\n# We now save the subsampled image as a .h5 file for input into ilastik for segmentation\n\ntcio.write_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image, axis_order='CZYX')\n\n\n# After creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n\nsegmentation = tcio.read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # Select the first channel of the segmentation - it's the probability a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (26, 151, 170)\n\n\n\n# look at the segmentation in a cross-section\n\nplt.imshow(segmentation[:,:,50], vmin=0, vmax=1)\n\n\n\n\n\n\n\n\n\n\nMeshing\nWe convert the segmentation into a triangular mesh using the marching cubes method and save the mesh. We save all meshes as wavefront .obj files (see wikipedia). In Python, we represent missing entries (such as a vertex that doesn’t have a normal by np.nan.\nImportant convention For sanity’s sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n\n# Now we create a 3d mesh using the marching cubes method\n\nvertices, faces = tcremesh.marching_cubes(segmentation, isovalue=0.5, sigma_smoothing=3)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n                                 /np.array(metadata_dict['subsampling_factors']))\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"basics_example_mesh_marching_cubes\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")\n\n\nOptional - mesh compression using igl\nThe mesh returned by the marching cubes method is normally much denser than necessary. You can automatically reduce its size here in Python, or later in Blender.\n\nmesh_compressed = tcremesh.qslim(mesh, max_n_faces=int(faces.shape[0]/2))\nmesh_compressed.name = \"basics_example_mesh_marching_cubes_compressed\"\nmesh_compressed.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes_compressed.obj\")\n\n\n\nOptional - improve mesh quality using MeshLab\nWe can remesh the output of the marching cubes algorithm to obtain an improved mesh, i.e. with more uniform triangle shapes. In this example, we first remesh to make the mesh more uniform. You can also try this out in the MeshLab GUI and export your workflow as a Python script. Be careful not to move the mesh or it will mess up the correspondence with the pixel coordinates!\nSee List of MeshLab filers\n\nmesh_remeshed = tcremesh_pymeshlab.remesh_pymeshlab(mesh)\nmesh_remeshed.write_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\n\nTo check all went well, let’s overlay the mesh coordinates over a cross-section of the image. To do so, we first select the vertices whose positions correspond to the image slice, and then correctly rescale coordinates from microns to pixel coordinates.\n\nsource\n\n\n\nget_cross_section_vertices_normals\n\n get_cross_section_vertices_normals (slice_axis, slice_index, image, mesh,\n                                     resolution, get_normals=True,\n                                     width=3)\n\n*Get mesh vertices and normals for diagnostic cross-section overlay plots.\nUsage example:\n`\nslice_image, slice_vertices, slice_normals = get_cross_section_vertices_normals(1, 100,\n                            image, mesh, metadata_dict[\"resolution_in_microns\"])\nplt.scatter(*slice_vertices.T, s=5, c=\"tab:red\")\nplt.quiver(*slice_vertices.T, *slice_normals.T, color=\"tab:red\")\nplt.imshow(slice_image[0], vmax=10000, origin=\"lower\")\n`\nNote: origin=\"lower\" in plt.imshow() is essential for a correctly oriented plot in Python!!*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nslice_axis\nint, 0,1,2\n\nAxis along which to slice image array\n\n\nslice_index\nint\n\nIndex along the sliced axis\n\n\nimage\n4d np.ndarray of shape (channels, n_x, n_y, n_z)\n\nImage. Axis 0 is channel\n\n\nmesh\ntcmesh.ObjMesh\n\nMesh\n\n\nresolution\nint, default 256\n\nResolution in pixels/micron.\n\n\nget_normals\nbool\nTrue\nWhether to return normals also\n\n\nwidth\nint\n3\nWidth of slice for vertex selection, in microns\n\n\nReturns\nnp.array, np.array, np.array\n\nslice_image : np.array  Slice of image. Axis 0 is channelslice_vertices : np.array Projected vertices in the slice. The second axis is the coordinate one.slice_normals : 2d np.array (…, 2) Projected normals in slice. The second axis is the coordinate one.\n\n\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\n\n\nslice_image, slice_vertices = get_cross_section_vertices_normals(2, 200,\n                                image, mesh, metadata_dict[\"resolution_in_microns\"], get_normals=False)\n\n\nfig = plt.figure(figsize=(10,10))\nplt.scatter(*slice_vertices[:,::-1].T, s=5, c=\"tab:red\")\nplt.imshow(slice_image[0].T, origin=\"lower\")",
    "crumbs": [
      "Python library",
      "Cartographic interpolation"
    ]
  },
  {
    "objectID": "Python library/02_cartographic_interpolation.html#uv-mapping-in-blender",
    "href": "Python library/02_cartographic_interpolation.html#uv-mapping-in-blender",
    "title": "blender-tissue-cartography",
    "section": "UV-mapping in blender",
    "text": "UV-mapping in blender\nWe now switch to blender and create a new empty project, which we will call f\"{metadata_dict['filename']}.blend\". We import the mesh just generated (File-&gt;Import).\n\n\n\nimage-3.png\n\n\nI recommend using the “object” tab (orange square on the right toolbar) to lock mesh position and rotation so we don’t accidentally move it.\nLet’s try to move forward and get a UV map of the mesh. To do so, we go to the “UV Editing” tab on the top toolbar, and press “3” then “A” to select all faces (“1” selects vertices, “2” edges, and “3” faces). Click “UV-&gt;unwrap” on the top panel.  For more complicated meshes (e.g. a sphere), we will need to use extra steps, e.g. define seams.\n\nBlender export\nWe then click on “File-&gt;Export” and save as .obj with UV and normals:\n\n\n\nimage.png\n\n\nA few things are important: - Always include UV and normals. Otherwise, cartographic projection will fail! - Only export selected items! With a more complicated blender project, you might end up exporting multiple meshes. This will trip up the cartographic projection algorithm. - Export as triangulated mesh, since many of this package’s tools for more advanced examples work best/only with triangular meshes. This option will subdivide any quads/polygons your mesh may have.\nThe new mesh file f\"{metadata_dict['filename']}_mesh_uv.obj\" now contains vertex normals and UV coordinates as vn and vt lines. Note - there can be more vt’s than v’s.",
    "crumbs": [
      "Python library",
      "Cartographic interpolation"
    ]
  },
  {
    "objectID": "Python library/02_cartographic_interpolation.html#interpolation-onto-uv-grid",
    "href": "Python library/02_cartographic_interpolation.html#interpolation-onto-uv-grid",
    "title": "blender-tissue-cartography",
    "section": "Interpolation onto UV grid",
    "text": "Interpolation onto UV grid\nWe now read in the new .obj file to interpolate the image data onto the 3d mesh. We first introduce the functions necessary to do so, which are based on the scipy.interpolation module. Interpolation proceeds in two steps: 1. Interpolate the 3d coordinates from the mesh UV vertex positions onto the whole UV grid 2. Evaluate the image signal at the UV gridded 3d coordinates using a second interpolation step. This ensures that the resolution of the cartographic projection is not limited by the resolution of the mesh.\nThe UV grid always covers the unit square \\([0,1]^2\\).\nWe first show how the interpolation works in a step-by-step manner, and then give a function that packages the whole process.\n\n# let's read in the mesh and match up the vertices, texture vertices, and normal vectors using the\n# mesh connectivity information\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_uv.obj\")\n\n# let's also load the image\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n\n\n\n# let's make a scatter plot of the mesh texture coordinates, and color it by 3d coordinate values\n\nfig = plt.figure(figsize=(4,4),)\nplt.scatter(*mesh.texture_vertices.T, s=0.2, c=mesh.vertices[mesh.get_vertex_to_texture_vertex_indices(),2])\nplt.axis(\"equal\");\n\n\n\n\n\n\n\n\nIt’s not very noticeable in this example, but the part of the UV square covered by the unwrapped mesh can be extremely non-convex, in particular, if there are multiple patches (see notebook 3). Here is an example: \nWe set values outside the UV “islands” to np.nan, since they are undefined. For non-triangular meshes, this requires a little hack to generate UV layout mask which delineates the part of the UV square covered by the unwrapped mesh. For triangular meshes, our barycentric interpolation method can take care of this issue by construction.\n\nsource\n\nget_uv_layout_mask_mask\n\n get_uv_layout_mask_mask (mesh:blender_tissue_cartography.mesh.ObjMesh,\n                          uv_grid_steps=256)\n\n*Get a layout mask of the UV square: 1 where the UV square is covered by the unwrapped mesh, 0 outside.\nBased on matplotlib hack - this function works for non-triangular meshes.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nMesh with texture_vertices\n\n\nuv_grid_steps\nint\n256\nSize of UV grid. Determines resolution of result.\n\n\nReturns\nnp.array of shape (uv_grid_steps, uv_grid_steps)\n\nMask of the part of the UV square covered by the unwrapped mesh\n\n\n\n\nuv_mask = get_uv_layout_mask_mask(mesh, uv_grid_steps=256)\n\nfig = plt.figure(figsize=(4,4),)\nplt.imshow(uv_mask, cmap='binary', alpha=0.5, extent=(0,1,0,1))\nplt.scatter(*mesh.texture_vertices.T, s=0.2, c=mesh.vertices[mesh.get_vertex_to_texture_vertex_indices(),2])\nplt.axis(\"equal\");\n\n\n\n\n\n\n\n\n\nsource\n\n\ninterpolate_barycentric\n\n interpolate_barycentric (points, vertices, faces, values,\n                          distance_threshold=inf)\n\n*Interpolate values defined on triangular mesh vertices onto points using barycentric interpolation.\nCan handle triangular meshes in both 3d and 2d. Points not on the triangular mesh are projected onto the closest point. Points more distant than the distance_threshold will be set to np.nan. The data on the triangular mesh must be defined per vertex and can have any number of axes (scalars, vectors, tensors, …).\nThis function can also be used to transfer values defined on one mesh to another mesh’s vertices (if the two meshes are well-aligned in space).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npoints\nnp.array of shape (n_points, 2) or (n_points, 3)\n\nPoints at which to evaluate\n\n\nvertices\nnp.array of shape (n_vertices, 2) or (n_vertices, 3)\n\nMesh vertices\n\n\nfaces\nnp.array of shape (n_faces, 3)\n\nMesh triangles, indices into vertices array\n\n\nvalues\nnp.array of shape (n_vertices, …)\n\nValues at mesh vertices. Can have any number of additional axes.\n\n\ndistance_threshold\nfloat\ninf\nPoints with a squared distance to mesh &gt; distance_thresholdare set to np.nan\n\n\nReturns\nnp.array of shape (n_points, …)\n\nValues at points.\n\n\n\n\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_uv.obj\")\n\nuv_grid_steps = 1024\nu, v = 2*[np.linspace(0,1, uv_grid_steps),]\nUV = np.stack(np.meshgrid(u, v), axis=-1).reshape((-1, 2))\n \ninterpolated = interpolate_barycentric(UV, mesh.texture_vertices, mesh.texture_tris,\n                                       mesh.vertices[mesh.get_vertex_to_texture_vertex_indices()],\n                                       distance_threshold=1e-5)\ninterpolated = interpolated.reshape((uv_grid_steps, uv_grid_steps, 3))[::-1]\n\n# add flip check and warn!\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n\n\n\n# plot the interpolated positions\nfig, (ax1, ax2) = plt.subplots(figsize=(8,4), ncols=2)\n\nax1.imshow(interpolated[...,1])\nax2.imshow(interpolated[...,2])\n\n\n\n\n\n\n\n\n\nsource\n\n\ninterpolate_per_vertex_field_to_UV\n\n interpolate_per_vertex_field_to_UV (mesh, field, domain='per-vertex',\n                                     uv_grid_steps=256, map_back=True,\n                                     distance_threshold=0.0001,\n                                     use_fallback=False)\n\n*Interpolate a field defined per-vertex into the UV square.\nThe field can be defined per texture-vertex or per 3D-vertex. Make sure you use the right option!\nAssumes the map x,y,z -&gt; u,v to be invertible. This is not guaranteed - you can create overlapping UV coordinates in Blender. Raises RuntimeWarning if any of the triangles in the UV map are flipped, indicating self-intersections.\nThe provided UV coordinates will be mapped back to [0, 1]**2 if map_back is True. Else, coordinates outside [0,1] are ignored.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\ntcmesh.ObjMesh\n\nInput mesh with UV coordinates.\n\n\nfield\nnp.array of shape (mesh.texture_vertices.shape[0],…)\n\nInput field. Can be an array with any number of axes (e.g. scalar or vector field).\n\n\ndomain\nstr\nper-vertex\nWhether the field is defined per-vertex or per texture vertex.\n\n\nuv_grid_steps\nint\n256\nSize of UV grid. Determines resolution of result.\n\n\nmap_back\nbool\nTrue\nMap back the UV coordinates to [0,1]**2. Else, coordinates outside [0,1] are ignored.\n\n\ndistance_threshold\nfloat\n0.0001\nPoints at a squared distance &gt; distance_threshold in the UV square are considered“outside” the unwrapped mesh and are set to np.nan.\n\n\nuse_fallback\nbool\nFalse\nIgnore mesh connectivity when interpolating. This is to be used as a fallbackif you have a UV map with lots of flipped triangles (i.e. self-intersections).If ‘auto’, the fallback option is chosen automatically if there are any flipped triangles.\n\n\nReturns\nnp.array of shape (uv_grid_steps, uv_grid_steps, …)\n\nField across [0,1]2 UV grid, with a uniform step size. UV positions that don’tcorrespond to any value are set to np.nan.**\n\n\n\n\n# try an example\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_uv.obj\")\nfield = mesh.vertices[:,:] # Let's consider the coordinates of the 3d vertex\nmesh.texture_vertices[2355] -= .2 # Let's trip the warning by messing up the UV map\n\ninterpolated = interpolate_per_vertex_field_to_UV(mesh, field, \"per-vertex\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n/tmp/ipykernel_1939637/2667590714.py:49: RuntimeWarning: UV map has self-intersections, 3 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n\n\n\nplt.imshow(interpolated[:,:,1])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\nsource\n\n\ninterpolate_UV_to_per_vertex_field\n\n interpolate_UV_to_per_vertex_field (mesh, field, domain='per-vertex')\n\n*Interpolate a field defined by gridded values across UV square onto mesh vertices.\nThis is useful for downstream geometric analysis. For example, you compute a vector field on a grid of the UV square and now want to get its values at the mesh vertices for geometric analysis.\nThere may be some np.nans at the mesh boundary!\nThe result can be defined per texture-vertex or per 3D-vertex. Make sure you use the right option!*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\ntcmesh.ObjMesh\n\nInput mesh with UV coordinates.\n\n\nfield\nnp.array of shape (uv_grid_steps, uv_grid_steps,…)\n\nInput field. Can be an array with any number of axes (e.g. scalar or vector field).Must be defined on a square grid with uniform step size of the UV square.\n\n\ndomain\nstr\nper-vertex\nWhether the result will be defined per-vertex or per texture vertex.If per-vertex, the values corresponding to all texture vertices thatmap to a vertex are averaged.\n\n\nReturns\nnp.array of shape (n_vertices, …)\n\nField evaluated at mesh vertices.\n\n\n\n\nsource\n\n\ninterpolate_volumetric_data_to_uv\n\n interpolate_volumetric_data_to_uv (image, interpolated_3d_positions,\n                                    resolution)\n\n*Interpolate volumetric image data onto UV coordinate grid.\nUses 3d positions corresponding to each UV grid point as computed by interpolate_per_vertex_field_to_UV. 3d coordinates (in microns) are converted into image coordinates via the scaling factor.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimage\n4d np.array\nImage, axis 0 is assumed to be the channel axis\n\n\ninterpolated_3d_positions\nnp.array of shape (uv_grid_steps, uv_grid_steps, 3)\n3d positions across [0,1]^2 UV grid, with uniform step size. UV positions that don’t correspond to any value are set to np.nan.\n\n\nresolution\nnp.array of shape (3,)\nResolution in pixels/microns for each of the three spatial axes.\n\n\nReturns\nnp.array of shape (n_channels, uv_grid_steps, uv_grid_steps)\n3d volumetric data interpolated onto UV grid.\n\n\n\n\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_uv.obj\")\n#mesh.texture_vertices[2355] -= .2 # trigger the fallback interpolation method by messing up UV map\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n\n\n\n# first interpolation step \n\nuv_grid_steps = 256  # set UV grid size\nuv_mask = ndimage.binary_erosion(get_uv_layout_mask_mask(mesh, uv_grid_steps=uv_grid_steps), iterations=3)\n\n# this is our UV grid\nu, v = 2*[np.linspace(0,1, uv_grid_steps),]\nU, V = np.meshgrid(u, v)\n\ninterpolated_3d_positions = interpolate_per_vertex_field_to_UV(mesh, mesh.vertices, domain=\"per-vertex\",\n                                                               uv_grid_steps=uv_grid_steps, map_back=True)\ninterpolated_normals = interpolate_per_vertex_field_to_UV(mesh, mesh.normals, domain=\"per-vertex\",\n                                                          uv_grid_steps=uv_grid_steps, map_back=True)\n\n\ninterpolated_3d_positions = interpolate_per_vertex_field_to_UV(mesh, mesh.vertices, domain=\"per-vertex\",\n                                                               uv_grid_steps=uv_grid_steps, map_back=True)\ninterpolated_normals = interpolate_per_vertex_field_to_UV(mesh, mesh.normals, domain=\"per-vertex\",\n                                                          uv_grid_steps=uv_grid_steps, map_back=True)\n\nCPU times: user 228 ms, sys: 4.21 ms, total: 233 ms\nWall time: 146 ms\n\n\n\nnp.linalg.norm(mesh.normals, axis=-1).min()\n\n0.9694902988837123\n\n\n\nnp.nanmin(np.linalg.norm(interpolated_normals, axis=-1))\n\n0.953786333836412\n\n\n\n# plot the interpolated positions\nfig, (ax1, ax2) = plt.subplots(figsize=(8,4), ncols=2)\n\nax1.imshow(interpolated_3d_positions[...,1])\nax2.imshow(interpolated_3d_positions[...,2])\n\n\n\n\n\n\n\n\n\n# plot the interpolated normals\nfig, (ax1, ax2) = plt.subplots(figsize=(8,4), ncols=2)\n\nax1.imshow(interpolated_normals[...,1])\nax2.imshow(interpolated_normals[...,2])\n\n\n\n\n\n\n\n\n\nsource\n\n\ninterpolate_volumetric_data_to_uv_multilayer\n\n interpolate_volumetric_data_to_uv_multilayer (image,\n                                               interpolated_3d_positions,\n                                               interpolated_normals,\n                                               normal_offsets, resolution)\n\n*Multilayer-interpolate volumetric image data onto UV coordinate grid.\nUses 3d positions corresponding to each UV grid point as computed by interpolate_per_vertex_field_to_UV. 3d coordinates (in microns) are converted into image coordinates via the scaling factor.\nGenerates multiple “layers” by shifting surface along its normals.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimage\n4d np.array\nImage, axis 0 is assumed to be the channel axis\n\n\ninterpolated_3d_positions\nnp.array of shape (uv_grid_steps, uv_grid_steps, 3)\n3d positions across [0,1]^2 UV grid, with uniform step size. UV positions that don’t correspond to any value are set to np.nan.\n\n\ninterpolated_normals\nnp.array of shape (uv_grid_steps, uv_grid_steps, 3)\n3d normals across [0,1]^2 UV grid, with uniform step size. UV positions that don’t correspond to any value are set to np.nan. Normal vectors will be automatically normalized.\n\n\nnormal_offsets\nnp.array of shape (n_layers,)\nOffsets along normal direction, in same units as interpolated_3d_positions (i.e. microns).0 corresponds to no shift.\n\n\nresolution\nnp.array of shape (3,)\nResolution in pixels/microns for each of the three spatial axes.\n\n\nReturns\nnp.array of shape (n_channels, n_layers, uv_grid_steps, uv_grid_steps)\n3d volumetric data multilayer-interpolated onto UV grid.\n\n\n\n\n# second interpolation step. here we have to include the conversion factor from microns back to pixels\n\ninterpolated_data = interpolate_volumetric_data_to_uv(image, interpolated_3d_positions,\n                                                      metadata_dict['resolution_in_microns'])\n\n\nplt.imshow(interpolated_data[1])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\n# now we can create a multilayer projection by  shifting the surface according to the vertex normals\n\nnormal_offsets = np.linspace(-2, 2, 5) # offset in microns\n\ninterpolated_data_multilayer = interpolate_volumetric_data_to_uv_multilayer(image,\n                                                                 interpolated_3d_positions,\n                                                                 interpolated_normals,\n                                                                 normal_offsets,\n                                                                 metadata_dict['resolution_in_microns'])\nprint(\"Shape: axis 0 is channel, axis 1 is normal offset\", interpolated_data_multilayer.shape)\n\nShape: axis 0 is channel, axis 1 is normal offset (2, 5, 256, 256)\n\n\n\nplt.imshow(interpolated_data_multilayer[1, 0])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\n\nCartographic projection\nLet’s package the interpolation workflow up into a single function.\n\nsource\n\n\ncreate_cartographic_projections\n\n create_cartographic_projections (image, mesh, resolution,\n                                  normal_offsets=(0,), uv_grid_steps=256,\n                                  map_back=True, use_fallback='auto')\n\n*Create multilayer cartographic projections of an image using mesh.\nComputes multiple layers along the surface normal, with given normal offsets (in microns). 0 offset corresponds to no shift away from the mesh. Also computes 3d positions (in microns) and surface normals interpolated onto the UV grid.\nUV positions that don’t correspond to any 3d position are set to np.nan.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\nstr or 4d np.array\n\nImage, either as a path to a file or as an array. If array, axis 0 is assumed to be the channel axis\n\n\nmesh\nstr or tcmesh.ObjMesh\n\nMesh, either as path to file, or as ObjMesh object.\n\n\nresolution\nnp.array of shape (3,)\n\nImage resolution in pixels/micron for the three spatial axes\n\n\nnormal_offsets\ntuple\n(0,)\nOffsets along normal direction, in same units as interpolated_3d_positions (i.e. microns).0 corresponds to no shift.\n\n\nuv_grid_steps\nint\n256\nSize of UV grid. Determines resolution of result.\n\n\nmap_back\nbool\nTrue\nMap back the UV coordinates to [0,1]^2. Else, coordinates outside [0,1] are ignored.\n\n\nuse_fallback\nstr\nauto\nIgnore mesh connectivity when interpolating. This is to be used as a fallbackif you have a UV map with lots of flipped triangles (i.e. self-intersections).If ‘auto’, the fallback option is chosen automatically if there are any flipped triangles.\n\n\nReturns\nnp.array, np.array, np.array\n\ninterpolated_data : np.array of shape (n_channels, n_layers, uv_grid_steps, uv_grid_steps) 3d volumetric data multilayer-interpolated across [0,1]^2 UV grid, with uniform step size.interpolated_3d_positions : np.array of shape (uv_grid_steps, uv_grid_steps, 3) 3d positions across [0,1]^2 UV grid, with uniform step size. interpolated_normals : np.array of shape (uv_grid_steps, uv_grid_steps, 3) Normals across [0,1]^2 UV grid, with a uniform step size.\n\n\n\n\nnormal_offsets = np.linspace(-2, 2, 5) # in microns\n\n\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_uv.obj\")\nmesh.texture_vertices[2355] -= .2 # trigger the fallback interpolation method by messing up UV map\n\nWarning: readOBJ() ignored non-comment line 4:\n  o basic_example_mesh_remeshed\n\n\n\nprojected_data, projected_coordinates, projected_normals = create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    #mesh=f\"{metadata_dict['filename']}_mesh_uv.obj\",\n    mesh=mesh,\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=256,\n    use_fallback='auto')\n\n/tmp/ipykernel_1939637/2667590714.py:49: RuntimeWarning: UV map has self-intersections, 3 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n/tmp/ipykernel_1939637/2667590714.py:58: DeprecationWarning: Use of non-triangular meshes is discouraged\n  warnings.warn(\"Use of non-triangular meshes is discouraged\", DeprecationWarning)\n\n\n\n# let's add the normal offset we want to our metadata - it will be important for analysis!\nmetadata_dict[\"normal_offsets\"] = normal_offsets\n\n\nplt.imshow(projected_data[1, 0])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\n\nSaving and visualizing the results\nWe can now save the cartographic projections as .tif stack for quantitative analysis and as .png’s for visualization as mesh texture in Blender. We will also save the metadata to a .json file\nAnnoyingly, we have to normalize our data and convert it to 8-bit to save it as png.\n\n# save metadata\ntcio.save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)\n\n\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data, z_axis=1)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_3d_coordinates.tif\", projected_coordinates)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_normals.tif\", projected_normals)\n\n\ntexture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\ntcio.save_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))\n\n\n# let's make a max projection of each channel and save them also\n\nmax_projected_ch_0, max_projected_ch_1 = projected_data.max(axis=1)\ntcio.imsave(f'{texture_path}/max_channel_0.png', tcio.normalize_quantiles_for_png(max_projected_ch_0))\ntcio.imsave(f'{texture_path}/max_channel_1.png', tcio.normalize_quantiles_for_png(max_projected_ch_1))\n\n\nVisualization in blender\nYou can set up textures in the “shading” tab:\n\n\n\nimage-3.png\n\n\nI find it helpful to remove the distracting “world” background, which you can do by either enabling “scene world” or setting “world opacity” to 0 in the viewport shading settings (arrow above “options” in the top right corner)\nGo to the bottom panel, add a new material (center top of bottom panel), and press “shift+A” to add a new shader element (the search bar is very helpful). Here is an example configuration mixing two channels as red and green:\n\n\n\nimage-4.png\n\n\nAnd there we go! Many further options exist to make more sophisticated renderings.",
    "crumbs": [
      "Python library",
      "Cartographic interpolation"
    ]
  },
  {
    "objectID": "Python library/remeshing_pymeshlab.html",
    "href": "Python library/remeshing_pymeshlab.html",
    "title": "Mesh creation and remeshing with MeshLab",
    "section": "",
    "text": "This notebook contains functions for (a) creating a mesh from a point cloud, (b) subdividing meshes while preserving UV information, and (c) remeshing (which destroys UV information), using pymeshlab.\nCompared to the igl-based remesh module, there are some more powerful remeshing algorithms: isotropic remeshing, which creates a new mesh with (close-to) equal-sized, equilateral triangles (great for improving a marching cubes mesh), and adaptive subdivision (subdivide long edges only).\nConstructing a mesh from a point cloud (rather than from a volumetric using marching cubes) is sometimes useful when the surface you are interested in is not the boundary of a volume (e.g. a membrane floating in free space).\nSubdivision is useful since it allows one to increase or decrease the mesh “resolution” as needed when the mesh is deformed. The advantage of pymeshlab-based subdivision is that it allows subdividing “long” edges only, while igl-based subdivision subdivides all edges, which (exponentially) increases the number of your vertices.",
    "crumbs": [
      "Python library",
      "Mesh creation and remeshing with MeshLab"
    ]
  },
  {
    "objectID": "Python library/remeshing_pymeshlab.html#poisson-reconstruction",
    "href": "Python library/remeshing_pymeshlab.html#poisson-reconstruction",
    "title": "Mesh creation and remeshing with MeshLab",
    "section": "Poisson reconstruction",
    "text": "Poisson reconstruction\nPoisson reconstruction is an algorithm to reconstruct a mesh from a point cloud. It requires the points to be equipped with normal vectors, which are computed in the first step.\nThis is useful for surfaces that are not the boundary of a solid volume, like a membrane.\n\n# let's look at an example first. let's load the example segmentation\n\nsegmentation = tcio.read_h5(f\"datasets/reconstruction_example/ilastik_probabilities.h5\")[0]\n\n# now let's select all the points where the segmentation probability exceeds some threshold to get our point cloud\nthreshold = 0.95\nsegmentation_binary = segmentation&gt;threshold\nsegmentation_binary = ndimage.binary_erosion(segmentation_binary, iterations=1)\npoints = np.stack(np.where(segmentation_binary), axis=-1)\n\n\nzslice = 80\nplt.imshow(segmentation_binary[zslice,:,:])\n\n\n\n\n\n\n\n\n\n# A point cloud is simply a mesh with no faces\n\npoint_cloud = tcmesh.ObjMesh(vertices=points, faces=[])\npoint_cloud_pymeshlab = intmsl.convert_to_pymeshlab(point_cloud)\n\n\n# let's create a pymeshlab instance and add out point cloud to it\n\n# There are three relevant filters we will use:\n# generate_simplified_point_cloud - reduce number of points in point cloud\n# compute_normal_for_point_clouds - estimate normals for point cloid. This is required for the next step\n# generate_surface_reconstruction_screened_poisson - Surface reconstruction by Poisson reconstruction\n\nms = pymeshlab.MeshSet()\nms.add_mesh(point_cloud_pymeshlab)\n\nms.generate_simplified_point_cloud(samplenum=1000)\nms.compute_normal_for_point_clouds(k=20, smoothiter=2)\nms.generate_surface_reconstruction_screened_poisson(depth=8, fulldepth=5,)\n\nms.meshing_isotropic_explicit_remeshing(iterations=10, targetlen=pymeshlab.PercentageValue(1))\n\nmesh_reconstructed = intmsl.convert_from_pymeshlab(ms.current_mesh())\n\n\nmesh_reconstructed.faces.shape\n\n(22156, 3)\n\n\n\nsource\n\nreconstruct_poisson\n\n reconstruct_poisson (points, samplenum=1000, k_neighbor_normals=10,\n                      reconstruc_args=None)\n\n*Reconstruct triangular mesh from a point cloud.\nRecommended to use isotropic remeshing after to improve mesh quality (remesh_pymeshlab). Wrapper of pymeshlab filter generate_surface_reconstruction_screened_poisson See pymeshlab.readthedocs.io/en/latest/filter_list.html for its arguments.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npoints\nnp.array of shape (n_points, 3)\n\nPoint cloud.\n\n\nsamplenum\nint\n1000\nSample number for point cloud simplification. Smaller valueslead to more simplification of the point cloud and faster processing\n\n\nk_neighbor_normals\nint\n10\nNearest neighbors for constructing normal vectors to point cloud\n\n\nreconstruc_args\nNoneType\nNone\nArguments to surface reconstruction algorithm/\n\n\nReturns\ntcmehs.ObjMesh\n\nReconstructed surface.\n\n\n\n\nreconstruct_poisson(points)\n\n&lt;blender_tissue_cartography.mesh.ObjMesh&gt;",
    "crumbs": [
      "Python library",
      "Mesh creation and remeshing with MeshLab"
    ]
  },
  {
    "objectID": "Python library/remeshing_pymeshlab.html#subdivision-simplification",
    "href": "Python library/remeshing_pymeshlab.html#subdivision-simplification",
    "title": "Mesh creation and remeshing with MeshLab",
    "section": "Subdivision / simplification",
    "text": "Subdivision / simplification\n\nsource\n\nsubdivide_pymeshlab\n\n subdivide_pymeshlab (mesh, threshold=1, iterations=3, reglue=True,\n                      decimals=None)\n\n*Refine mesh by edge subdivision using pymeshlab.\nSubdivides all edges by placing new vertices at edge midpoints. Preserves UV information, by cutting the mesh along seams and (optionally) gluing back after. New texture vertices are also placed at texture-edge midpoints.\nIn contrast to remeshing_igl.subdivide_igl, you can choose to subdivide long edges only, which is very useful to increase mesh resolution only where necessary.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nthreshold\nint\n1\nAll the edges longer than this threshold will be refined. In percent?Lower values mean more subdivision. Setting this value to zero willforce an uniform refinement.\n\n\niterations\nint\n3\nFilter iterations\n\n\nreglue\nbool\nTrue\nGlue back after cutting\n\n\ndecimals\nNoneType\nNone\nDecimal precision for merging vertices when regluing. If None, estimate from the averageedge mesh length as -4*log_10(avg length)\n\n\nReturns\nObjMesh\n\nSubdivided mesh.\n\n\n\n\nmesh_test = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/initial_uv.obj\")\nmesh_subdiv = subdivide_pymeshlab(mesh_test, reglue=True, threshold=1)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o mesh_01_cylinder_seams_uv\n\n\n\nmesh_test.vertices.shape, mesh_subdiv.vertices.shape\n\n((20212, 3), (12625, 3))\n\n\n\nplt.triplot(*mesh_subdiv.texture_vertices.T, mesh_subdiv.texture_tris, lw=0.2)",
    "crumbs": [
      "Python library",
      "Mesh creation and remeshing with MeshLab"
    ]
  },
  {
    "objectID": "Python library/remeshing_pymeshlab.html#isotropic-remeshing",
    "href": "Python library/remeshing_pymeshlab.html#isotropic-remeshing",
    "title": "Mesh creation and remeshing with MeshLab",
    "section": "Isotropic remeshing",
    "text": "Isotropic remeshing\nWill dstroy UV information. Only use it before you create your UV maps.\n\nsource\n\nremesh_pymeshlab\n\n remesh_pymeshlab (mesh, targetlen=1, iterations=10)\n\n*Remesh mesh using pymeshlab.\nThis creates a triangulation where triangles are close to equilateral and everywhere the same size. Very useful to improve the output of marching cubes. Uses the meshing_isotropic_explicit_remeshing filter.\nThis function erases UV information!*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\ntargetlen\nint\n1\nPercent value for target edge length.\n\n\niterations\nint\n10\nNumber of iterations.\n\n\nReturns\nObjMesh\n\nSimplified mesh.\n\n\n\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/uv_sphere.obj\")\n\n\nmesh_remeshed = remesh_pymeshlab(mesh, iterations=10, targetlen=5)\n\n\nmesh_remeshed.only_vertices\n\nTrue\n\n\n\nplt.triplot(*mesh_remeshed.vertices[:,:2].T, mesh_remeshed.tris, lw=0.2)\n\n\n\n\n\n\n\n\n\nmesh_remeshed.write_obj(\"datasets/movie_example/isotropic_remeshed.obj\")",
    "crumbs": [
      "Python library",
      "Mesh creation and remeshing with MeshLab"
    ]
  },
  {
    "objectID": "Python library/04c_smoothing.html",
    "href": "Python library/04c_smoothing.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Smooth mesh with various filters\n\n\n\nWe implement Laplacian and Taubin smoothing for vertex positions using libigl’s Laplacian operator.\n\nsource\n\n\n\n\n get_uniform_laplacian (tris, normalize=True)\n\n*Get uniform Laplacian (purely connectivity-based) as a sparse matrix.\nIf normalize, the diagonal = -1. Else, the diagonal equals the number of neighbors.*\n\nsource\n\n\n\n\n smooth_laplacian (mesh:blender_tissue_cartography.mesh.ObjMesh, lamb=0.5,\n                   n_iter=10, method='explicit', boundary='fixed')\n\n*Smooth mesh vertex positions using Laplacian filter.\nAssumes mesh is triangular.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nlamb\nfloat\n0.5\nFilter strength. Higher = more smoothing.\n\n\nn_iter\nint\n10\nFilter iterations\n\n\nmethod\nstr\nexplicit\nCan use “explicit” (fast, simple) or “implicit” (slow, more accurate) methods.\n\n\nboundary\nstr\nfixed\nWhether to allow mesh boundaries to move\n\n\nReturns\nObjMesh\n\nSmoothed mesh.\n\n\n\n\n# Load a test mesh\nmesh_registered = tcmesh.ObjMesh.read_obj(f\"datasets/wrapping_example/Drosophila_reference_registered.obj\")\n\nWarning: readOBJ() ignored non-comment line 1:\n  o Drosophila_reference_preregistered\n\n\n\nmesh_smoothed = smooth_laplacian(mesh_registered, lamb=0.5, n_iter=10, method=\"explicit\")\nmesh_smoothed.write_obj(\"datasets/wrapping_example/Drosophila_reference_smoothed_uniform_igl.obj\")\n\n\nboundary_vertices = igl.boundary_facets(mesh_registered.texture_tris)[:, 0]\n\n\nnp.allclose(mesh_smoothed.texture_vertices[boundary_vertices],\n            mesh_registered.texture_vertices[boundary_vertices])\n\nTrue\n\n\n\nsource\n\n\n\n\n smooth_taubin (mesh:blender_tissue_cartography.mesh.ObjMesh, lamb=0.5,\n                nu=0.53, n_iter=10)\n\n*Smooth using Taubin filter (like Laplacian, but avoids shrinkage).\nAssumes mesh is triangular. See “Improved Laplacian Smoothing of Noisy Surface Meshes” J. Vollmer, R. Mencl, and H. Muller.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nlamb\nfloat\n0.5\nFilter strength. Higher = more smoothing.\n\n\nnu\nfloat\n0.53\nCounteract shrinkage. Higher = more dilation.\n\n\nn_iter\nint\n10\nFilter iterations\n\n\nReturns\nObjMesh\n\nSmoothed mesh.\n\n\n\n\nmesh_smoothed_taubin = smooth_taubin(mesh_registered, lamb=0.5, nu=0.53, n_iter=10)\nmesh_smoothed_taubin.write_obj(\"datasets/wrapping_example/Drosophila_reference_smoothed_taubin_igl.obj\")",
    "crumbs": [
      "Python library",
      "Mesh smoothing"
    ]
  },
  {
    "objectID": "Python library/04c_smoothing.html#mesh-smoothing",
    "href": "Python library/04c_smoothing.html#mesh-smoothing",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Smooth mesh with various filters\n\n\n\nWe implement Laplacian and Taubin smoothing for vertex positions using libigl’s Laplacian operator.\n\nsource\n\n\n\n\n get_uniform_laplacian (tris, normalize=True)\n\n*Get uniform Laplacian (purely connectivity-based) as a sparse matrix.\nIf normalize, the diagonal = -1. Else, the diagonal equals the number of neighbors.*\n\nsource\n\n\n\n\n smooth_laplacian (mesh:blender_tissue_cartography.mesh.ObjMesh, lamb=0.5,\n                   n_iter=10, method='explicit', boundary='fixed')\n\n*Smooth mesh vertex positions using Laplacian filter.\nAssumes mesh is triangular.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nlamb\nfloat\n0.5\nFilter strength. Higher = more smoothing.\n\n\nn_iter\nint\n10\nFilter iterations\n\n\nmethod\nstr\nexplicit\nCan use “explicit” (fast, simple) or “implicit” (slow, more accurate) methods.\n\n\nboundary\nstr\nfixed\nWhether to allow mesh boundaries to move\n\n\nReturns\nObjMesh\n\nSmoothed mesh.\n\n\n\n\n# Load a test mesh\nmesh_registered = tcmesh.ObjMesh.read_obj(f\"datasets/wrapping_example/Drosophila_reference_registered.obj\")\n\nWarning: readOBJ() ignored non-comment line 1:\n  o Drosophila_reference_preregistered\n\n\n\nmesh_smoothed = smooth_laplacian(mesh_registered, lamb=0.5, n_iter=10, method=\"explicit\")\nmesh_smoothed.write_obj(\"datasets/wrapping_example/Drosophila_reference_smoothed_uniform_igl.obj\")\n\n\nboundary_vertices = igl.boundary_facets(mesh_registered.texture_tris)[:, 0]\n\n\nnp.allclose(mesh_smoothed.texture_vertices[boundary_vertices],\n            mesh_registered.texture_vertices[boundary_vertices])\n\nTrue\n\n\n\nsource\n\n\n\n\n smooth_taubin (mesh:blender_tissue_cartography.mesh.ObjMesh, lamb=0.5,\n                nu=0.53, n_iter=10)\n\n*Smooth using Taubin filter (like Laplacian, but avoids shrinkage).\nAssumes mesh is triangular. See “Improved Laplacian Smoothing of Noisy Surface Meshes” J. Vollmer, R. Mencl, and H. Muller.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nlamb\nfloat\n0.5\nFilter strength. Higher = more smoothing.\n\n\nnu\nfloat\n0.53\nCounteract shrinkage. Higher = more dilation.\n\n\nn_iter\nint\n10\nFilter iterations\n\n\nReturns\nObjMesh\n\nSmoothed mesh.\n\n\n\n\nmesh_smoothed_taubin = smooth_taubin(mesh_registered, lamb=0.5, nu=0.53, n_iter=10)\nmesh_smoothed_taubin.write_obj(\"datasets/wrapping_example/Drosophila_reference_smoothed_taubin_igl.obj\")",
    "crumbs": [
      "Python library",
      "Mesh smoothing"
    ]
  },
  {
    "objectID": "Python library/04c_smoothing.html#texture-smoothing",
    "href": "Python library/04c_smoothing.html#texture-smoothing",
    "title": "blender-tissue-cartography",
    "section": "Texture smoothing",
    "text": "Texture smoothing\nSometimes, UV maps can become very deformed, or even display self-intersection. Smoothing can fix this.\n\nsource\n\nsmooth_laplacian_texture\n\n smooth_laplacian_texture (mesh:blender_tissue_cartography.mesh.ObjMesh,\n                           lamb=0.5, n_iter=10, boundary='fixed')\n\n*Smooth mesh texture positions using Laplacian filter.\nThis function is very helpful in fixing UV maps with flipped triangles, as detected by igl.flipped_triangles. Assumes mesh is triangular.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nlamb\nfloat\n0.5\nFilter strength. Higher = more smoothing.\n\n\nn_iter\nint\n10\nFilter iterations\n\n\nboundary\nstr\nfixed\nWhether to allow UV “island” boundary to move\n\n\nReturns\nObjMesh\n\nSmoothed mesh.\n\n\n\n\nmesh_texture_smoothed = smooth_laplacian_texture(mesh_registered, lamb=0.5, n_iter=10,)\n\n\n\nOn-surface smoothing\nSmooth, but project at each step so the mesh vertices are back on the surface. This is very useful to smooth out surface-surface maps.\n\nsource\n\n\nsmooth_laplacian_on_surface\n\n smooth_laplacian_on_surface\n                              (mesh:blender_tissue_cartography.mesh.ObjMes\n                              h, n_iter=10, lamb=0.5, n_iter_laplace=10,\n                              boundary='fixed')\n\n*Smooth mesh vertex positions using Laplacian filter and project vertices back to the original surface.\nAlternates between Laplacian smoothing and projecting back to the original surface. Uses explicit method for Laplacian smoothing*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nn_iter\nint\n10\nNumber of iterations at each step\n\n\nlamb\nfloat\n0.5\nFilter strength. Higher = more smoothing.\n\n\nn_iter_laplace\nint\n10\nLaplace filter iterations. If reprojection messes up your mesh, decrease this number.\n\n\nboundary\nstr\nfixed\nWhether to allow mesh boundaries to move\n\n\nReturns\nObjMesh\n\nSmoothed mesh.\n\n\n\n\nmesh_test = tcmesh.ObjMesh.read_obj(f\"datasets/movie_example/meshes_wrapped_reverse/mesh_01_wrapped_reverse.obj\")\nmesh_smoothed = smooth_laplacian_on_surface(mesh_test, n_iter=20, lamb=0.5, n_iter_laplace=5)\nmesh_smoothed.write_obj(f\"datasets/movie_example/on_surface_smooth.obj\")",
    "crumbs": [
      "Python library",
      "Mesh smoothing"
    ]
  },
  {
    "objectID": "Python library/01b_mesh.html",
    "href": "Python library/01b_mesh.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Data structure for triangular meshes and UV maps\n\nIn this notebook, we define a class for triangular meshes with UV maps, and show how to use it with the data from the basics_example folder.\nLet’s define the metadata for the example dataset: the filename, resolution in microns, and how much we subsampled for segmentation purposes. This is important for correctly scaling mesh vertex positions when computing the mesh from image data.\nImportant convention For sanity’s sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n\nmetadata_dict = {'filename': 'datasets/basics_example/basics_example',\n                 'resolution_in_microns': (1, 0.36, 0.36), # you can typically get this from the .tif metadata\n                 'subsampling_factors': (1, 1/3, 1/3), # how much you subsampled your image for segmentation\n                }\n\n\nsource\n\n\n\n invert_dictionary (my_map, assume_unique=False)\n\n*Invert key -&gt; value map defined by a dictionary\nIf assume_unique is True, key/value pairs are assumed to be unique. Else, a dictionary of lists is returned. Each entry is a list of keys that map to the given value.*\n\nsource\n\n\n\n\n index_else_nan (arr, inds)\n\nReturn arr[inds], masked so that the result is np.nan wherever ind is nan\n\nsource\n\n\n\n\n unique (sequence)\n\nCreate list of unique entries in sequence while preserving order\n\nsource\n\n\n\n\n pad_list (lst, length=3, fill_value=nan)\n\nPad end of list with fill_value if shorter than the desired length.\n\nsource\n\n\n\n\n flatten (lst, max_depth=1000, iter_count=0)\n\n*Flatten a list of lists into a list.\nAlso works with inhomogeneous lists, e.g., [[0,1],2]. The argument depth determines how “deep” to flatten the list, e.g. with max_depth=1: [[(1,0), (1,0)]] -&gt; [(1,0), (1,0)].*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlst\nlist\n\nlist-of-lists.\n\n\nmax_depth\nint\n1000\nTo what depth to flatten the list.\n\n\niter_count\nint\n0\nHelper argument for recursion depth determination.\n\n\nReturns\niterator\n\nflattened list.\n\n\n\n\n\n\nWe now define our main class for mesh io and mesh handling. We save all meshes as wavefront .obj files (see wikipedia). In Python, we represent missing entries (such as a vertex that doesn’t have a normal by np.nan.\nFor reading, writing, holding, and handling .obj meshes, we will create the ObjMesh class. You can think of an .obj mesh as a map between two meshes (the mesh in 3d, and its texture coordinates in 2d), defined on a per-face basis.\nHandling of non-triangular meshes\nWe prefer to work with triangulated meshes. For triangulated meshes: 1. there is a well-defined way to interpolate data across mesh faces (barycentric interpolation) 2. the deformation of a face under a mapping (e.g. cartographic map to the plane) is uniquely defined 3. many more algorithms are available (notably in igl)\nNon-triangular meshes and meshes with mixes of triangles and higher-order faces, like quads, are supported by this data structure, but not recommended.\n\nsource\n\n\n\n\n ObjMesh (vertices, faces, texture_vertices=None, normals=None, name=None)\n\n*Simple class for reading, holding, transforming, and saving 3d polygonal meshes in the .obj format. See https://en.wikipedia.org/wiki/Wavefront_.obj_file.\nAn ObjMesh comprises vertices and faces, describing a surface in 3d, (optionally) per-vertex normals, and (optionally), texture vertices and texture faces that describe how the surface is mapped to 2d.\nVertices, texture_vertices, normals are np.arrays, faces is a list. Each face is either a list of vertex indices, or, if the mesh has texture information, a list of vertex/texture vertex index pairs, describing which face maps to which texture face. Note: the number of texture vertices and vertices is not necessarily equal!\nNormals are always defined per-vertex, i.e. self.normals[i] is the normal vector at self.vertices[i]. Missing data is represented by np.nan. Faces can be any length (triangles, quads, …). Indices start at 0!\nAttributes\nvertices : np.array of shape (#vertices, dimension).\ntexture_vertices : np.array of shape (#texture vertices, 2) or None\nnormals : np.array of shape (#vertices, dimension) or None\nfaces : List[List[int]] or List[List[(int, int]]]\nProperty methods (use like attributes)\nonly_vertices : bool, whether mesh has texture information\nis_triangular : bool\ntris : np.array of shape (#triangular faces, 3). Triangular 3d faces\ntexture_tris : np.array of shape (#triangular faces, 3). Triangular texture faces. Note: undefined texture faces are represented by [0, 0, 0]*\n\nnbdev.show_doc(ObjMesh.set_normals)\n\n\nsource\n\n\n\n ObjMesh.set_normals ()\n\nRecompute normals based on 3d positions. Only works for triangular meshes.\n\n\n\n\nnbdev.show_doc(ObjMesh.get_uv_index_to_vertex_index_map)\n\n\nsource\n\n\n\n ObjMesh.get_uv_index_to_vertex_index_map ()\n\nGet map from texture vertex index to the corresponding 3d vertex index as a dictionary.\n\n\n\n\nnbdev.show_doc(ObjMesh.get_vertex_index_to_uv_index_map)\n\n\nsource\n\n\n\n ObjMesh.get_vertex_index_to_uv_index_map ()\n\n*Get map from 3d vertex index to the corresponding UV vertex index as a dictionary.\nNote: each dict value is a list, since a 3d vertex can map to multiple UV vertices. For vertices that do not have any corresponding UV vertex, the list is empty.*\n\n\n\n\nnbdev.show_doc(ObjMesh.get_vertex_to_texture_vertex_indices)\n\n\nsource\n\n\n\n ObjMesh.get_vertex_to_texture_vertex_indices ()\n\n*Get an array of indices into 3d vertices that map them to the corresponding texture vertices.\nUseful for translating per-vertex data into per-texture-vertex data: array -&gt; array[self.get_vertex_to_texture_vertex_indices()]*\n\n\n\n\nnbdev.show_doc(ObjMesh.map_per_vertex_to_per_texture_vertex)\n\n\nsource\n\n\n\n ObjMesh.map_per_vertex_to_per_texture_vertex (field)\n\nMap per-vertex field (array of shape (n_vertices, …)) to texture vertices. vertices.\n\n\n\n\nnbdev.show_doc(ObjMesh.map_per_texture_vertex_to_per_vertex)\n\n\nsource\n\n\n\n ObjMesh.map_per_texture_vertex_to_per_vertex (field)\n\nMap per-texture vertex field (array of shape (n_texture_vertices, …)) to 3d vertices. vertices.\n\n\n\n\nnbdev.show_doc(ObjMesh.cut_along_seams)\n\n\nsource\n\n\n\n ObjMesh.cut_along_seams ()\n\n*Cut mesh along texture seams.\nReturns a new ObjMesh in which the topology of the vertices matches the topology of the texture vertices, by duplicating vertices along “seams” (i.e. which have multiple corresponding texture vertices), and discarding any vertices without texture coordinates.*\n\n\n\n\nnbdev.show_doc(ObjMesh.apply_affine_to_mesh)\n\n\nsource\n\n\n\n ObjMesh.apply_affine_to_mesh (trafo)\n\n*Apply affine transformation to mesh.\nRotate/shear and translate vertices, rotate/shear and renormalize normals, flip faces if the transformation determinant is -1.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntrafo\nnp.array of shape (4,4) or (3,3)\nTransformation matrix. If (4,4), it is interpreted as an affine transformation.\n\n\nReturns\nObjMesh\nTransformed mesh.\n\n\n\n\n\n\n\nsource\n\n\n\n\n read_other_formats_without_uv (filename)\n\n*Return vertices and faces from a non-.obj mesh file format. file.\nSupported formats are .obj, .off, .stl, .wrl, .ply, .mesh. Will NOT read in normals or texture coordinates. If you have texture coordinates, save your mesh as .obj. Will only return triangular faces.\nSee https://libigl.github.io/libigl-python-bindings/igl_docs/#read_triangle_mesh.*\n\n\n\n\nType\nDetails\n\n\n\n\nfilename\nstr\nfilename\n\n\n\n\n\n\nBy design, blender_tissue_cartography does not feature functionality to create UV maps computationally - that’s what you’re supposed to do in Blender! With one exception: projection from an axis. This takes the 3d coordinates of a mesh, and projects it along a user-selected axis to 2d. This is useful when creating a projection of a mildly curved, disk-topology surface, and can directly be applied to time-lapse images.\n\nsource\n\n\n\n\n compute_project_from_axis_scale (image_shape, resolution, axis1, axis2)\n\n*Compute scaling factor for UV coordinates obtained projecting 3D coordinates along an axis.\nUV coordinates must be scaled to lie between 0 and 1. To get a consistent scaling across e.g.  different frames of a movie, this function computes the scale factor based on the projection axes and the shape of the volumetric image.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimage_shape\nlist or np.array of length 3\nShape of image in pixels\n\n\nresolution\nnp.array of shape (3,)\nResolution of image in microns/pixel along each axis\n\n\naxis1\n\n\n\n\naxis2\nnp.array of shape (3,)\nvector defining the axis projected to V coordinate\n\n\nReturns\nfloat\nscaling factor to ensure UV coordinates lie in [0, 1]\n\n\n\n\nsource\n\n\n\n\n project_from_axis (mesh, axis1, axis2, translate=None, scale=None)\n\n*Create UV map by projecting 3D coordinates along an axis.\nThe UV topology will be identical to the 3D topology (no seams), and there may be self-intersections.\nThe projected coordinates should be scaled to lie in [0,1]^2. The scale factor can be found automatically or be computed from the shape and resolution of a 3D image using*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\n\n\n\naxis1\nnp.array of shape (3,)\n\nvector defining the axis projected to U coordinate\n\n\naxis2\nnp.array of shape (3,)\n\nvector defining the axis projected to V coordinate\n\n\ntranslate\nNoneType\nNone\ntranslation to ensure UV coordinates are positive. If None, the minimum of U & V is used.\n\n\nscale\nNoneType\nNone\nscaling factor to ensure UV coordinates lie in [0, 1]. If None, the maximum of U & V is used.\n\n\nReturns\nmesh_projected: ObjMesh\n\nNew mesh, with UV map. UV and \n\n\n\n\nmesh_test = ObjMesh.read_obj(\"datasets/basics_example/basics_example_mesh_remeshed.obj\")\n\n# get the scale from the datasets/basics_example/basics_example.tif volumetric dataset\nscale = compute_project_from_axis_scale(image_shape=(26, 454, 511), resolution=(1, 0.36, 0.36),\n                                       axis1=np.array([0,1,0]), axis2=np.array([0,0,1]))\nmesh_projected = project_from_axis(mesh_test, axis1=np.array([0,1,0]), axis2=np.array([0,0,1]), scale=scale, translate=0)\n\n\nplt.scatter(*mesh_projected.texture_vertices.T, s=0.1)\nplt.axis(\"equal\")\n\n(-0.04403131115459883,\n 0.9246575342465754,\n -0.049608610567514685,\n 1.0417808219178084)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# let's consider some examples\nmesh_cube = \"datasets/basics_example/cube.obj\"\nmesh_mixed = \"datasets/basics_example/cube_mixed.obj\"\n\nmesh_large = \"datasets/registration_example/Drosophila_reference.obj\"\nmesh_no_uv = \"datasets/basics_example/cube_no_uv.obj\"\nmesh_only_normals = \"datasets/basics_example/cube_only_normals.obj\"\n\nmesh_partial_uv = \"datasets/basics_example/cube_partial_uv.obj\"\nmesh_partial_uv_large = \"datasets/basics_example/torus.obj\"\n\n\nmesh = ObjMesh.read_obj(mesh_cube)\n\n/tmp/ipykernel_1942162/3286167482.py:118: DeprecationWarning: Warning: non-triangular meshes are deprecated\n  warnings.warn(f\"Warning: non-triangular meshes are deprecated\", DeprecationWarning)\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\n\n\n\nObjMesh.read_obj(mesh_mixed)\n\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\n/tmp/ipykernel_1942162/3286167482.py:82: DeprecationWarning: Warning: non-triangular meshes are deprecated\n  warnings.warn(f\"Warning: non-triangular meshes are deprecated\", DeprecationWarning)\n\n\n&lt;__main__.ObjMesh&gt;\n\n\n\nread_other_formats_without_uv(\"datasets/wrapping_example/mixed_faces_example.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Cube\n\n\n&lt;__main__.ObjMesh&gt;\n\n\n\n# test the read_obj function on an example\nmesh_fname_data = \"datasets/registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\n\nmesh = ObjMesh.read_obj(mesh_fname_data)\n\nWarning: readOBJ() ignored non-comment line 1:\n  o Drosophila_reference_preregistered\n\n\n\nObjMesh.read_obj(mesh_mixed).faces\n\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\n/tmp/ipykernel_1942162/3286167482.py:82: DeprecationWarning: Warning: non-triangular meshes are deprecated\n  warnings.warn(f\"Warning: non-triangular meshes are deprecated\", DeprecationWarning)\n\n\n[[0, 1, 3, 2],\n [2, 3, 7, 6],\n [6, 7, 5, 4],\n [4, 5, 1, 0],\n [2, 6, 4, 0],\n [3, 5, 7],\n [3, 1, 5]]\n\n\n\n# for meshes with partial UV information, missing data is represented by np.nan\n\nObjMesh.read_obj(mesh_partial_uv).faces\n\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\nError: readOBJ() face on line 32 has invalid format\n\n\n[[[1, nan], [2, nan], [0, nan]],\n [[3, nan], [6, nan], [2, nan]],\n [[7, nan], [4, nan], [6, nan]],\n [[5, 1], [0, 2], [4, 3]],\n [[6, nan], [0, nan], [2, nan]],\n [[3, 4], [5, 1], [7, 5]],\n [[1, nan], [3, 0], [2, nan]],\n [[3, nan], [7, nan], [6, nan]],\n [[7, 12], [5, nan], [4, nan]],\n [[5, 1], [1, 6], [0, 2]],\n [[6, nan], [4, nan], [0, nan]],\n [[3, 4], [1, 6], [5, 1]]]",
    "crumbs": [
      "Python library",
      "Mesh data structure"
    ]
  },
  {
    "objectID": "Python library/01b_mesh.html#mesh-data-structure",
    "href": "Python library/01b_mesh.html#mesh-data-structure",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Data structure for triangular meshes and UV maps\n\nIn this notebook, we define a class for triangular meshes with UV maps, and show how to use it with the data from the basics_example folder.\nLet’s define the metadata for the example dataset: the filename, resolution in microns, and how much we subsampled for segmentation purposes. This is important for correctly scaling mesh vertex positions when computing the mesh from image data.\nImportant convention For sanity’s sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n\nmetadata_dict = {'filename': 'datasets/basics_example/basics_example',\n                 'resolution_in_microns': (1, 0.36, 0.36), # you can typically get this from the .tif metadata\n                 'subsampling_factors': (1, 1/3, 1/3), # how much you subsampled your image for segmentation\n                }\n\n\nsource\n\n\n\n invert_dictionary (my_map, assume_unique=False)\n\n*Invert key -&gt; value map defined by a dictionary\nIf assume_unique is True, key/value pairs are assumed to be unique. Else, a dictionary of lists is returned. Each entry is a list of keys that map to the given value.*\n\nsource\n\n\n\n\n index_else_nan (arr, inds)\n\nReturn arr[inds], masked so that the result is np.nan wherever ind is nan\n\nsource\n\n\n\n\n unique (sequence)\n\nCreate list of unique entries in sequence while preserving order\n\nsource\n\n\n\n\n pad_list (lst, length=3, fill_value=nan)\n\nPad end of list with fill_value if shorter than the desired length.\n\nsource\n\n\n\n\n flatten (lst, max_depth=1000, iter_count=0)\n\n*Flatten a list of lists into a list.\nAlso works with inhomogeneous lists, e.g., [[0,1],2]. The argument depth determines how “deep” to flatten the list, e.g. with max_depth=1: [[(1,0), (1,0)]] -&gt; [(1,0), (1,0)].*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlst\nlist\n\nlist-of-lists.\n\n\nmax_depth\nint\n1000\nTo what depth to flatten the list.\n\n\niter_count\nint\n0\nHelper argument for recursion depth determination.\n\n\nReturns\niterator\n\nflattened list.\n\n\n\n\n\n\nWe now define our main class for mesh io and mesh handling. We save all meshes as wavefront .obj files (see wikipedia). In Python, we represent missing entries (such as a vertex that doesn’t have a normal by np.nan.\nFor reading, writing, holding, and handling .obj meshes, we will create the ObjMesh class. You can think of an .obj mesh as a map between two meshes (the mesh in 3d, and its texture coordinates in 2d), defined on a per-face basis.\nHandling of non-triangular meshes\nWe prefer to work with triangulated meshes. For triangulated meshes: 1. there is a well-defined way to interpolate data across mesh faces (barycentric interpolation) 2. the deformation of a face under a mapping (e.g. cartographic map to the plane) is uniquely defined 3. many more algorithms are available (notably in igl)\nNon-triangular meshes and meshes with mixes of triangles and higher-order faces, like quads, are supported by this data structure, but not recommended.\n\nsource\n\n\n\n\n ObjMesh (vertices, faces, texture_vertices=None, normals=None, name=None)\n\n*Simple class for reading, holding, transforming, and saving 3d polygonal meshes in the .obj format. See https://en.wikipedia.org/wiki/Wavefront_.obj_file.\nAn ObjMesh comprises vertices and faces, describing a surface in 3d, (optionally) per-vertex normals, and (optionally), texture vertices and texture faces that describe how the surface is mapped to 2d.\nVertices, texture_vertices, normals are np.arrays, faces is a list. Each face is either a list of vertex indices, or, if the mesh has texture information, a list of vertex/texture vertex index pairs, describing which face maps to which texture face. Note: the number of texture vertices and vertices is not necessarily equal!\nNormals are always defined per-vertex, i.e. self.normals[i] is the normal vector at self.vertices[i]. Missing data is represented by np.nan. Faces can be any length (triangles, quads, …). Indices start at 0!\nAttributes\nvertices : np.array of shape (#vertices, dimension).\ntexture_vertices : np.array of shape (#texture vertices, 2) or None\nnormals : np.array of shape (#vertices, dimension) or None\nfaces : List[List[int]] or List[List[(int, int]]]\nProperty methods (use like attributes)\nonly_vertices : bool, whether mesh has texture information\nis_triangular : bool\ntris : np.array of shape (#triangular faces, 3). Triangular 3d faces\ntexture_tris : np.array of shape (#triangular faces, 3). Triangular texture faces. Note: undefined texture faces are represented by [0, 0, 0]*\n\nnbdev.show_doc(ObjMesh.set_normals)\n\n\nsource\n\n\n\n ObjMesh.set_normals ()\n\nRecompute normals based on 3d positions. Only works for triangular meshes.\n\n\n\n\nnbdev.show_doc(ObjMesh.get_uv_index_to_vertex_index_map)\n\n\nsource\n\n\n\n ObjMesh.get_uv_index_to_vertex_index_map ()\n\nGet map from texture vertex index to the corresponding 3d vertex index as a dictionary.\n\n\n\n\nnbdev.show_doc(ObjMesh.get_vertex_index_to_uv_index_map)\n\n\nsource\n\n\n\n ObjMesh.get_vertex_index_to_uv_index_map ()\n\n*Get map from 3d vertex index to the corresponding UV vertex index as a dictionary.\nNote: each dict value is a list, since a 3d vertex can map to multiple UV vertices. For vertices that do not have any corresponding UV vertex, the list is empty.*\n\n\n\n\nnbdev.show_doc(ObjMesh.get_vertex_to_texture_vertex_indices)\n\n\nsource\n\n\n\n ObjMesh.get_vertex_to_texture_vertex_indices ()\n\n*Get an array of indices into 3d vertices that map them to the corresponding texture vertices.\nUseful for translating per-vertex data into per-texture-vertex data: array -&gt; array[self.get_vertex_to_texture_vertex_indices()]*\n\n\n\n\nnbdev.show_doc(ObjMesh.map_per_vertex_to_per_texture_vertex)\n\n\nsource\n\n\n\n ObjMesh.map_per_vertex_to_per_texture_vertex (field)\n\nMap per-vertex field (array of shape (n_vertices, …)) to texture vertices. vertices.\n\n\n\n\nnbdev.show_doc(ObjMesh.map_per_texture_vertex_to_per_vertex)\n\n\nsource\n\n\n\n ObjMesh.map_per_texture_vertex_to_per_vertex (field)\n\nMap per-texture vertex field (array of shape (n_texture_vertices, …)) to 3d vertices. vertices.\n\n\n\n\nnbdev.show_doc(ObjMesh.cut_along_seams)\n\n\nsource\n\n\n\n ObjMesh.cut_along_seams ()\n\n*Cut mesh along texture seams.\nReturns a new ObjMesh in which the topology of the vertices matches the topology of the texture vertices, by duplicating vertices along “seams” (i.e. which have multiple corresponding texture vertices), and discarding any vertices without texture coordinates.*\n\n\n\n\nnbdev.show_doc(ObjMesh.apply_affine_to_mesh)\n\n\nsource\n\n\n\n ObjMesh.apply_affine_to_mesh (trafo)\n\n*Apply affine transformation to mesh.\nRotate/shear and translate vertices, rotate/shear and renormalize normals, flip faces if the transformation determinant is -1.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntrafo\nnp.array of shape (4,4) or (3,3)\nTransformation matrix. If (4,4), it is interpreted as an affine transformation.\n\n\nReturns\nObjMesh\nTransformed mesh.\n\n\n\n\n\n\n\nsource\n\n\n\n\n read_other_formats_without_uv (filename)\n\n*Return vertices and faces from a non-.obj mesh file format. file.\nSupported formats are .obj, .off, .stl, .wrl, .ply, .mesh. Will NOT read in normals or texture coordinates. If you have texture coordinates, save your mesh as .obj. Will only return triangular faces.\nSee https://libigl.github.io/libigl-python-bindings/igl_docs/#read_triangle_mesh.*\n\n\n\n\nType\nDetails\n\n\n\n\nfilename\nstr\nfilename\n\n\n\n\n\n\nBy design, blender_tissue_cartography does not feature functionality to create UV maps computationally - that’s what you’re supposed to do in Blender! With one exception: projection from an axis. This takes the 3d coordinates of a mesh, and projects it along a user-selected axis to 2d. This is useful when creating a projection of a mildly curved, disk-topology surface, and can directly be applied to time-lapse images.\n\nsource\n\n\n\n\n compute_project_from_axis_scale (image_shape, resolution, axis1, axis2)\n\n*Compute scaling factor for UV coordinates obtained projecting 3D coordinates along an axis.\nUV coordinates must be scaled to lie between 0 and 1. To get a consistent scaling across e.g.  different frames of a movie, this function computes the scale factor based on the projection axes and the shape of the volumetric image.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nimage_shape\nlist or np.array of length 3\nShape of image in pixels\n\n\nresolution\nnp.array of shape (3,)\nResolution of image in microns/pixel along each axis\n\n\naxis1\n\n\n\n\naxis2\nnp.array of shape (3,)\nvector defining the axis projected to V coordinate\n\n\nReturns\nfloat\nscaling factor to ensure UV coordinates lie in [0, 1]\n\n\n\n\nsource\n\n\n\n\n project_from_axis (mesh, axis1, axis2, translate=None, scale=None)\n\n*Create UV map by projecting 3D coordinates along an axis.\nThe UV topology will be identical to the 3D topology (no seams), and there may be self-intersections.\nThe projected coordinates should be scaled to lie in [0,1]^2. The scale factor can be found automatically or be computed from the shape and resolution of a 3D image using*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\n\n\n\naxis1\nnp.array of shape (3,)\n\nvector defining the axis projected to U coordinate\n\n\naxis2\nnp.array of shape (3,)\n\nvector defining the axis projected to V coordinate\n\n\ntranslate\nNoneType\nNone\ntranslation to ensure UV coordinates are positive. If None, the minimum of U & V is used.\n\n\nscale\nNoneType\nNone\nscaling factor to ensure UV coordinates lie in [0, 1]. If None, the maximum of U & V is used.\n\n\nReturns\nmesh_projected: ObjMesh\n\nNew mesh, with UV map. UV and \n\n\n\n\nmesh_test = ObjMesh.read_obj(\"datasets/basics_example/basics_example_mesh_remeshed.obj\")\n\n# get the scale from the datasets/basics_example/basics_example.tif volumetric dataset\nscale = compute_project_from_axis_scale(image_shape=(26, 454, 511), resolution=(1, 0.36, 0.36),\n                                       axis1=np.array([0,1,0]), axis2=np.array([0,0,1]))\nmesh_projected = project_from_axis(mesh_test, axis1=np.array([0,1,0]), axis2=np.array([0,0,1]), scale=scale, translate=0)\n\n\nplt.scatter(*mesh_projected.texture_vertices.T, s=0.1)\nplt.axis(\"equal\")\n\n(-0.04403131115459883,\n 0.9246575342465754,\n -0.049608610567514685,\n 1.0417808219178084)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# let's consider some examples\nmesh_cube = \"datasets/basics_example/cube.obj\"\nmesh_mixed = \"datasets/basics_example/cube_mixed.obj\"\n\nmesh_large = \"datasets/registration_example/Drosophila_reference.obj\"\nmesh_no_uv = \"datasets/basics_example/cube_no_uv.obj\"\nmesh_only_normals = \"datasets/basics_example/cube_only_normals.obj\"\n\nmesh_partial_uv = \"datasets/basics_example/cube_partial_uv.obj\"\nmesh_partial_uv_large = \"datasets/basics_example/torus.obj\"\n\n\nmesh = ObjMesh.read_obj(mesh_cube)\n\n/tmp/ipykernel_1942162/3286167482.py:118: DeprecationWarning: Warning: non-triangular meshes are deprecated\n  warnings.warn(f\"Warning: non-triangular meshes are deprecated\", DeprecationWarning)\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\n\n\n\nObjMesh.read_obj(mesh_mixed)\n\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\n/tmp/ipykernel_1942162/3286167482.py:82: DeprecationWarning: Warning: non-triangular meshes are deprecated\n  warnings.warn(f\"Warning: non-triangular meshes are deprecated\", DeprecationWarning)\n\n\n&lt;__main__.ObjMesh&gt;\n\n\n\nread_other_formats_without_uv(\"datasets/wrapping_example/mixed_faces_example.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Cube\n\n\n&lt;__main__.ObjMesh&gt;\n\n\n\n# test the read_obj function on an example\nmesh_fname_data = \"datasets/registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\n\nmesh = ObjMesh.read_obj(mesh_fname_data)\n\nWarning: readOBJ() ignored non-comment line 1:\n  o Drosophila_reference_preregistered\n\n\n\nObjMesh.read_obj(mesh_mixed).faces\n\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\n/tmp/ipykernel_1942162/3286167482.py:82: DeprecationWarning: Warning: non-triangular meshes are deprecated\n  warnings.warn(f\"Warning: non-triangular meshes are deprecated\", DeprecationWarning)\n\n\n[[0, 1, 3, 2],\n [2, 3, 7, 6],\n [6, 7, 5, 4],\n [4, 5, 1, 0],\n [2, 6, 4, 0],\n [3, 5, 7],\n [3, 1, 5]]\n\n\n\n# for meshes with partial UV information, missing data is represented by np.nan\n\nObjMesh.read_obj(mesh_partial_uv).faces\n\nWarning: readOBJ() ignored non-comment line 3:\n  o Cube\nError: readOBJ() face on line 32 has invalid format\n\n\n[[[1, nan], [2, nan], [0, nan]],\n [[3, nan], [6, nan], [2, nan]],\n [[7, nan], [4, nan], [6, nan]],\n [[5, 1], [0, 2], [4, 3]],\n [[6, nan], [0, nan], [2, nan]],\n [[3, 4], [5, 1], [7, 5]],\n [[1, nan], [3, 0], [2, nan]],\n [[3, nan], [7, nan], [6, nan]],\n [[7, 12], [5, nan], [4, nan]],\n [[5, 1], [1, 6], [0, 2]],\n [[6, nan], [4, nan], [0, nan]],\n [[3, 4], [1, 6], [5, 1]]]",
    "crumbs": [
      "Python library",
      "Mesh data structure"
    ]
  },
  {
    "objectID": "Python library/01b_mesh.html#mesh-cutting-and-gluing",
    "href": "Python library/01b_mesh.html#mesh-cutting-and-gluing",
    "title": "blender-tissue-cartography",
    "section": "Mesh cutting and gluing",
    "text": "Mesh cutting and gluing\nThe cut_along_seams method can be used to “cut” a mesh along its UV seams. This duplicates 3d vertices so that the 3d topology and the unwrapped UV topology match. For example, if your UV map cuts your sphere mesh into a north and a south hemisphere for unwrapping, cut_along_seams will cut the mesh into two halves.\nConversely, glue_seams undoes the cutting by merging close vertices.\n\nsource\n\nglue_seams\n\n glue_seams (mesh, decimals=None)\n\n*Merge close vertices.\nUseful to undo cutting of meshes along UV seams. Note: the exact order of vertices will not in general be recovered by gluing after cutting.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\n\n\n\ndecimals\nNoneType\nNone\nVertices whose positions agree up to ‘decimals’ decimals are merged. Note: you can use negative values.If None, estimate a value based on the shortest edge length in the mesh (-2*log_10(minimum length))\n\n\nReturns\nObjMesh\n\nMesh with merged vertices.\n\n\n\n\nmesh = ObjMesh.read_obj(\"datasets/movie_example/initial_uv.obj\")\nmesh_cut = mesh.cut_along_seams()\nglued_mesh = glue_seams(mesh_cut)\nglued_mesh.write_obj(\"datasets/movie_example/initial_uv_glued.obj\")\n\nmesh.vertices.shape, mesh_cut.vertices.shape, glued_mesh.vertices.shape\n\nWarning: readOBJ() ignored non-comment line 4:\n  o mesh_01_cylinder_seams_uv\n\n\n((20212, 3), (20623, 3), (12036, 3))",
    "crumbs": [
      "Python library",
      "Mesh data structure"
    ]
  },
  {
    "objectID": "Python library/01c_interface_pymeshlab.html",
    "href": "Python library/01c_interface_pymeshlab.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Convert ObjMesh to pymeshlab’s mesh class to apply meshlab filters.\n\nIn this notebook, we define functions to convert our ObjMesh class to and from pymeshlab.\n\nsource\n\n\n\n convert_to_pymeshlab (mesh:blender_tissue_cartography.mesh.ObjMesh,\n                       add_texture_info=None)\n\n*Convert tcmesh.ObjMesh to pymeshlab.Mesh.\nSee https://pymeshlab.readthedocs.io/en/latest/classes/mesh.html Note: normal information is recalculated by pymeshlab. Discards any non-triangle faces.\nThe texture information is saved as a vertex attribute via v_tex_coords_matrix. Note that this discards information since a vertex can have multiple texture coordinates. For this reason, we also save it as wedge_tex_coord_matrix (i.e. per triangle).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\n\n\n\nadd_texture_info\nNoneType\nNone\nWhether to add texture info to the pymeshlab.Mesh. If None, texture is added ifavailable for at least one vertex.\n\n\nReturns\nMesh\n\n\n\n\n\n\nmesh_fname_data = \"datasets/registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\nmesh_fname_ref = \"datasets/registration_example/Drosophila_reference.obj\"\n\nmesh_data = tcmesh.ObjMesh.read_obj(mesh_fname_data)\nmesh_ref = tcmesh.ObjMesh.read_obj(mesh_fname_ref)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\npymesh_data = convert_to_pymeshlab(mesh_data)\n\n\npymesh_normals = pymesh_data.vertex_normal_matrix()\npymesh_normals = (pymesh_normals.T / np.linalg.norm(pymesh_normals, axis=-1)).T\n\nmesh_normals = (mesh_data.normals.T / np.linalg.norm(mesh_data.normals, axis=-1)).T\n\nnp.einsum('vi,vi-&gt;v', mesh_normals, pymesh_normals)\n\narray([0.99999487, 0.99997637, 0.99996542, ..., 0.99966711, 0.99984559,\n       0.99975186])\n\n\n\nnp.allclose(mesh_data.vertices, pymesh_data.vertex_matrix())\n\nTrue\n\n\n\n### check correctness of wedge coords\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/mesh_subdiv.obj\")\nconverted = convert_to_pymeshlab(mesh)\n\n\nms = pymeshlab.MeshSet()\n#ms.add_mesh(converted)\nms.load_new_mesh(\"datasets/movie_example/mesh_subdiv.obj\")\n\n\nnp.allclose(np.nan_to_num(ms.current_mesh().wedge_tex_coord_matrix()),\n            np.nan_to_num(converted.wedge_tex_coord_matrix()))\n\nTrue\n\n\n\nsource\n\n\n\n\n convert_from_pymeshlab (mesh:pymeshlab.pmeshlab.Mesh,\n                         reconstruct_texture_from_faces=True,\n                         texture_vertex_decimals=10)\n\n*Convert pymeshlab mesh to ObjMesh.\nTexture vertices can be reconstructed from wedge_tex_coord_matrix (per face) or from the vertex attribute vertex_tex_coord_matrix. Reconstruction from face texture can accommodate multiple texture coordinates per vertex (e.g. for UV maps with seams).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nMesh\n\n\n\n\nreconstruct_texture_from_faces\nbool\nTrue\nWhether to reconstruct texture information from per-face data (True), orper-vertex data (False)\n\n\ntexture_vertex_decimals\nint\n10\nTexture vertices are rounded to texture_vertex_decimals decimals.\n\n\nReturns\nObjMesh\n\n\n\n\n\n\npymesh_ref = convert_to_pymeshlab(mesh_ref)\npymesh_ref_converted = convert_from_pymeshlab(pymesh_ref)\n\n\nnp.allclose(mesh_ref.texture_vertices[mesh_ref.texture_tris],\n pymesh_ref_converted.texture_vertices[pymesh_ref_converted.texture_tris])\n\nTrue\n\n\n\nmesh_seams = tcmesh.ObjMesh.read_obj(\"datasets/drosophila_example/Drosophila_CAAX-mCherry_mesh_uv.obj\")\npymesh_seams = convert_to_pymeshlab(mesh_seams,add_texture_info=True)\nmesh_seams_reconverted = convert_from_pymeshlab(pymesh_seams, reconstruct_texture_from_faces=True)\nmesh_seams_reconverted.write_obj(\"datasets/drosophila_example/Drosophila_CAAX-mCherry_mesh_uv_resaved.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_CAAX-mCherry_mesh_remeshed\nWarning: readOBJ() ignored non-comment line 48073:\n  l 2534 8160\n/tmp/ipykernel_1942205/15752383.py:16: RuntimeWarning: invalid value encountered in divide\n  normals = (normals.T / np.linalg.norm(normals, axis=-1)).T\n\n\n\nmesh_seams.texture_vertices.shape, mesh_seams_reconverted.texture_vertices.shape\n\n((8288, 2), (8288, 2))\n\n\nYou can now use MeshLab filters like in the following, and the face attributes will hopefully be correctly updated.\n\nms = pymeshlab.MeshSet()\nms.add_mesh(pymesh_seams)\n\nms.meshing_merge_close_vertices(threshold=pymeshlab.PercentageValue(100))\npymesh_seams_remeshed = ms.current_mesh()\n\nconvert_from_pymeshlab(pymesh_seams_remeshed, reconstruct_texture_from_faces=True,\n                       texture_vertex_decimals=12)\n\n&lt;blender_tissue_cartography.mesh.ObjMesh&gt;",
    "crumbs": [
      "Python library",
      "`pymeshlab` interface"
    ]
  },
  {
    "objectID": "Python library/01c_interface_pymeshlab.html#pymeshlab-interface",
    "href": "Python library/01c_interface_pymeshlab.html#pymeshlab-interface",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Convert ObjMesh to pymeshlab’s mesh class to apply meshlab filters.\n\nIn this notebook, we define functions to convert our ObjMesh class to and from pymeshlab.\n\nsource\n\n\n\n convert_to_pymeshlab (mesh:blender_tissue_cartography.mesh.ObjMesh,\n                       add_texture_info=None)\n\n*Convert tcmesh.ObjMesh to pymeshlab.Mesh.\nSee https://pymeshlab.readthedocs.io/en/latest/classes/mesh.html Note: normal information is recalculated by pymeshlab. Discards any non-triangle faces.\nThe texture information is saved as a vertex attribute via v_tex_coords_matrix. Note that this discards information since a vertex can have multiple texture coordinates. For this reason, we also save it as wedge_tex_coord_matrix (i.e. per triangle).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\n\n\n\nadd_texture_info\nNoneType\nNone\nWhether to add texture info to the pymeshlab.Mesh. If None, texture is added ifavailable for at least one vertex.\n\n\nReturns\nMesh\n\n\n\n\n\n\nmesh_fname_data = \"datasets/registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\nmesh_fname_ref = \"datasets/registration_example/Drosophila_reference.obj\"\n\nmesh_data = tcmesh.ObjMesh.read_obj(mesh_fname_data)\nmesh_ref = tcmesh.ObjMesh.read_obj(mesh_fname_ref)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\npymesh_data = convert_to_pymeshlab(mesh_data)\n\n\npymesh_normals = pymesh_data.vertex_normal_matrix()\npymesh_normals = (pymesh_normals.T / np.linalg.norm(pymesh_normals, axis=-1)).T\n\nmesh_normals = (mesh_data.normals.T / np.linalg.norm(mesh_data.normals, axis=-1)).T\n\nnp.einsum('vi,vi-&gt;v', mesh_normals, pymesh_normals)\n\narray([0.99999487, 0.99997637, 0.99996542, ..., 0.99966711, 0.99984559,\n       0.99975186])\n\n\n\nnp.allclose(mesh_data.vertices, pymesh_data.vertex_matrix())\n\nTrue\n\n\n\n### check correctness of wedge coords\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/mesh_subdiv.obj\")\nconverted = convert_to_pymeshlab(mesh)\n\n\nms = pymeshlab.MeshSet()\n#ms.add_mesh(converted)\nms.load_new_mesh(\"datasets/movie_example/mesh_subdiv.obj\")\n\n\nnp.allclose(np.nan_to_num(ms.current_mesh().wedge_tex_coord_matrix()),\n            np.nan_to_num(converted.wedge_tex_coord_matrix()))\n\nTrue\n\n\n\nsource\n\n\n\n\n convert_from_pymeshlab (mesh:pymeshlab.pmeshlab.Mesh,\n                         reconstruct_texture_from_faces=True,\n                         texture_vertex_decimals=10)\n\n*Convert pymeshlab mesh to ObjMesh.\nTexture vertices can be reconstructed from wedge_tex_coord_matrix (per face) or from the vertex attribute vertex_tex_coord_matrix. Reconstruction from face texture can accommodate multiple texture coordinates per vertex (e.g. for UV maps with seams).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nMesh\n\n\n\n\nreconstruct_texture_from_faces\nbool\nTrue\nWhether to reconstruct texture information from per-face data (True), orper-vertex data (False)\n\n\ntexture_vertex_decimals\nint\n10\nTexture vertices are rounded to texture_vertex_decimals decimals.\n\n\nReturns\nObjMesh\n\n\n\n\n\n\npymesh_ref = convert_to_pymeshlab(mesh_ref)\npymesh_ref_converted = convert_from_pymeshlab(pymesh_ref)\n\n\nnp.allclose(mesh_ref.texture_vertices[mesh_ref.texture_tris],\n pymesh_ref_converted.texture_vertices[pymesh_ref_converted.texture_tris])\n\nTrue\n\n\n\nmesh_seams = tcmesh.ObjMesh.read_obj(\"datasets/drosophila_example/Drosophila_CAAX-mCherry_mesh_uv.obj\")\npymesh_seams = convert_to_pymeshlab(mesh_seams,add_texture_info=True)\nmesh_seams_reconverted = convert_from_pymeshlab(pymesh_seams, reconstruct_texture_from_faces=True)\nmesh_seams_reconverted.write_obj(\"datasets/drosophila_example/Drosophila_CAAX-mCherry_mesh_uv_resaved.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_CAAX-mCherry_mesh_remeshed\nWarning: readOBJ() ignored non-comment line 48073:\n  l 2534 8160\n/tmp/ipykernel_1942205/15752383.py:16: RuntimeWarning: invalid value encountered in divide\n  normals = (normals.T / np.linalg.norm(normals, axis=-1)).T\n\n\n\nmesh_seams.texture_vertices.shape, mesh_seams_reconverted.texture_vertices.shape\n\n((8288, 2), (8288, 2))\n\n\nYou can now use MeshLab filters like in the following, and the face attributes will hopefully be correctly updated.\n\nms = pymeshlab.MeshSet()\nms.add_mesh(pymesh_seams)\n\nms.meshing_merge_close_vertices(threshold=pymeshlab.PercentageValue(100))\npymesh_seams_remeshed = ms.current_mesh()\n\nconvert_from_pymeshlab(pymesh_seams_remeshed, reconstruct_texture_from_faces=True,\n                       texture_vertex_decimals=12)\n\n&lt;blender_tissue_cartography.mesh.ObjMesh&gt;",
    "crumbs": [
      "Python library",
      "`pymeshlab` interface"
    ]
  },
  {
    "objectID": "Python library/registration_rotation.html",
    "href": "Python library/registration_rotation.html",
    "title": "3D-rotation registration",
    "section": "",
    "text": "Here, we build tools to rotationally align data defined on a 2d-sphere using spherical harmonics. We consider the following problem: given two scalar functions \\(f,h: S^2 \\rightarrow \\mathbb{R}\\) on the sphere, find the rotation \\(g\\in O(3)\\) that best aligns them, in the sense that it maximizes \\(\\int d\\Omega f(x) h(g^{-1}x)\\). We call \\(f\\) the source and \\(h\\) the target signal.\nWe expand the functions in terms of spherical harmonics, use the Wigner D-matrices to compute the effect of rotation, and find the best possible rotation via optimization.\nIn principle, the above could be done more efficiently using the fast spherical harmonics transform and the fast inverse Wigner transform. To minimize dependencies and keep the code simple, we won’t do that here, .",
    "crumbs": [
      "Python library",
      "3D-rotation registration"
    ]
  },
  {
    "objectID": "Python library/registration_rotation.html#spherical-harmonics",
    "href": "Python library/registration_rotation.html#spherical-harmonics",
    "title": "3D-rotation registration",
    "section": "Spherical harmonics",
    "text": "Spherical harmonics\nWe will represent spherical harmonics coefficients as dicts, indexed by total angular momentum \\(l\\), with an entry being a vector for the different \\(m=-l,..., l\\). Since we have real signals, we can get negative \\(m\\) from the positive ones.\n\nsource\n\nspherical_to_cartesian\n\n spherical_to_cartesian (r, theta, phi)\n\n*Convert spherical coordinates to cartesian coordinates.\nUses en.wikipedia.org/wiki/Spherical_coordinate_system convention.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nr\n\n\n\n\ntheta\n\n\n\n\nphi\n\n\n\n\nReturns\nnp.array of shape (…, 3)\narray of x/y/z coordinates. Last axis indexes coordinate axes.\n\n\n\n\nsource\n\n\ncartesian_to_spherical\n\n cartesian_to_spherical (arr)\n\n*Convert cartesian coordinates to spherical coordinates.\nUses en.wikipedia.org/wiki/Spherical_coordinate_system convention.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\narr\nnp.array of shape (…, 3)\narray of x/y/z coordinates. last axis indexes coordinate axes.\n\n\nReturns\nnp.array, np.array, np.array\nr, theta, phi spherical coordinates\n\n\n\n\n# let's get a test case - a mesh obtained by mapping a mesh conformally to the sphere\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/map_to_sphere.obj\")\nmesh_3d = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/final_uv.obj\")\n\n\n# as test signal, let's use the log(areas) of the mesh triangles\n\nareas_sphere = igl.doublearea(mesh.vertices, mesh.tris)\nareas_3d = igl.doublearea(mesh_3d.vertices, mesh_3d.tris)\n\nsignal = np.log(areas_sphere/areas_3d)\nsignal -= signal.mean()\n\ncentroids = mesh.vertices[mesh.tris].mean(axis=1)\nr, theta, phi = cartesian_to_spherical(centroids)\n\n\nplt.scatter(phi, theta, c=signal, s=1, vmin=-3, vmax=3)\nplt.axis(\"equal\");\n\n\n\n\n\n\n\n\n\nnp.allclose(spherical_to_cartesian(r, theta, phi), centroids)\n\nTrue\n\n\n\n## Interpolate onto a grid as a check\n\nn_grid = 256\nphi_grid, theta_grid = np.meshgrid(np.linspace(-np.pi,np.pi, 2*n_grid), np.linspace(0, np.pi, n_grid)[::-1], )\ninterpolated = interpolate.griddata(np.stack([phi, theta], axis=-1), signal,\n                                    (phi_grid, theta_grid), method='linear')\ndTheta = np.pi/(n_grid-1)\ndPhi = 2*np.pi/(2*n_grid-1)\ninterpolated = np.nan_to_num(interpolated)\n\n\nplt.imshow(interpolated, vmin=-2.75, vmax=2.75)\n\n\n\n\n\n\n\n\n\n# Check normalization and orthogonality of spherical harmonics for grid\n\nprint(np.sum(np.abs(special.sph_harm(1, 2, phi_grid, theta_grid))**2 * np.sin(theta_grid) * dTheta * dPhi))\nprint(np.abs(np.sum(special.sph_harm(2, 3, phi_grid, theta_grid)\n              *np.conjugate(special.sph_harm(2, 4, phi_grid, theta_grid))\n              *np.sin(theta_grid)*dTheta*dPhi)))\n\n1.0019569486052557\n1.0290434898119816e-16\n\n\n\n# Check the relation between negative and positive m. Keep in mind (-1)^m factor\n\nl, m = (4, 3)\nnp.allclose((-1)**(-m) * np.conjugate(special.sph_harm(m, l, phi_grid,theta_grid)),\n            special.sph_harm(-m, l, phi_grid,theta_grid))\n\nTrue\n\n\n\n# Now let's do the direct calculation without extra interpolation. Check orthonormality\n\nweights = igl.doublearea(mesh.vertices, mesh.tris)/2\n\nprint(np.sum(np.abs(special.sph_harm(1, 2, phi, theta))**2 * weights))\nprint(np.abs(np.sum(special.sph_harm(2, 3, phi, theta)\n              *np.conjugate(special.sph_harm(2, 4, phi, theta))\n              *weights)))\n\n0.999525046106509\n0.00010996886047295266\n\n\n\nsource\n\n\ncompute_spherical_harmonics_coeffs\n\n compute_spherical_harmonics_coeffs (f, phi, theta, weights, max_l)\n\n*Compute spherical harmonic coefficients for a scalar real-valued function defined on the unit sphere.\nTakes as input values of the function at sample points (and sample weights), and computes the overlap with each spherical harmonic by “naive” numerical integration.\nSince the function is assumed real, we have f^{l}{-m} = np.conjugate(f^{l}{m}).*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nf\nnp.array\nSample values\n\n\nphi\nnp.array of shape\nSample point azimuthal coordinates\n\n\ntheta\nnp.array of shape\nSample point longditudinal coordinates\n\n\nweights\nnp.array\nSample weights. For instance, if you have a function sampled on a regular phi-theta grid,this should be dThetadPhinp.sin(theta)\n\n\nmax_l\nint\nMaximum angular momentum\n\n\nReturns\ndict of np.array\n**Dictionary, indexed by total angular momentum l=0 ,…, max_l-1. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l**\n\n\n\n\nweights_grid = np.sin(theta_grid)*dTheta*dPhi\nspherical_harmonics_coeffs = compute_spherical_harmonics_coeffs(interpolated, phi_grid, theta_grid,\n                                                                weights_grid, max_l=15)\n\nCPU times: user 1.81 s, sys: 3.1 ms, total: 1.82 s\nWall time: 1.81 s\n\n\n\nspherical_harmonics_coeffs[3]\n\narray([-0.24842273+0.41967878j,  0.32333336-0.37140101j,\n        0.24521225-0.18171839j, -0.38792664-0.j        ,\n       -0.24521225-0.18171839j,  0.32333336+0.37140101j,\n        0.24842273+0.41967878j])\n\n\n\nsource\n\n\nspherical_harmonics_to_grid\n\n spherical_harmonics_to_grid (coeffs, n_grid=256)\n\n*Compute signal on rectangular phi-theta grid given spherical harmonics coefficients.\nAssumes underlying function is real-valued*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncoeffs\ndict of np.array\n\nDictionary, indexed by total angular momentum l=0 ,…, max_l. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l\n\n\nn_grid\nint\n256\n\n\n\nReturns\n2d np.array\n\nReconstructed signal interpolated on rectangular phi-theta grid\n\n\n\n\nreconstructed = spherical_harmonics_to_grid(spherical_harmonics_coeffs, n_grid=256)\n\n\nnp.abs(reconstructed.imag).mean(), np.abs(reconstructed.real).mean() # as\n\n(2.2578012798576635e-17, 1.7509102818359779)\n\n\n\nplt.imshow(reconstructed.real, vmin=-2.75, vmax=2.75)\n\n\n\n\n\n\n\n\n\n## now without the extra interpolation step\n\nmax_l = 15\n\nweights = igl.doublearea(mesh.vertices, mesh.tris)/2\nspherical_harmonics_coeffs_direct = compute_spherical_harmonics_coeffs(signal, phi, theta, weights,\n                                                                       max_l=max_l)\n\n\nreconstructed_direct = spherical_harmonics_to_grid(spherical_harmonics_coeffs_direct, n_grid=256)\n\n\nplt.imshow(reconstructed_direct.real, vmin=-2.75, vmax=2.75)",
    "crumbs": [
      "Python library",
      "3D-rotation registration"
    ]
  },
  {
    "objectID": "Python library/registration_rotation.html#rotational-alignment",
    "href": "Python library/registration_rotation.html#rotational-alignment",
    "title": "3D-rotation registration",
    "section": "Rotational alignment",
    "text": "Rotational alignment\nAs a test, let’s randomly rotate our signal.\n\nrot_mat = stats.special_ortho_group.rvs(3)\nrot_mat\n\narray([[ 0.01887775,  0.81035922, -0.5856292 ],\n       [ 0.89941855,  0.24206006,  0.36394121],\n       [ 0.43668055, -0.53359616, -0.72428256]])\n\n\n\nmesh_rotated = deepcopy(mesh)\nmesh_rotated.vertices = mesh_rotated.vertices @ rot_mat.T\n\nareas_sphere_rotated  = igl.doublearea(mesh_rotated.vertices , mesh.tris)\n\nsignal_rotated = np.log(areas_sphere_rotated/areas_3d)\nsignal_rotated -= signal_rotated.mean()\n\ncentroids_rotated = mesh_rotated.vertices[mesh_rotated.tris].mean(axis=1)\n_, theta_rotated, phi_rotated = cartesian_to_spherical(centroids_rotated)\n\n\nplt.scatter(phi_rotated, theta_rotated, c=signal_rotated , s=1, vmin=-3, vmax=3)\nplt.axis(\"equal\");\n\n\n\n\n\n\n\n\n\nweights_rotated = igl.doublearea(mesh_rotated.vertices, mesh.tris)/2\nspherical_harmonics_coeffs_rotated = compute_spherical_harmonics_coeffs(signal_rotated,\n                                                                        phi_rotated, theta_rotated,\n                                                                        weights_rotated, max_l=max_l)\n\n\n# let's check that the power per band is conserved\npower_direct = {key: np.sum(np.abs(val)**2) for key, val in spherical_harmonics_coeffs_direct.items()}\npower_rotated = {key: np.sum(np.abs(val)**2) for key, val in spherical_harmonics_coeffs_rotated.items()}\n\n\npower_direct\n\n{0: 31.75093316228961,\n 1: 1.0569845383858716,\n 2: 7.656637801049782,\n 3: 1.2934800832192117,\n 4: 1.1934413112502484,\n 5: 0.16200066481681066,\n 6: 0.050219817507973934,\n 7: 0.03481762596812547,\n 8: 0.01422448292437451,\n 9: 0.04835575407817688,\n 10: 0.01666320569699937,\n 11: 0.04036958829270747,\n 12: 0.0241966285740686,\n 13: 0.024417478492580472,\n 14: 0.015292984314499655}\n\n\n\npower_rotated\n\n{0: 31.75093316228961,\n 1: 1.0569845383858694,\n 2: 7.656637801049786,\n 3: 1.29348008321921,\n 4: 1.1934413112502456,\n 5: 0.16200066481681097,\n 6: 0.05021981750797396,\n 7: 0.034817625968125335,\n 8: 0.014224482924374391,\n 9: 0.048355754078175966,\n 10: 0.016663205696999295,\n 11: 0.04036958829270838,\n 12: 0.02419662857406937,\n 13: 0.024417478492580014,\n 14: 0.015292984314499138}\n\n\n\nInference of the rotation matrix from the spherical harmonics\nNow for the difficult part. We need to infer the rotation matrix. The spherical harmonics will transform as blocks for each \\(l\\). Wigner D-matrices describe how. The code below is based on https://github.com/moble/spherical.\n\n# Load the spherical module for comparison, even though we will re-implement it to minimize dependencies\n\nimport quaternionic\nimport spherical\n\n\n# Indeed, using the correct Wigner matrix can undo our rotation\n\nwigner = spherical.Wigner(15)\nD = wigner.D(quaternionic.array.from_rotation_matrix(rot_mat))\n\nl = 5\nmp = 3\n\nnp.sum([D[wigner.Dindex(l, mp, m)]*spherical_harmonics_coeffs_direct[l][l+m]\n        for m in range(-l, l+1)]), spherical_harmonics_coeffs_rotated[l][l+mp]\n\n((0.0014361876877559377+0.11925102266293089j),\n (0.0014361876877558369+0.11925102266292989j))\n\n\n\nQuaternions and rotation\nRotations can be respresensted by unit quaternion \\(\\mathbf{q}=(q_1, q_i, q_j, q_k)\\). The spatial part \\((q_i,q_j,q_k)\\) defines the orientation of the rotation axis, and \\(\\alpha=2\\arccos(u_1)\\) is the rotation angle. For a unit vector \\(\\mathbf{u}\\) defining the orientation of rotation and rotation angle \\(\\alpha\\): \\[q = \\sin(\\alpha/2) \\mathbf{u} + \\cos(\\alpha/2)\\] See https://fr.wikipedia.org/wiki/Quaternions_et_rotation_dans_l%27espace\n\nsource\n\n\n\nquaternion_power\n\n quaternion_power (q, n)\n\nRaise quaternion to an integer power, potentially negative\n\nsource\n\n\nmultiply_quaternions\n\n multiply_quaternions (q, p)\n\n\nsource\n\n\ninvert_quaternion\n\n invert_quaternion (q)\n\nInvert a quaternion\n\nsource\n\n\nconjugate_quaternion\n\n conjugate_quaternion (q)\n\nConjugate a quaternion\n\nsource\n\n\nquaternion_to_complex_pair\n\n quaternion_to_complex_pair (q)\n\nConvert quaternion to pair of complex numbers q0+iq3, q2+iq1\n\nsource\n\n\nrot_mat_to_quaternion\n\n rot_mat_to_quaternion (Q)\n\n*Convert 3d rotation matrix into unit quaternion.\nIf determinant(Q) = -1, returns the unit quaternion corresponding to -Q\nSee https://fr.wikipedia.org/wiki/Quaternions_et_rotation_dans_l%27espace*\n\nsource\n\n\nquaternion_to_rot_max\n\n quaternion_to_rot_max (q)\n\n*Convert unit quaternion into a 3d rotation matrix.\nSee https://fr.wikipedia.org/wiki/Quaternions_et_rotation_dans_l%27espace*\n\nQ = stats.special_ortho_group.rvs(3)\nprint(Q)\n\n[[-0.1132397  -0.71935166 -0.6853539 ]\n [-0.98651227  0.16346272 -0.00857196]\n [ 0.11819607  0.67513934 -0.7281597 ]]\n\n\n\nnp.array(quaternionic.array.from_rotation_matrix(Q))/rot_mat_to_quaternion(Q)\n\narray([-1., -1., -1., -1.])\n\n\n\nq = rot_mat_to_quaternion(Q)\nQ_back = quaternion_to_rot_max(q)\nnp.allclose(Q_back, Q)\n\nTrue\n\n\n\nQ1 = stats.special_ortho_group.rvs(3)\nQ2 = stats.special_ortho_group.rvs(3)\n\nR1 = quaternionic.array.from_rotation_matrix(Q1)\nR2 = quaternionic.array.from_rotation_matrix(Q2)\n\n\nR1 * R2\n\nquaternionic.array([-0.55563224, -0.13607597,  0.55312392, -0.60564847])\n\n\n\nmultiply_quaternions(rot_mat_to_quaternion(Q1), rot_mat_to_quaternion(Q2))\n\narray([-0.55563224, -0.13607597,  0.55312392, -0.60564847])\n\n\n\nR1**4\n\nquaternionic.array([ 0.97660873, -0.05627274, -0.19008602,  0.08328306])\n\n\n\nq = rot_mat_to_quaternion(Q1)\nquaternion_power(q, 4)\n\narray([ 0.97660873, -0.05627274, -0.19008602,  0.08328306])\n\n\n\n\nWigner D-matrix as a function of quaternions\nWe use this formula: https://spherical.readthedocs.io/en/main/WignerDMatrices/:\n\n\n\nimage.png\n\n\nTo execute it rapidly, it is best to pre-compute the combinatorial coefficients via a recursion.\n\nsource\n\n\nget_binomial_matrix\n\n get_binomial_matrix (N_max)\n\n*Get N_max by N_max matrix with entries binomial_matrix[i, j] = (i choose j).\nComputed via Pascal’s triangle.*\n\nget_binomial_matrix(20)\n\n46.9 μs ± 604 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\nget_binomial_matrix(20)[12, 3], special.comb(12, 3)\n\n(220.0, 220.0)\n\n\n\nsource\n\n\nget_wigner_D_matrix\n\n get_wigner_D_matrix (q, l, binomial_matrix=None)\n\nGet (2l+1, 2l+1) Wigner D matrix for angular momentum l and rotation (unit quatertion) q\n\nmat = get_wigner_D_matrix(q, l=10) \n# 30 ms. all. down to 2 by binomial recursion\n\n3.73 ms ± 32.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nget_wigner_D_matrix(q, l=10)[12, 2]\n\n(0.3103085067597108-0.1994583649968599j)\n\n\n\nQ = stats.special_ortho_group.rvs(3)\nq = rot_mat_to_quaternion(Q)\n\n# q = np.array([ 0.99950656,  0.        ,  0.        , -0.03141076]) # special case: important to check\n\n\n# compare with the spherical package. looks pretty good, small diff if q[0]==0\n\nwigner = spherical.Wigner(15)\n#D = wigner.D(quaternionic.array.from_rotation_matrix(Q))\nD = wigner.D(q)\n\nl = 2\nbinomial_matrix = get_binomial_matrix(2*l)\nmp, m = (0, 0)\n\n(D[wigner.Dindex(l, mp, m)],\n _get_wigner_D_element(*quaternion_to_complex_pair(q), l, mp, m, binomial_matrix),)\n\n((-0.2254678510932408+0j), (-0.22546785109324072+0j))\n\n\n\nmat = get_wigner_D_matrix(q, l=10) \n# a little slow\n\nCPU times: user 3.44 ms, sys: 85 μs, total: 3.53 ms\nWall time: 3.52 ms\n\n\n\nsource\n\n\noverlap_spherical_harmonics\n\n overlap_spherical_harmonics (coeffsA, coeffsB, normalized=False)\n\n*Compute overlap (L2 inner product) between two sets of spherical harmonics.\nOptionally, normalize by the norm of coeffsA, coeffsB.*\n\nsource\n\n\nrotate_spherical_harmonics_coeffs\n\n rotate_spherical_harmonics_coeffs (R, coeffs)\n\n*Rotate spherical harmonics by the given (improper or proper) rotation matrix.\nUses Wigner-D matrices. Don’t use this function in an optimization context - you can in general save a bunch of time by reusing D-matrices etc.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nR\nnp.array of shape (3, 3)\nRotation matrix, may be improper. If you have a quaternion, use quaternion_to_rot_mat.\n\n\ncoeffs\ndict of np.array\nDictionary, indexed by total angular momentum l=0 ,…, max_l-1. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l\n\n\nReturns\ndict of np.array\nRotated spherical harmonics coefficients.\n\n\n\n\nsource\n\n\nparity_spherical_harmonics_coeffs\n\n parity_spherical_harmonics_coeffs (coeffs)\n\n*Apply parity operator to spherical harmonics coefficients.\nParity means (x,y,z) -&gt; (-x,-y,-z) and f^l_m -&gt; (-1)^m * f^l_m.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncoeffs\ndict of np.array\nDictionary, indexed by total angular momentum l=0 ,…, max_l-1. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l\n\n\nReturns\ndict of np.array\nParity-transformed spherical harmonics coefficients.\n\n\n\n\n# Check that the Wigner D correctly re-aligns our harmonic coefficients\n\nl = 7\nD_matrix = get_wigner_D_matrix(rot_mat_to_quaternion(rot_mat), l=l)\n\nnp.allclose(D_matrix@spherical_harmonics_coeffs_direct[l], spherical_harmonics_coeffs_rotated[l])\n# success!\n\nTrue\n\n\n\n# before alignment, the coefficients are different\nnot np.allclose(spherical_harmonics_coeffs_direct[l], spherical_harmonics_coeffs_rotated[l])\n\nTrue\n\n\n\n\nCross-correlation analysis\nWe now want to find the rotation that maximizes the cross-correlation between the two signals. To do so, we compute the overlap for several trial rotations to get an initial guess and refine it by optimization.\n\n# This is how you compute the overlap, here using my code\n\nmax_l = 10\nbinomial_matrix = get_binomial_matrix(2*max_l)\nR = np.copy(rot_mat)\n\ncorr_coeff = 0\nfor l in range(0, max_l):\n    D_matrix = get_wigner_D_matrix(rot_mat_to_quaternion(R), l=l, binomial_matrix=binomial_matrix)\n    corr_coeff += np.sum(D_matrix@spherical_harmonics_coeffs_direct[l]\n                         *np.conjugate(spherical_harmonics_coeffs_rotated[l]))\n\nCPU times: user 10.6 ms, sys: 18 μs, total: 10.6 ms\nWall time: 10.4 ms\n\n\n\ncorr_coeff\n\n(43.26109524149018+2.5746956649943757e-16j)\n\n\nNow let’s consider multiple rotations. we can save time by minimizing the number of times we compute the D-matrix by using the fact that \\(D(R^n) = D(R)^n\\) - hence, for every rotation axis, we can try out a large number of rotation angles efficiently.\n\nD_mat = get_wigner_D_matrix(rot_mat_to_quaternion(R), l=2)\nD_mat_of_square = get_wigner_D_matrix(rot_mat_to_quaternion(R@R), l=2)\nnp.allclose(D_mat@D_mat, D_mat_of_square)\n\nTrue\n\n\n\n\nSampling the sphere\nHow do we get a good sampling of unit vectors on the sphere as rotation axes for our initial guess? Let’s use an “icosphere” https://en.wikipedia.org/wiki/Geodesic_polyhedron\n\nsource\n\n\nget_icosphere\n\n get_icosphere (subdivide=0)\n\n*Return the icosphere triangle mesh with 42 regularly spaced vertices on the unit sphere.\nOptionally, subdivide mesh n times, increasing vertex count by factor 4^n.*\n\nsource\n\n\nrotation_alignment_brute_force\n\n rotation_alignment_brute_force (sph_harmonics_source,\n                                 sph_harmonics_target, max_l=None,\n                                 n_angle=100, n_subdiv_axes=1,\n                                 allow_flip=False)\n\n*Compute rotational alignment between two signals on the sphere by brute force.\nThe two signals have to be represented by their spherical harmonics coefficients. Uses Wigner-D matrices to calculate the overlap between the two signals for a set of rotations and finds the rotation that maximizes the overlap.\nThe trial rotations are generated by taking a set of approx. equidistant points on the 2d sphere as rotation axes, and a set of equally spaced angles [0,…, 2*pi] as rotation angles.\nThe rotation is such that it transforms the source signal to match the target.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsph_harmonics_source\ndict of np.array\n\nDictionary, indexed by total angular momentum l=0 ,…, max_l. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l. Source signal, to betransformed.\n\n\nsph_harmonics_target\ndict of np.array\n\nDictionary, indexed by total angular momentum l=0 ,…, max_l. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l. Target signal.\n\n\nmax_l\nNoneType\nNone\nMaximum angular momentum. If None, the maximum value available in the inputspherical harmonics is used.\n\n\nn_angle\nint\n100\nNumber of trial rotation angles [0,…, 2*pi]\n\n\nn_subdiv_axes\nint\n1\nControls the number of trial rotation axes. Rotation axes are vertices ofthe icosphere which can be subdivided. There will be roughly40*4**n_subdiv_axes trial axes. This parameter has the strongest influenceon the run time.\n\n\nallow_flip\nbool\nFalse\nWhether to allow improper rotations with determinant -1. In this case, we return two sets of overlaps and rotation matrices, one for the bestproper, and one for the best improper rotation.\n\n\nReturns\nnp.array, float or (np.array, float), (np.array, float)\n\noptimal_trial_rotation : (3,3) np.array Best trial rotation as rotation matrix.overlap : float Normalized overlap. 1=perfect alignment.\n\n\n\n\n# let's set up our example\n\n#rot_mat = -np.eye(3)\n#rot_mat = stats.special_ortho_group.rvs(3)\nrot_mat = -stats.special_ortho_group.rvs(3)\n\nrot_mat, np.linalg.det(rot_mat)\n\n(array([[ 0.93858452, -0.25998611, -0.22686191],\n        [ 0.29335272,  0.94740493,  0.12793776],\n        [-0.18166805,  0.18663096, -0.96548724]]),\n -1.0000000000000004)\n\n\n\nmesh_rotated = deepcopy(mesh)\nmesh_rotated.vertices = mesh_rotated.vertices @ rot_mat.T\nsignal_rotated = np.copy(signal)\n\ncentroids_rotated = mesh_rotated.vertices[mesh_rotated.tris].mean(axis=1)\n_, theta_rotated, phi_rotated = cartesian_to_spherical(centroids_rotated)\n\nweights_rotated = igl.doublearea(mesh_rotated.vertices, mesh.tris)/2\nspherical_harmonics_coeffs_rotated = compute_spherical_harmonics_coeffs(signal_rotated,\n                                                                        phi_rotated, theta_rotated,\n                                                                        weights_rotated, max_l=max_l)\n\n\nR_opt, overlap, R_opt_flipped, overlap_flipped = rotation_alignment_brute_force(spherical_harmonics_coeffs_direct,\n                                                                                 spherical_harmonics_coeffs_rotated,\n                                                            max_l=10, n_angle=100, n_subdiv_axes=2, allow_flip=True)\n\nrot_mat_to_quaternion(R_opt_flipped), rot_mat_to_quaternion(rot_mat), overlap, overlap_flipped\n\nCPU times: user 3.62 s, sys: 0 ns, total: 3.62 s\nWall time: 3.62 s\n\n\n(array([ 0.15643447, -0.07492257,  0.13505986, -0.97553765]),\n array([ 0.14097676, -0.10408312,  0.08014416, -0.98125897]),\n (0.9910003623902994+7.84107514069421e-17j),\n (0.9931975537491813+6.548624948738893e-17j))\n\n\n\nnp.linalg.norm(R_opt-rot_mat), np.linalg.norm(R_opt_flipped-rot_mat)\n\n(2.0000958259305763, 0.18184532747844342)\n\n\n\nsource\n\n\nrotation_alignment_refined\n\n rotation_alignment_refined (sph_harmonics_source, sph_harmonics_target,\n                             R_initial, max_l=None, maxfev=100)\n\n*Refine rotational alignment between two signals on the sphere by optimization.\nThe two signals have to be represented by their spherical harmonics coefficients. Uses Wigner-D matrices to calculate the overlap between the two signals and uses Nelder-Mead optimization to find the best one.\nRequires a good initial guess for the rotation, as created by rotation_alignment_brute_force.\nThe rotation is such that it transforms the source signal to match the target.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsph_harmonics_source\ndict of np.array\n\nDictionary, indexed by total angular momentum l=0 ,…, max_l. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l. Source signal, to betransformed.\n\n\nsph_harmonics_target\ndict of np.array\n\nDictionary, indexed by total angular momentum l=0 ,…, max_l. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l. Target signal.\n\n\nR_initial\n(3, 3) np.array\n\nInitial rotation as a rotation matrix\n\n\nmax_l\nNoneType\nNone\nMaximum angular momentum. If None, the maximum value available in the inputspherical harmonics is used.\n\n\nmaxfev\nint\n100\nNumber of function evaluations during optimization. This parameter hasthe strongest influence on the run time.\n\n\nReturns\nnp.array, float\n\noptimal_rotation : (3, 3) np.array Best rotation as rotation matrix. Will always have the same determinant (i.e. -1, 1) as the initial guess.overlap : float Normalized overlap. 1=perfect alignment.\n\n\n\n\nR_refined, overlap_refined = rotation_alignment_refined(spherical_harmonics_coeffs_direct,\n                                                        spherical_harmonics_coeffs_rotated, R_opt_flipped,\n                                                        max_l=10, maxfev=200)\n\nrot_mat_to_quaternion(R_refined), np.linalg.det(R_refined), overlap_refined\n\nCPU times: user 1.74 s, sys: 20 ms, total: 1.76 s\nWall time: 1.75 s\n\n\n(array([ 0.14097678, -0.10408313,  0.08014421, -0.98125896]),\n -0.9999999999999999,\n 0.9999999999999993)\n\n\n\nnp.linalg.norm(R_refined-rot_mat)\n\n1.5330429766260706e-07\n\n\n\nsource\n\n\nrotational_alignment\n\n rotational_alignment (sph_harmonics_source, sph_harmonics_target,\n                       allow_flip=False, max_l=None, n_angle=100,\n                       n_subdiv_axes=1, maxfev=100)\n\n*Rotational alignment between two signals on the sphere.\nThe two signals have to be represented by their spherical harmonics coefficients. Uses Wigner-D matrices to calculate the overlap between the two signals.\nAlignment happens in two steps: first, a coarse alignment using a large number of trial rotation axes and angles, and then refinement via optimization.\nThe rotation is such that it transforms the source signal to match the target.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsph_harmonics_source\ndict of np.array\n\nDictionary, indexed by total angular momentum l=0 ,…, max_l. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l. Source signal, to betransformed.\n\n\nsph_harmonics_target\ndict of np.array\n\nDictionary, indexed by total angular momentum l=0 ,…, max_l. Each entry is a vectorof coefficients for the different values of m=-2l,…,2*l. Target signal.\n\n\nallow_flip\nbool\nFalse\nWhether to allow improper rotations with determinant -1.\n\n\nmax_l\nNoneType\nNone\nMaximum angular momentum. If None, the maximum value available in the inputspherical harmonics is used.\n\n\nn_angle\nint\n100\nNumber of trial rotation angles [0,…, 2*pi]\n\n\nn_subdiv_axes\nint\n1\nControls the number of trial rotation axes. Rotation axes are vertices ofthe icosphere which can be subdivided. There will be roughly40*4**n_subdiv_axes trial axes. This parameter has the strongest influenceon the run time.\n\n\nmaxfev\nint\n100\nNumber of function evaluations during fine optimization.\n\n\nReturns\nnp.array, float\n\noptimal_rotation : (3, 3) np.array Best rotation as rotation matrix. Will always have the same determinant (i.e. -1, 1) as the initial guess.overlap : float Normalized overlap. 1=perfect alignment.\n\n\n\n\nR_refined, overlap = rotational_alignment(spherical_harmonics_coeffs_direct, spherical_harmonics_coeffs_rotated,\n                                          max_l=10, n_angle=100, n_subdiv_axes=1, maxfev=200,\n                                          allow_flip=True)\n\nnp.linalg.norm(R_refined-rot_mat), np.round(np.linalg.det(R_refined)), overlap\n\nCPU times: user 4.49 s, sys: 35 μs, total: 4.49 s\nWall time: 4.48 s\n\n\n(1.5571961945381298e-07, -1.0, 0.999999999999998)\n\n\n\n\nHow well does this work?\nLooks pretty good! I find that using about 160 trial rotation axes in the brute force step (subdiv=1) is a good choice - fewer trial axes lead to poor initial guesses.\nHowever, the first guess can often get the parity wrong! Therefore, it is best to run brute force and fine alignment for both parities, and then choose which one is better.",
    "crumbs": [
      "Python library",
      "3D-rotation registration"
    ]
  },
  {
    "objectID": "Python library/01d_interface_trimesh.html",
    "href": "Python library/01d_interface_trimesh.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Convert ObjMesh to trimesh’s mesh class.\n\nIn this notebook, we define functions to convert our ObjMesh class to and from trimesh (https://trimesh.org/) which is a python library for triangle meshes. I generally prefer to use igl, but maybe trimesh has some feature you want.\nNote: trimesh represents triangular meshes only, and its way of representing UV information is not ideal. It is not recommended to edit mesh topology in trimesh if the UV mapping has already been defined.\n\nsource\n\n\n\n convert_to_trimesh (mesh:blender_tissue_cartography.mesh.ObjMesh,\n                     add_texture_info=None)\n\n*Convert tcmesh.ObjMesh to trimesh.Trimesh\nSee https://trimesh.org/trimesh.base.html Note: normal information is recalculated. Discards any non-triangle faces.\nTexture is saved as a vertex attribute via v_tex_coords_matrix. Note that this discards information since a vertex can have multiple texture coordinates! For this reason, we also add the texture coordinates as a (n_faces, 3, s)-array attribute face_tex. Note: this will not be updated if you remesh etc.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\n\n\n\nadd_texture_info\nNoneType\nNone\nWhether to add texture info to the trimesh.Trimesh. If None, texture is added ifavailable for at least one vertex.\n\n\nReturns\nTrimesh\n\n\n\n\n\n\nmesh_fname_data = \"datasets/registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\nmesh_fname_ref = \"datasets/registration_example/Drosophila_reference.obj\"\nmesh_data = tcmesh.ObjMesh.read_obj(mesh_fname_data)\nmesh_ref = tcmesh.ObjMesh.read_obj(mesh_fname_ref)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\ntrimesh_data = convert_to_trimesh(mesh_data)\nnp.allclose(mesh_data.vertices, trimesh_data.vertices)\n\nTrue\n\n\n\ntrimesh_normals = trimesh_data.vertex_normals\ntrimesh_normals = (trimesh_normals.T / np.linalg.norm(trimesh_normals, axis=-1)).T\n\nnp.einsum('vi,vi-&gt;v', mesh_data.normals, trimesh_normals)\n\narray([6.27953731, 6.26481851, 6.27572446, ..., 6.24734426, 6.22183872,\n       6.25817202])\n\n\n\nsource\n\n\n\n\n convert_from_trimesh (mesh:trimesh.base.Trimesh,\n                       reconstruct_texture_from_faces=True,\n                       texture_vertex_decimals=10)\n\n*Convert trimesh mesh to ObjMesh.\nTexture vertices can be reconstructed from face attribute face_tex or from vertex attribute vertex_tex_coord_matrix. Reconstruction from face texture can accommodate multiple texture coordinates per vertex (e.g. for UV maps with seams).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nTrimesh\n\n\n\n\nreconstruct_texture_from_faces\nbool\nTrue\nWhether to reconstruct texture information from per-face data (True), orper-vertex data (False)\n\n\ntexture_vertex_decimals\nint\n10\nTexture vertices are rounded to texture_vertex_decimals decimals.\n\n\nReturns\nObjMesh\n\n\n\n\n\n\ntrimesh_ref = convert_to_trimesh(mesh_ref)\n\n\nmesh_seams = tcmesh.ObjMesh.read_obj(\"datasets/drosophila_example/Drosophila_CAAX-mCherry_mesh_uv.obj\")\ntrimesh_seams = convert_to_trimesh(mesh_seams,add_texture_info=True)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_CAAX-mCherry_mesh_remeshed\nWarning: readOBJ() ignored non-comment line 48073:\n  l 2534 8160\n/home/nikolas/Programs/miniconda3/envs/blender-tissue-cartography/lib/python3.11/site-packages/trimesh/grouping.py:99: RuntimeWarning: invalid value encountered in cast\n  stacked = np.column_stack(stacked).round().astype(np.int64)\n\n\n\nconvert_from_trimesh(trimesh_seams, reconstruct_texture_from_faces=False).texture_vertices.shape, mesh_seams.texture_vertices.shape\n\n((8159, 2), (8288, 2))",
    "crumbs": [
      "Python library",
      "`trimesh` interface"
    ]
  },
  {
    "objectID": "Python library/01d_interface_trimesh.html#trimesh-interface",
    "href": "Python library/01d_interface_trimesh.html#trimesh-interface",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Convert ObjMesh to trimesh’s mesh class.\n\nIn this notebook, we define functions to convert our ObjMesh class to and from trimesh (https://trimesh.org/) which is a python library for triangle meshes. I generally prefer to use igl, but maybe trimesh has some feature you want.\nNote: trimesh represents triangular meshes only, and its way of representing UV information is not ideal. It is not recommended to edit mesh topology in trimesh if the UV mapping has already been defined.\n\nsource\n\n\n\n convert_to_trimesh (mesh:blender_tissue_cartography.mesh.ObjMesh,\n                     add_texture_info=None)\n\n*Convert tcmesh.ObjMesh to trimesh.Trimesh\nSee https://trimesh.org/trimesh.base.html Note: normal information is recalculated. Discards any non-triangle faces.\nTexture is saved as a vertex attribute via v_tex_coords_matrix. Note that this discards information since a vertex can have multiple texture coordinates! For this reason, we also add the texture coordinates as a (n_faces, 3, s)-array attribute face_tex. Note: this will not be updated if you remesh etc.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\n\n\n\nadd_texture_info\nNoneType\nNone\nWhether to add texture info to the trimesh.Trimesh. If None, texture is added ifavailable for at least one vertex.\n\n\nReturns\nTrimesh\n\n\n\n\n\n\nmesh_fname_data = \"datasets/registration_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\"\nmesh_fname_ref = \"datasets/registration_example/Drosophila_reference.obj\"\nmesh_data = tcmesh.ObjMesh.read_obj(mesh_fname_data)\nmesh_ref = tcmesh.ObjMesh.read_obj(mesh_fname_ref)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\ntrimesh_data = convert_to_trimesh(mesh_data)\nnp.allclose(mesh_data.vertices, trimesh_data.vertices)\n\nTrue\n\n\n\ntrimesh_normals = trimesh_data.vertex_normals\ntrimesh_normals = (trimesh_normals.T / np.linalg.norm(trimesh_normals, axis=-1)).T\n\nnp.einsum('vi,vi-&gt;v', mesh_data.normals, trimesh_normals)\n\narray([6.27953731, 6.26481851, 6.27572446, ..., 6.24734426, 6.22183872,\n       6.25817202])\n\n\n\nsource\n\n\n\n\n convert_from_trimesh (mesh:trimesh.base.Trimesh,\n                       reconstruct_texture_from_faces=True,\n                       texture_vertex_decimals=10)\n\n*Convert trimesh mesh to ObjMesh.\nTexture vertices can be reconstructed from face attribute face_tex or from vertex attribute vertex_tex_coord_matrix. Reconstruction from face texture can accommodate multiple texture coordinates per vertex (e.g. for UV maps with seams).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nTrimesh\n\n\n\n\nreconstruct_texture_from_faces\nbool\nTrue\nWhether to reconstruct texture information from per-face data (True), orper-vertex data (False)\n\n\ntexture_vertex_decimals\nint\n10\nTexture vertices are rounded to texture_vertex_decimals decimals.\n\n\nReturns\nObjMesh\n\n\n\n\n\n\ntrimesh_ref = convert_to_trimesh(mesh_ref)\n\n\nmesh_seams = tcmesh.ObjMesh.read_obj(\"datasets/drosophila_example/Drosophila_CAAX-mCherry_mesh_uv.obj\")\ntrimesh_seams = convert_to_trimesh(mesh_seams,add_texture_info=True)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_CAAX-mCherry_mesh_remeshed\nWarning: readOBJ() ignored non-comment line 48073:\n  l 2534 8160\n/home/nikolas/Programs/miniconda3/envs/blender-tissue-cartography/lib/python3.11/site-packages/trimesh/grouping.py:99: RuntimeWarning: invalid value encountered in cast\n  stacked = np.column_stack(stacked).round().astype(np.int64)\n\n\n\nconvert_from_trimesh(trimesh_seams, reconstruct_texture_from_faces=False).texture_vertices.shape, mesh_seams.texture_vertices.shape\n\n((8159, 2), (8288, 2))",
    "crumbs": [
      "Python library",
      "`trimesh` interface"
    ]
  },
  {
    "objectID": "Python library/05c_wrapping.html",
    "href": "Python library/05c_wrapping.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Morph one surface mesh onto another by projecting each vertex to closest point on target surface. Part of pipeline for dynamic data.\n\nThis notebook builds the tools for automated mesh “shrink-wrapping” (setting vertex positions of one mesh to the closest point on another mesh). This is a part of the pipeline for dynamic surfaces discussed in tutorial 8. Shrink-wrapping can also be done interactively in blender, using the shrink-wrap modifier.\n\nfrom tqdm.notebook import tqdm",
    "crumbs": [
      "Python library",
      "Shrink-wrapping"
    ]
  },
  {
    "objectID": "Python library/05c_wrapping.html#shrink-wrapping",
    "href": "Python library/05c_wrapping.html#shrink-wrapping",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Morph one surface mesh onto another by projecting each vertex to closest point on target surface. Part of pipeline for dynamic data.\n\nThis notebook builds the tools for automated mesh “shrink-wrapping” (setting vertex positions of one mesh to the closest point on another mesh). This is a part of the pipeline for dynamic surfaces discussed in tutorial 8. Shrink-wrapping can also be done interactively in blender, using the shrink-wrap modifier.\n\nfrom tqdm.notebook import tqdm",
    "crumbs": [
      "Python library",
      "Shrink-wrapping"
    ]
  },
  {
    "objectID": "Python library/05c_wrapping.html#loading-and-segmenting-the-dataset",
    "href": "Python library/05c_wrapping.html#loading-and-segmenting-the-dataset",
    "title": "blender-tissue-cartography",
    "section": "Loading and segmenting the dataset",
    "text": "Loading and segmenting the dataset\nWe will use the same dataset - a Drosophila example - as in tutorial 4.\n\nmetadata_dict = {'filename': 'datasets/wrapping_example/Drosophila_CAAX-mCherry',\n                 'resolution_in_microns': (1.05, 1.05, 1.05), # lightsheet data has isotropic resolution\n                 'subsampling_factors': (1/2, 1/2, 1/2),\n                }\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order\n\nimage shape: (1, 190, 509, 188)\n\n\n\n3d segmentation / Meshing\nAlready done - we’ll just read in the results.\n\n\nRegistration\nUsing the tools from notebook 4a, compute an affine registration of the reference to the data mesh.\n\nmesh_data = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\nmesh_ref = tcmesh.ObjMesh.read_obj(f\"datasets/wrapping_example/Drosophila_reference.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\ntrafo_centroid_intertia, _ = tcreg.align_by_centroid_and_intertia(mesh_ref.vertices, mesh_data.vertices,\n                                                                  q=0.01, shear=True)\n\ntrafo_icp, _, _ = tcreg.icp(mesh_ref.vertices, mesh_data.vertices, initial=trafo_centroid_intertia,\n                            threshold=1e-4, max_iterations=100, include_scaling=True, n_samples=5000)\n\n\n# We now save the pre-registered mesh. Note that we want to transform both our\n# mesh coordinates and the corresponding normals, as well as potentially flip face orientation\n\nmesh_registered = mesh_ref.apply_affine_to_mesh(trafo_icp)\nmesh_registered.name = \"Drosophila_reference_preregistered\"\nmesh_registered.write_obj(f\"datasets/wrapping_example/Drosophila_reference_registered.obj\")\n\n\n\nShrink-wrapping using libigl\n\n# Load source and target mesh\nmesh_target = tcmesh.ObjMesh.read_obj(\"datasets/wrapping_example/Drosophila_CAAX-mCherry_mesh_remeshed.obj\")\nmesh_source = tcmesh.ObjMesh.read_obj(\"datasets/wrapping_example/Drosophila_reference_registered.obj\")\n\nWarning: readOBJ() ignored non-comment line 1:\n  o Drosophila_reference_preregistered\n\n\n\n# compute closest point on target mesh for each source vertex\ndistances, indices, points = igl.point_mesh_squared_distance(mesh_source.vertices,\n                                                             mesh_target.vertices, mesh_target.tris)\n\n\n# points are closest points on the surface, not necessarily vertices\nnp.linalg.norm(points[10]-mesh_target.vertices, axis=1).min()\n\n1.927821820512808\n\n\n\n# indices are triangle indices\nindices.max(), mesh_target.vertices.shape[0]\n\n(16845, 8425)\n\n\n\nmesh_wrapped = tcmesh.ObjMesh(points, mesh_source.faces, texture_vertices=mesh_source.texture_vertices,\n                            normals=None, name=mesh_source.name)\nmesh_wrapped.set_normals()\n\n\nmesh_wrapped.write_obj(\"datasets/wrapping_example/Drosophila_reference_wrapped_igl.obj\")\n\n\nsource\n\n\nshrinkwrap_igl\n\n shrinkwrap_igl (mesh_source, mesh_target, n_iter_smooth_target=10,\n                 n_iter_smooth_wrapped=10)\n\n*Shrink-wrap the source mesh onto the target mesh using trimesh.\nSets the vertex positions of mesh_source to the closest point on the surface of mesh_target (not necessarily a vertex). Optionally, smooth the target mesh and the wrapped mesh for smoother results using a Taubin filter (recommended). Gives out a warning if the shrink-wrapping flips any vertex normals, which can indicate problems.\nThe shrinkwrapped mesh still has the UV maps of the source mesh, and so can be used to compute cartographic projections. Assumes mesh is triangular.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh_source\ntcmesh.ObjMesh\n\nMesh to be deformed\n\n\nmesh_target\ntcmesh.ObjMesh\n\nMesh with the target shape\n\n\nn_iter_smooth_target\nint\n10\nTaubin smoothing iterations for target\n\n\nn_iter_smooth_wrapped\nint\n10\nTaubin smoothing iterations for shrinkwrapped mesh, after shrinkwrapping\n\n\nReturns\ntcmesh.ObjMesh\n\n\n\n\n\n\nmesh_wrapped = shrinkwrap_igl(mesh_source, mesh_target)\nmesh_wrapped.write_obj(\"datasets/wrapping_example/Drosophila_reference_wrapped_igl.obj\")\n\nCPU times: user 2.09 s, sys: 51.9 ms, total: 2.14 s\nWall time: 2.04 s\n\n\n\n\nAttribute copying\nWe can use the shrink-wrap technique to copy over per-vertex attributes from one mesh to another. This is nothing other than barycentric interpolation, and we can use interpolation.interpolate_barycentric!",
    "crumbs": [
      "Python library",
      "Shrink-wrapping"
    ]
  },
  {
    "objectID": "Python library/05c_wrapping.html#uv-projection",
    "href": "Python library/05c_wrapping.html#uv-projection",
    "title": "blender-tissue-cartography",
    "section": "UV projection",
    "text": "UV projection\nTo see how well all of this has worked, let’s use the wrapped mesh to generate UV projections.\n\nnormal_offsets = np.array([-4, -2, 0, 2]) ##np.linspace(-5, 2, 8) # in microns\nmetadata_dict[\"normal_offsets\"] = normal_offsets # add the info to the metadata\nuv_grid_steps = 512\n\n\ntcio.save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)\n\n\nprojected_data_wrapped, projected_coordinates_wrapped, projected_normals_wrapped = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=f\"datasets/wrapping_example/Drosophila_reference_wrapped_igl.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps,\n    use_fallback=False) # don't worry about warning - it just looks like the reference mesh has a UV map with 'flipped' orientation.\n\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/interpolation.py:215: RuntimeWarning: UV map has self-intersections, 111104 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n\n\n\n# show the projected data - looks good!\n\nfig, (ax1, ax2) = plt.subplots(figsize=(8,8), ncols=2)\nax1.imshow(projected_data_wrapped[0, 0], vmax=10000)\nax2.imshow(projected_data_wrapped[0, 2], vmax=10000)",
    "crumbs": [
      "Python library",
      "Shrink-wrapping"
    ]
  },
  {
    "objectID": "Python library/differential_geometry.html",
    "href": "Python library/differential_geometry.html",
    "title": "Surface differential geometry",
    "section": "",
    "text": "When analyzing data in cartographic projections, we have to make sure we are always accounting for mapping distortion and curvature. This notebook builds the relevant tools, including for vector calculus on curved surfaces.\nWhen analyzing quantities computed in the UV cartographic projection (e.g. cell areas, orientation, tissue flow speed obtained from optical flow), you need to account for the cartographic projection. I generally recommend doing this by mapping every object back to 3d as soon as possible. This will minimize the number of possible errors. Example: you want to evaluate the angle between two edges of a cell. 1. You could do so in the UV map using the induced metric (see below). This is error-prone: did you calculate the metric correctly? Did you use the metric or its inverse? 2. You could map back the location of the cell vertices to 3d, and then calculate the edge vectors in 3d. This minimizes possibilities for confusion.",
    "crumbs": [
      "Python library",
      "Surface differential geometry"
    ]
  },
  {
    "objectID": "Python library/differential_geometry.html#vector-calculus-on-curved-surfaces",
    "href": "Python library/differential_geometry.html#vector-calculus-on-curved-surfaces",
    "title": "Surface differential geometry",
    "section": "Vector calculus on curved surfaces",
    "text": "Vector calculus on curved surfaces\nHow to generalize the familiar operations of vector calculus (div, rot, grad, etc) to curved surfaces is a key part of differential geometry. There are different approaches for doing surface differential geometry numerically:\n\nUsing local coordinates/parametrizations (i.e. the way physicists learn it in GR). This is prone to errors and numerically unfavorable\nUsing “intrinsic” discretization schemes like Discrete Exterior Calculus. These are elegant and numerically efficient, but difficult to use and understand for non-experts.\n\nHere, we take a less elegant, but simpler approach that takes advantage of the fact that our surfaces are embedded in 3d cartesian space. All vector and tensor fields are mapped back into 3d so that their components are defined in \\(x,y,z\\) coordinates. Then we can imagine “extending” from the surface into full 3d space by defining them to be constant along the local surface normal. Then we are back to normal vector calculus!\nIn practice, we don’t need to do this extension explicitly. We can compute the derivative of any quantity defined on mesh vertices in such a way that the derivative along the normal direction is 0. This is implemented by standard “finite-element” gradient operators. See here: https://libigl.github.io/libigl-python-bindings/tut-chapter1/#gradient\n\nsource\n\ntri_grad\n\n tri_grad (field, vertices, faces, grad_matrix=None)\n\n*Calculate the gradient of a function defined on vertices of a triangular mesh.\nIf a vector or tensor field is passed, the gradient is applied to each component individually.\nSee https://libigl.github.io/libigl-python-bindings/tut-chapter1/#gradient*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfield\nnp.array of shape (#vertices,…)\n\nscalar, vector, or tensor field defined at mesh vertices\n\n\nvertices\nnp.array of shape (#vertices, dim)\n\nvertices.\n\n\nfaces\nnp.array of shape (#faces, 3)\n\nTriangular faces.\n\n\ngrad_matrix\nNoneType\nNone\nGradient operator. The default is None (calculate from vertices, faces).\n\n\nReturns\nnp.array of shape (#vertices, dim, …)\n\nGradient of scalar function/tensor, defined on vertices.Axis 1 comprises the gradients along x,y,z.\n\n\n\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/initial_uv.obj\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o mesh_01_cylinder_seams_uv\n\n\n\n# get some random fields - eigenfunctions of the laplacian\n\nlaplacian = igl.cotmatrix(mesh.vertices, mesh.tris)\nmass = igl.massmatrix(mesh.vertices, mesh.tris)\n\neigen_vals, eigen_vecs = sparse.linalg.eigsh(-laplacian, M=mass, k=10, which=\"SM\")\neigen_vals, eigen_vecs = (eigen_vals[1:], eigen_vecs[:, 1:])\n\neigen_vals = eigen_vals / eigen_vals[0]\neigen_vecs = eigen_vecs / np.abs(eigen_vecs).mean()\n\nscalar_field = eigen_vecs[:,0]\nvector_field = eigen_vecs[:,:3]\ntensor_field = eigen_vecs.reshape((-1, 3, 3))\n\n\ntri_grad(tensor_field, mesh.vertices, mesh.tris, ).shape\n\n(20212, 3, 3, 3)\n\n\n\nscalar_gradient = tri_grad(scalar_field, mesh.vertices, mesh.tris,)\nnormals = igl.per_vertex_normals(mesh.vertices, mesh.tris)\nnormals = (normals.T/np.linalg.norm(normals, axis=-1)).T\n\n\nnormal_component = (np.abs(np.einsum('vi,vi-&gt;v', normals, scalar_gradient))\n                    / np.linalg.norm(scalar_gradient, axis=-1))\n\n\nnp.mean(normal_component)\n\n0.0011230080906213756\n\n\n\n# Here is how you would map the gradients to 2d. You first need to apply the Jacobian,\n# and then you still need to reindex.\n\njac = compute_per_vertex_jacobian(mesh.vertices, mesh.tris, mesh.texture_vertices, mesh.texture_tris, )\nscalar_gradient_projected = np.einsum('vij,vj-&gt;vi', jac, scalar_gradient)[mesh.get_vertex_to_texture_vertex_indices()]\n\n\nplt.scatter(*mesh.texture_vertices.T, c=scalar_field[mesh.get_vertex_to_texture_vertex_indices()])\n\nsk = 25\nplt.quiver(mesh.texture_vertices[::sk, 0], mesh.texture_vertices[::sk, 1],\n           scalar_gradient_projected[::sk, 0], scalar_gradient_projected[::sk, 1])\n\nplt.axis(\"equal\")",
    "crumbs": [
      "Python library",
      "Surface differential geometry"
    ]
  },
  {
    "objectID": "Python library/differential_geometry.html#projection-onto-surface-normal",
    "href": "Python library/differential_geometry.html#projection-onto-surface-normal",
    "title": "Surface differential geometry",
    "section": "Projection onto surface normal",
    "text": "Projection onto surface normal\n\nsource\n\nget_normal_projector\n\n get_normal_projector (vertices=None, faces=None, normals=None)\n\n*Get projection matrix that removes components normal to the surface\nMathematically, 1-n.n^T where n is the unit surface normal. Defined per vertex.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvertices\nNoneType\nNone\nvertices. If None, must supply normals\n\n\nfaces\nNoneType\nNone\nTriangular faces. If None, must supply normals\n\n\nnormals\nNoneType\nNone\nIf None, recompute normals from vertices and faces\n\n\nReturns\nnp.array of shape (#vertices, dim, dim)\n\nProjector\n\n\n\n\nsource\n\n\nseparate_tangential_normal\n\n separate_tangential_normal (field, vertices=None, faces=None,\n                             normals=None)\n\n*Separate tangential and normal components of field defined at vertices.\nVector and rank-2 tensor fields are supported. For a rank-2 tensor, normal-tangential cross components are discarded.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfield\nnp.array of shape (#vertices, dim) or (#vertices, dim, dim)\n\nVector or rank-2 tensor field defined at vertices\n\n\nvertices\nNoneType\nNone\nvertices. If None, must supply normals\n\n\nfaces\nNoneType\nNone\nTriangular faces. If None, must supply normals\n\n\nnormals\nNoneType\nNone\nIf None, recompute normals from vertices and faces\n\n\nReturns\nnp.array, np.array\n\ntangential_component : np.array of shape (#vertices, dim) or (#vertices, dim, dim)normal_component : np.array of shape (#vertices, dim) or (#vertices, dim, dim)\n\n\n\n\nP = get_normal_projector(mesh.vertices, mesh.tris)\n\n\ntangential, normal = separate_tangential_normal(vector_field, normals=mesh.normals)\n\n\nnp.einsum('vi,vi-&gt;v', tangential, mesh.normals), np.einsum('vi,vi-&gt;v', normal, mesh.normals)\n\n(array([-0.00080435, -0.000788  , -0.00274249, ...,  0.00730656,\n         0.0039478 ,  0.00005069]),\n array([-0.09762832, -0.09401892, -0.10817697, ...,  2.10673152,\n         2.12017993,  0.08589414]))",
    "crumbs": [
      "Python library",
      "Surface differential geometry"
    ]
  },
  {
    "objectID": "Python library/differential_geometry.html#div-and-rot",
    "href": "Python library/differential_geometry.html#div-and-rot",
    "title": "Surface differential geometry",
    "section": "div and rot",
    "text": "div and rot\n\nsource\n\nget_grad_perp\n\n get_grad_perp (field, vertices, faces, normals=None)\n\n*Calculate the gradient of a scalar field, rotated by 90 deg around the surface normal.\nAs occurs e.g. when calculating vector field from stream function.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfield\nnp.array of shape (#vertices,)\n\nScalar field defined at mesh vertices\n\n\nvertices\nnp.array of shape (#vertices, dim)\n\nvertices.\n\n\nfaces\nnp.array of shape (#faces, 3)\n\nTriangular faces.\n\n\nnormals\nNoneType\nNone\nIf None, recompute normals from vertices and faces.sign of normals determines sense of rotation.\n\n\nReturns\nnp.array of shape (#vertices, dim)\n\n90-degree rotated gradient of scalar field. \n\n\n\n\nsource\n\n\nget_rot\n\n get_rot (field, vertices, faces, normals=None)\n\n*Calculate tangent-plane rotation of vector field defined on vertices of triangular mesh.\nThis result is a scalar, equal to (Nabla x field).normals*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfield\nnp.array of shape (#vertices, dim)\n\nvector field defined at mesh vertices\n\n\nvertices\nnp.array of shape (#vertices, dim)\n\nvertices.\n\n\nfaces\nnp.array of shape (#faces, 3)\n\nTriangular faces.\n\n\nnormals\nNoneType\nNone\nIf None, recompute normals from vertices and faces\n\n\nReturns\nnp.array of shape (#vertices,)\n\nCurl of vector field. \n\n\n\n\nsource\n\n\nget_div\n\n get_div (field, vertices, faces, normals=None)\n\nCalculate tangent-plane divergence of vector field defined on vertices of triangular mesh.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfield\nnp.array of shape (#vertices, dim)\n\nvector field defined at mesh vertices\n\n\nvertices\nnp.array of shape (#vertices, dim)\n\nvertices.\n\n\nfaces\nnp.array of shape (#faces, 3)\n\nTriangular faces.\n\n\nnormals\nNoneType\nNone\nIf None, recompute normals from vertices and faces\n\n\nReturns\nnp.array of shape (#vertices,)\n\nDivergence of vector field.\n\n\n\n\nget_div(vector_field, mesh.vertices, mesh.tris)\n\narray([-0.00132147, -0.00133631, -0.00139445, ..., -0.01190545,\n       -0.01149585, -0.00547986])\n\n\n\nget_rot(vector_field, mesh.vertices, mesh.tris)\n\narray([-0.00019284, -0.00021695, -0.00021081, ..., -0.00062418,\n       -0.00052652,  0.00221077])",
    "crumbs": [
      "Python library",
      "Surface differential geometry"
    ]
  },
  {
    "objectID": "Python library/01e_morphsnakes.html",
    "href": "Python library/01e_morphsnakes.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Morphological snakes algorithm for 3D image segmentation. Source: https://github.com/pmneila/morphsnakes\n\nThis code was written by P. Márquez Neila p.mneila@upm.es and is included in this repository for ease of installation.\nLICENSE\nCopyright (c) 2013-2015, P. M. Neila All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of morphsnakes nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nMorphological Snakes [1]_ are a family of methods for image segmentation. Their behavior is similar to that of active contours (for example, Geodesic Active Contours [2]_ or Active Contours without Edges [3]_). However, Morphological Snakes use morphological operators (such as dilation or erosion) over a binary array instead of solving PDEs over a floating point array, which is the standard approach for active contours. This makes Morphological Snakes faster and numerically more stable than their traditional counterpart.\nThere are two Morphological Snakes methods available in this implementation: Morphological Geodesic Active Contours (MorphGAC, implemented in the function [morphological_geodesic_active_contour](https://nikolas-claussen.github.io/blender-tissue-cartography/Python library/morphsnakes.html#morphological_geodesic_active_contour)) and Morphological Active Contours without Edges (MorphACWE, implemented in the function [morphological_chan_vese](https://nikolas-claussen.github.io/blender-tissue-cartography/Python library/morphsnakes.html#morphological_chan_vese)).\nMorphGAC is suitable for images with visible contours, even when these contours might be noisy, cluttered, or partially unclear. It requires, however, that the image is preprocessed to highlight the contours. This can be done using the function [inverse_gaussian_gradient](https://nikolas-claussen.github.io/blender-tissue-cartography/Python library/morphsnakes.html#inverse_gaussian_gradient), although the user might want to define their own version. The quality of the MorphGAC segmentation depends greatly on this preprocessing step.\nOn the contrary, MorphACWE works well when the pixel values of the inside and the outside regions of the object to segment have different averages. Unlike MorphGAC, MorphACWE does not require that the contours of the object are well defined, and it works over the original image without any preceding processing. This makes MorphACWE easier to use and tune than MorphGAC.",
    "crumbs": [
      "Python library",
      "Morphsnakes segmentation"
    ]
  },
  {
    "objectID": "Python library/01e_morphsnakes.html#morphsnakes-segmentation",
    "href": "Python library/01e_morphsnakes.html#morphsnakes-segmentation",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Morphological snakes algorithm for 3D image segmentation. Source: https://github.com/pmneila/morphsnakes\n\nThis code was written by P. Márquez Neila p.mneila@upm.es and is included in this repository for ease of installation.\nLICENSE\nCopyright (c) 2013-2015, P. M. Neila All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of morphsnakes nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nMorphological Snakes [1]_ are a family of methods for image segmentation. Their behavior is similar to that of active contours (for example, Geodesic Active Contours [2]_ or Active Contours without Edges [3]_). However, Morphological Snakes use morphological operators (such as dilation or erosion) over a binary array instead of solving PDEs over a floating point array, which is the standard approach for active contours. This makes Morphological Snakes faster and numerically more stable than their traditional counterpart.\nThere are two Morphological Snakes methods available in this implementation: Morphological Geodesic Active Contours (MorphGAC, implemented in the function [morphological_geodesic_active_contour](https://nikolas-claussen.github.io/blender-tissue-cartography/Python library/morphsnakes.html#morphological_geodesic_active_contour)) and Morphological Active Contours without Edges (MorphACWE, implemented in the function [morphological_chan_vese](https://nikolas-claussen.github.io/blender-tissue-cartography/Python library/morphsnakes.html#morphological_chan_vese)).\nMorphGAC is suitable for images with visible contours, even when these contours might be noisy, cluttered, or partially unclear. It requires, however, that the image is preprocessed to highlight the contours. This can be done using the function [inverse_gaussian_gradient](https://nikolas-claussen.github.io/blender-tissue-cartography/Python library/morphsnakes.html#inverse_gaussian_gradient), although the user might want to define their own version. The quality of the MorphGAC segmentation depends greatly on this preprocessing step.\nOn the contrary, MorphACWE works well when the pixel values of the inside and the outside regions of the object to segment have different averages. Unlike MorphGAC, MorphACWE does not require that the contours of the object are well defined, and it works over the original image without any preceding processing. This makes MorphACWE easier to use and tune than MorphGAC.",
    "crumbs": [
      "Python library",
      "Morphsnakes segmentation"
    ]
  },
  {
    "objectID": "Python library/01e_morphsnakes.html#references",
    "href": "Python library/01e_morphsnakes.html#references",
    "title": "blender-tissue-cartography",
    "section": "References",
    "text": "References\n[1] A Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo Márquez-Neila, Luis Baumela and Luis Álvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI 10.1109/TPAMI.2013.106\n[2] Geodesic Active Contours, Vicent Caselles, Ron Kimmel and Guillermo Sapiro. In International Journal of Computer Vision (IJCV), 1997, DOI:10.1023/A:1007979827043\n[3] Active Contours without Edges, Tony Chan and Luminita Vese. In IEEE Transactions on Image Processing, 2001, DOI:10.1109/83.902291\n/opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section See Also\n  else: warn(msg)\n\nsource\n\nmorphological_geodesic_active_contour\n\n morphological_geodesic_active_contour (gimage, iterations,\n                                        init_level_set='circle',\n                                        smoothing=1, threshold='auto',\n                                        balloon=0, iter_callback=&lt;function\n                                        &lt;lambda&gt;&gt;)\n\n*Morphological Geodesic Active Contours (MorphGAC).\nGeodesic active contours implemented with morphological operators. It can be used to segment objects with visible but noisy, cluttered, broken borders.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngimage\n(M, N) or (L, M, N) array\n\nPreprocessed image or volume to be segmented. This is very rarely theoriginal image. Instead, this is usually a preprocessed version of theoriginal image that enhances and highlights the borders (or otherstructures) of the object to segment.morphological_geodesic_active_contour will try to stop the contourevolution in areas where gimage is small. Seemorphsnakes.inverse_gaussian_gradient as an example function toperform this preprocessing. Note that the quality ofmorphological_geodesic_active_contour might greatly depend on thispreprocessing.\n\n\niterations\nuint\n\nNumber of iterations to run.\n\n\ninit_level_set\nstr\ncircle\nInitial level set. If an array is given, it will be binarized and usedas the initial level set. If a string is given, it defines the methodto generate a reasonable initial level set with the shape of theimage. Accepted values are ‘checkerboard’ and ‘circle’. See thedocumentation of checkerboard_level_set and circle_level_setrespectively for details about how these level sets are created.\n\n\nsmoothing\nint\n1\nNumber of times the smoothing operator is applied per iteration.Reasonable values are around 1-4. Larger values lead to smoothersegmentations.\n\n\nthreshold\nstr\nauto\nAreas of the image with a value smaller than this threshold will beconsidered borders. The evolution of the contour will stop in thisareas.\n\n\nballoon\nint\n0\nBalloon force to guide the contour in non-informative areas of theimage, i.e., areas where the gradient of the image is too small to pushthe contour towards a border. A negative value will shrink the contour,while a positive value will expand the contour in these areas. Settingthis to zero will disable the balloon force.\n\n\niter_callback\nfunction\n\nIf given, this function is called once per iteration with the currentlevel set as the only argument. This is useful for debugging or forplotting intermediate results during the evolution.\n\n\nReturns\n(M, N) or (L, M, N) array\n\nFinal segmentation (i.e., the final level set)\n\n\n\n\nsource\n\n\nmorphological_chan_vese\n\n morphological_chan_vese (image, iterations,\n                          init_level_set='checkerboard', smoothing=1,\n                          lambda1=1, lambda2=1, iter_callback=&lt;function\n                          &lt;lambda&gt;&gt;)\n\n*Morphological Active Contours without Edges (MorphACWE)\nActive contours without edges implemented with morphological operators. It can be used to segment objects in images and volumes without well defined borders. It is required that the inside of the object looks different on average than the outside (i.e., the inner area of the object should be darker or lighter than the outer area on average).*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\n(M, N) or (L, M, N) array\n\nGrayscale image or volume to be segmented.\n\n\niterations\nuint\n\nNumber of iterations to run\n\n\ninit_level_set\nstr\ncheckerboard\nInitial level set. If an array is given, it will be binarized and usedas the initial level set. If a string is given, it defines the methodto generate a reasonable initial level set with the shape of theimage. Accepted values are ‘checkerboard’ and ‘circle’. See thedocumentation of checkerboard_level_set and circle_level_setrespectively for details about how these level sets are created.\n\n\nsmoothing\nint\n1\nNumber of times the smoothing operator is applied per iteration.Reasonable values are around 1-4. Larger values lead to smoothersegmentations.\n\n\nlambda1\nint\n1\nWeight parameter for the outer region. If lambda1 is larger thanlambda2, the outer region will contain a larger range of values thanthe inner region.\n\n\nlambda2\nint\n1\nWeight parameter for the inner region. If lambda2 is larger thanlambda1, the inner region will contain a larger range of values thanthe outer region.\n\n\niter_callback\nfunction\n\nIf given, this function is called once per iteration with the currentlevel set as the only argument. This is useful for debugging or forplotting intermediate results during the evolution.\n\n\nReturns\n(M, N) or (L, M, N) array\n\nFinal segmentation (i.e., the final level set)\n\n\n\n\nsource\n\n\ninverse_gaussian_gradient\n\n inverse_gaussian_gradient (image, alpha=100.0, sigma=5.0)\n\n*Inverse of gradient magnitude.\nCompute the magnitude of the gradients in the image and then inverts the result in the range [0, 1]. Flat areas are assigned values close to 1, while areas close to borders are assigned values close to 0.\nThis function or a similar one defined by the user should be applied over the image as a preprocessing step before calling morphological_geodesic_active_contour.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage\n(M, N) or (L, M, N) array\n\nGrayscale image or volume.\n\n\nalpha\nfloat\n100.0\nControls the steepness of the inversion. A larger value will make thetransition between the flat areas and border areas steeper in theresulting array.\n\n\nsigma\nfloat\n5.0\nStandard deviation of the Gaussian filter applied over the image.\n\n\nReturns\n(M, N) or (L, M, N) array\n\nPreprocessed image (or volume) suitable formorphological_geodesic_active_contour.\n\n\n\n\nsource\n\n\ncheckerboard_level_set\n\n checkerboard_level_set (image_shape, square_size=5)\n\nCreate a checkerboard level set with binary values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_shape\ntuple of positive integers\n\nShape of the image.\n\n\nsquare_size\nint\n5\nSize of the squares of the checkerboard. It defaults to 5.\n\n\nReturns\narray with shape image_shape\n\nBinary level set of the checkerboard.\n\n\n\n\nsource\n\n\nellipsoid_level_set\n\n ellipsoid_level_set (image_shape, center=None, semi_axis=None)\n\nCreate a ellipsoid level set with binary values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_shape\ntuple of positive integers\n\nShape of the image\n\n\ncenter\nNoneType\nNone\nCoordinates of the center of the ellipsoid.If not given, it defaults to the center of the image.\n\n\nsemi_axis\nNoneType\nNone\nLengths of the semi-axis of the ellispoid.If not given, it defaults to the half of the image dimensions.\n\n\nReturns\narray with shape image_shape\n\nBinary level set of the ellipsoid with the given centerand semi_axis.\n\n\n\n\nsource\n\n\ncircle_level_set\n\n circle_level_set (image_shape, center=None, radius=None)\n\nCreate a circle level set with binary values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_shape\ntuple of positive integers\n\nShape of the image\n\n\ncenter\nNoneType\nNone\nCoordinates of the center of the circle given in (row, column). If notgiven, it defaults to the center of the image.\n\n\nradius\nNoneType\nNone\nRadius of the circle. If not given, it is set to the 75% of thesmallest image dimension.\n\n\nReturns\narray with shape image_shape\n\nBinary level set of the circle with the given radius and center.\n\n\n\n\nsource\n\n\ninf_sup\n\n inf_sup (u)\n\nIS operator.\n\nsource\n\n\nsup_inf\n\n sup_inf (u)\n\nSI operator.",
    "crumbs": [
      "Python library",
      "Morphsnakes segmentation"
    ]
  },
  {
    "objectID": "Python library/06_harmonic_wrapping.html",
    "href": "Python library/06_harmonic_wrapping.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Morph one surface mesh onto another via map to reference shape (disk, sphere). Part of the pipeline for dynamic data.\n\nIn this notebook, we present some additional wrapping algorithms. So far, we have wrapped two meshes by simply projection each source mesh vertex onto the closest position on the target mesh and carrying out some on-surface smoothing. This works but is not necessarily extremely robust, and can lead to very deformed UV maps when there are large, localized deformations.\nHere, we’ll implement an alternative algorithm that works by (a) mapping each mesh into a common reference in a mathematically standardized way -harmonic maps- and then (b) chaining together the map from the source mesh to the common reference and from the reference to the target mesh to get a surface-surface map.\nWe provide algorithms for meshes of disk, cylindrical, and spherical topology (potentially with holes). For arbitrary genus (i.e. handles, like a torus), one can take a look at hyperbolic orbifolds https://github.com/noamaig/hyperbolic_orbifolds/. Not implemented here.\n\n\nLet’s load the test meshes from the fly midgut dataset.\n\nmesh_initial_UV = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/initial_uv.obj\")\nmesh_final_UV = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/final_uv.obj\") # this is a UV map defined for tpt 20\n\nmesh_1 = tcmesh.read_other_formats_without_uv(f\"datasets/movie_example/meshes/mesh_{str(1).zfill(2)}.ply\")\nmesh_2 = tcmesh.read_other_formats_without_uv(f\"datasets/movie_example/meshes/mesh_{str(2).zfill(2)}.ply\")\n\nmesh_60 = tcmesh.read_other_formats_without_uv(f\"datasets/movie_example/meshes/mesh_{str(60).zfill(2)}.ply\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o mesh_01_cylinder_seams_uv\nWarning: readOBJ() ignored non-comment line 3:\n  o mesh_20_uv",
    "crumbs": [
      "Python library",
      "Harmonic mapping"
    ]
  },
  {
    "objectID": "Python library/06_harmonic_wrapping.html#harmonic-mapping",
    "href": "Python library/06_harmonic_wrapping.html#harmonic-mapping",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Morph one surface mesh onto another via map to reference shape (disk, sphere). Part of the pipeline for dynamic data.\n\nIn this notebook, we present some additional wrapping algorithms. So far, we have wrapped two meshes by simply projection each source mesh vertex onto the closest position on the target mesh and carrying out some on-surface smoothing. This works but is not necessarily extremely robust, and can lead to very deformed UV maps when there are large, localized deformations.\nHere, we’ll implement an alternative algorithm that works by (a) mapping each mesh into a common reference in a mathematically standardized way -harmonic maps- and then (b) chaining together the map from the source mesh to the common reference and from the reference to the target mesh to get a surface-surface map.\nWe provide algorithms for meshes of disk, cylindrical, and spherical topology (potentially with holes). For arbitrary genus (i.e. handles, like a torus), one can take a look at hyperbolic orbifolds https://github.com/noamaig/hyperbolic_orbifolds/. Not implemented here.\n\n\nLet’s load the test meshes from the fly midgut dataset.\n\nmesh_initial_UV = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/initial_uv.obj\")\nmesh_final_UV = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/final_uv.obj\") # this is a UV map defined for tpt 20\n\nmesh_1 = tcmesh.read_other_formats_without_uv(f\"datasets/movie_example/meshes/mesh_{str(1).zfill(2)}.ply\")\nmesh_2 = tcmesh.read_other_formats_without_uv(f\"datasets/movie_example/meshes/mesh_{str(2).zfill(2)}.ply\")\n\nmesh_60 = tcmesh.read_other_formats_without_uv(f\"datasets/movie_example/meshes/mesh_{str(60).zfill(2)}.ply\")\n\nWarning: readOBJ() ignored non-comment line 4:\n  o mesh_01_cylinder_seams_uv\nWarning: readOBJ() ignored non-comment line 3:\n  o mesh_20_uv",
    "crumbs": [
      "Python library",
      "Harmonic mapping"
    ]
  },
  {
    "objectID": "Python library/06_harmonic_wrapping.html#disk",
    "href": "Python library/06_harmonic_wrapping.html#disk",
    "title": "blender-tissue-cartography",
    "section": "Disk",
    "text": "Disk\nFollowing https://libigl.github.io/libigl-python-bindings/tut-chapter4/. We first map each mesh to the unit disk using harmonic coordinates. Boundary conditions are such that the boundary loop is mapped to the unit circle isometrically (i.e. relative distances are preserved).\nThere is still a remaining rotation degree of freedom. This is fixed by optimizing the match of the conformal factors (i.e. the area distortion of the maps to the disk) using phase correlation.\n\nsource\n\nmap_to_disk\n\n map_to_disk (mesh, bnd=None, set_uvs=False)\n\n*Map mesh to unit disk by computing harmonic UV coordinates.\nThe longest boundary loop of the mesh is mapped to the unit circle. Follows https://libigl.github.io/libigl-python-bindings/tut-chapter4/. Note: the disk is centered at (1/2, 1/2).\nThe disk rotation angle is arbitrary*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\nbnd\nNoneType\nNone\nBoundary to map to the unit circle. If None, computed automatically.\n\n\nset_uvs\nbool\nFalse\nwhether to set the disk coordinates as UV coordinates of the mesh.\n\n\nReturns\nnp.array\n\n2d vertex coordinates mapping the mesh to the unit disk in [0,1]^1\n\n\n\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/plane_example.obj\")\n\ndisk_coordinates = map_to_disk(mesh, set_uvs=True)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Grid\n\n\n\nplt.triplot(*disk_coordinates.T, mesh.tris)\nplt.axis(\"equal\")\n\n(-0.049841709467476805,\n 1.049992462355594,\n -0.04994346724076043,\n 1.0496419841918154)\n\n\n\n\n\n\n\n\n\n\nigl.flipped_triangles(disk_coordinates, mesh.tris)\n\narray([], shape=(0, 0), dtype=int64)\n\n\n\nRotational alignment\nThe disk can still be rotated freely. Use method from the Moebius registration paper:\n“We next seek the rotation that best aligns the two parameterizations. To do so, we first sample the conformal factors of each mesh onto a regular spherical grid (Figure 6, top right). We then find the rotation that maximizes the correlation between conformal factors, via a fast spectral transform.”\n\nsource\n\n\n\nget_rot_mat2d\n\n get_rot_mat2d (phi)\n\nGet 2d rotation matrix with angle phi.\n\nmesh_a = deepcopy(mesh)\nmesh_b = deepcopy(mesh)\n\nphi = -0.8*np.pi\nmesh_b.texture_vertices = ((mesh_a.texture_vertices-np.array([0.5, 0.5]))@get_rot_mat2d(phi)+np.array([0.5, 0.5]))\n\n# test orientation flip\nflip = np.diag([-1,1])\nmesh_b.texture_vertices = ((mesh_b.texture_vertices-np.array([0.5, 0.5]))@flip+np.array([0.5, 0.5]))\nmesh_b.faces = [fc[::-1] for fc in mesh_b.faces]\n\n\nplt.triplot(*mesh_a.texture_vertices.T, mesh_a.texture_tris)\n\nplt.triplot(*mesh_b.texture_vertices.T, mesh_b.texture_tris)\nplt.axis(\"equal\")\n\n(-0.049841709467476805,\n 1.049992462355594,\n -0.049963925863437814,\n 1.0499208937180065)\n\n\n\n\n\n\n\n\n\nTo copy over attributes -like vertex 3d position- from one mesh to another, given a common parametrization, we can use barycentric interpolation.\nWe compute the remaining degree of freedom - rotations of the unit disk - by aligning the conformal factors of the map to the disk.\n\nmesh_source, mesh_target = (mesh_a, mesh_b)\nq = 0.01\nn_grid = 1024 \nallow_flip = True\n\ndisk_uv_source = np.copy(mesh_source.texture_vertices)\ndisk_tris_source = mesh_source.texture_tris\ndisk_uv_target = np.copy(mesh_target.texture_vertices)\ndisk_tris_target = mesh_target.texture_tris\n\n# compute conformal factors and clip outliers\nconformal_factor_source = tcdfg.compute_per_vertex_area_distortion(disk_uv_source, disk_tris_source,\n                                                                   mesh_source.vertices, mesh_source.tris)\nconformal_factor_source = np.clip(conformal_factor_source, np.quantile(conformal_factor_source, q),\n                                  np.quantile(conformal_factor_source, 1-q))\nconformal_factor_target = tcdfg.compute_per_vertex_area_distortion(disk_uv_target, disk_tris_target,\n                                                                   mesh_target.vertices, mesh_target.tris)\nconformal_factor_target = np.clip(conformal_factor_target, np.quantile(conformal_factor_target, q),\n                                  np.quantile(conformal_factor_target, 1-q))\n\n# interpolate into UV square\nu, v = 2*[np.linspace(0, 1, n_grid),]\nUV = np.stack(np.meshgrid(u, v), axis=-1).reshape((-1, 2))\ninterpolated_source = tcinterp.interpolate_barycentric(UV, disk_uv_source, disk_tris_source,\n                                                       conformal_factor_source, distance_threshold=np.inf)\ninterpolated_source = interpolated_source.reshape((n_grid, n_grid))[::-1]\n\ninterpolated_target = tcinterp.interpolate_barycentric(UV, disk_uv_target, disk_tris_target,\n                                                       conformal_factor_target, distance_threshold=np.inf)\ninterpolated_target = interpolated_target.reshape((n_grid, n_grid))[::-1]\n\n# compute rotational alignment\ninterpolated_source_polar = transform.warp_polar(interpolated_source, radius=n_grid/2-1)\ninterpolated_target_polar = transform.warp_polar(interpolated_target, radius=n_grid/2-1)\nshifts, error, _ = registration.phase_cross_correlation(interpolated_source_polar, interpolated_target_polar,\n                                                        normalization=None)\nif allow_flip:\n    interpolated_source_polar_flipped = interpolated_source_polar[::-1]\n    shifts_flipped, error_flipped, _ = registration.phase_cross_correlation(interpolated_source_polar_flipped,\n                                                    interpolated_target_polar,\n                                                    normalization=None)\n    if error &gt; error_flipped:\n        rot_angle = shifts_flipped[0]*np.pi/180\n        rot_mat = -get_rot_mat2d(rot_angle)\n        new_texture_vertices = (disk_uv_source-np.array([0.5,0.5]))@rot_mat.T\n        new_texture_vertices += np.array([0.5,0.5])\n    \nrot_angle = shifts[0]*np.pi/180\nrot_mat = get_rot_mat2d(rot_angle)\nnew_texture_vertices = (disk_uv_source-np.array([0.5,0.5]))@rot_mat.T\nnew_texture_vertices += np.array([0.5,0.5])\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(figsize=(10,5), ncols=3)\nax1.imshow(interpolated_source_polar)\nax2.imshow(interpolated_source_polar_flipped)\nax3.imshow(interpolated_target_polar)\n\nprint()\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nrotational_align_disk\n\n rotational_align_disk (mesh_source, mesh_target, disk_uv_source=None,\n                        disk_uv_target=None, allow_flip=True, q=0.01,\n                        n_grid=1024)\n\n*Rotationally align two UV maps to the disk by the conformal factor.\nComputes aligned UV coordinates. Assumes that the UV coordinates are in [0,1]^2. This works by computing the conformal factor (how much triangle size changes as it is mapped to the plane, and finding the optimal rotation to align the conformal factors via phase correlation.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh_source\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\nmesh_target\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\ndisk_uv_source\nNoneType\nNone\nDisk coordinates for each vertex in the source mesh. Optional.If None, the UV coordinates of mesh_source are used.\n\n\ndisk_uv_target\nNoneType\nNone\nDisk coordinates for each vertex in the target mesh. Optional.If None, the UV coordinates of disk_uv_target are used.\n\n\nallow_flip\nbool\nTrue\nWhether to allow flips (improper rotations). If a flipoccurs, np.linalg.det(rot_mat) &lt; 0\n\n\nq\nfloat\n0.01\nConformal factors are clipped at this quantile to avoid outliers.\n\n\nn_grid\nint\n1024\nGrid for interpolation of conformal factor during alignment\n\n\nReturns\nnp.array, np.array, float\n\nnew_texture_vertices_mesh_source : np.array Rotationally aligned texture verticesrot_mat : np.array of shape (2,2) Rotation matrixoverlap : float Overlap between aligned conformal factors. 1 = perfect alignment.\n\n\n\n\nmesh_a = deepcopy(mesh)\nmesh_b = deepcopy(mesh)\n\nphi = -0.8*np.pi\nmesh_b.texture_vertices = ((mesh_a.texture_vertices-np.array([0.5, 0.5]))@get_rot_mat2d(phi)+np.array([0.5, 0.5]))\n\nflip = np.diag([-1,1])\nmesh_b.texture_vertices = ((mesh_b.texture_vertices-np.array([0.5, 0.5]))@flip+np.array([0.5, 0.5]))\nmesh_b.faces = [fc[::-1] for fc in mesh_b.faces]\n\n\nnew_vertices, rot_mat, overlap = rotational_align_disk(mesh_a, mesh_b, allow_flip=True)\n\noverlap\n\n0.9995530944863507\n\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(figsize=(10,5), ncols=3)\nax1.triplot(*mesh_a.texture_vertices.T, mesh_a.texture_tris)\nax2.triplot(*mesh_b.texture_vertices.T, mesh_b.texture_tris)\nax3.triplot(*new_vertices.T, mesh_b.texture_tris)\nax3.triplot(*mesh_b.texture_vertices.T, mesh_b.texture_tris)\n\n\nfor ax in [ax1, ax2, ax3]:\n    ax.axis(\"equal\")\n\n\n\n\n\n\n\n\n\n\nMesh registration\n\nsource\n\n\nwrap_coords_via_disk\n\n wrap_coords_via_disk (mesh_source, mesh_target, disk_uv_source=None,\n                       disk_uv_target=None, align=True, q=0.01,\n                       n_grid=1024)\n\n*Map 3d coordinates of source mesh to target mesh via a disk parametrization.\nDisk parametrization can be provided or computed on the fly via harmonic coordinates. If desired, the two disks are also rotationally aligned.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh_source\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\nmesh_target\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\ndisk_uv_source\nNoneType\nNone\nDisk coordinates for each vertex in the source mesh. Optional.If None, computed via map_to_disk.\n\n\ndisk_uv_target\nNoneType\nNone\nDisk coordinates for each vertex in target mesh. Optional.If None, computed via map_to_disk.\n\n\nalign\nbool\nTrue\nWhether to rotationally align the parametrizations. If False, they are used as-is.\n\n\nq\nfloat\n0.01\nConformal factors are clipped at this quantile to avoid outliers.\n\n\nn_grid\nint\n1024\nGrid for interpolation of conformal factor during alignment.Higher values increase alignment precision.\n\n\nReturns\nnp.array\n\nNew 3d vertex coordinates for mesh_source, lying on the surfacedefined by mesh_target\n\n\n\n\nmesh_source = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/plane_example.obj\")\nmesh_target = deepcopy(mesh_source)\n\nrandom_rot = stats.special_ortho_group.rvs(3)\nmesh_target.vertices = mesh_target.vertices@random_rot + np.array([2, 9, 1])\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Grid\n\n\n\nmesh_source = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/plane_example.obj\")\nmesh_target = deepcopy(mesh_source)\n\nrandom_rot = stats.special_ortho_group.rvs(3)\nmesh_target.vertices = mesh_target.vertices@random_rot + np.array([2, 9, 1])\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Grid\n\n\n\nnew_coords, overlap = wrap_coords_via_disk(mesh_source, mesh_target)\n\n\nnp.linalg.norm(new_coords-mesh_target.vertices, axis=-1).mean()\n\n1.3403645763230948e-15",
    "crumbs": [
      "Python library",
      "Harmonic mapping"
    ]
  },
  {
    "objectID": "Python library/06_harmonic_wrapping.html#cylinder",
    "href": "Python library/06_harmonic_wrapping.html#cylinder",
    "title": "blender-tissue-cartography",
    "section": "Cylinder",
    "text": "Cylinder\nMap a cylinder to the disk: fill one boundary with an extra vertex, map to the disk harmonically, use Moebius to move the extra vertex to the disk center.\n\nsource\n\nmoebius_disk\n\n moebius_disk (pts, b)\n\nCompute a Moebius transformation of the disk. Moves disk origin by b. pts.shape is (…, 2)\n\nsource\n\n\ncomplex_to_xy\n\n complex_to_xy (arr)\n\nMap x+iy to (x, y). Return shape==(…,2 )\n\nsource\n\n\nxy_to_complex\n\n xy_to_complex (arr)\n\nMap (x,y) to x+iy. arr.shape==(…,2 )\n\nsource\n\n\npolygon_centroid\n\n polygon_centroid (pts)\n\nSee https://en.wikipedia.org/wiki/Centroid. pts.shape is (…, 2)\n\nsource\n\n\npolygon_area\n\n polygon_area (pts)\n\nPolygon area via shoe-lace formula. Assuming no self-intersection. pts.shape is (…, 2)\n\n#mesh = tcmesh.ObjMesh.read_obj(\"movie_example/cylinder.obj\") # cylinder_clean\n\n#mesh = tcmesh.ObjMesh.read_obj(\"movie_example/mesh_1_cylinder_cut.obj\") # cylinder_clean\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/mesh_1_cylinder_cut_non_centered.obj\") # cylinder_clean\n\nWarning: readOBJ() ignored non-comment line 3:\n  o mesh_01\n\n\n\n# determine the boundary \nfirst_boundary = igl.boundary_loop(mesh.tris)\nall_boundary_edges = igl.boundary_facets(mesh.tris)\nsecond_boundary = igl.edges_to_path(np.stack([e for e in all_boundary_edges if not e[0] in first_boundary]))[0][:-1]\n\n\n# add an extra vertex and triangles\nfilled_tris = igl.topological_hole_fill(mesh.tris, [second_boundary])\ncenter_second_boundary = mesh.vertices[second_boundary].mean(axis=0)\nfilled_vertices = np.vstack([mesh.vertices, [center_second_boundary]])\n\n\nfilled_vertices.shape[0], mesh.vertices.shape[0], filled_tris.shape[0]-mesh.tris.shape[0], second_boundary.shape[0]\n\n(20002, 20001, 16, 16)\n\n\n\n# check - looks good\nmesh_filled = tcmesh.ObjMesh(vertices=filled_vertices, faces=filled_tris)\nmesh_filled.write_obj(\"datasets/movie_example/mesh_1_cylinder_cut_non_centered_filled.obj\") # cylinder_clean\n\n\n# map to disk via harmonic map\n\nbnd_uv = 1 * igl.map_vertices_to_circle(filled_vertices, first_boundary)\nuv = igl.harmonic(filled_vertices, filled_tris, first_boundary, bnd_uv, 1)\n# map the center of the second boundary to disk center\nuv = moebius_disk(uv, -uv[-1])\n\n\nuv[-1] # last point -at the center of filled cylinder opening- is mapped to center of\n\narray([0., 0.])\n\n\n\nplt.triplot(*uv.T, filled_tris, lw=0.25)\nplt.axis(\"equal\")\n\n(-1.096551335368751,\n 1.0954288401027532,\n -1.0993918966333553,\n 1.0999552780113782)\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nmap_cylinder_to_disk\n\n map_cylinder_to_disk (mesh, outer_boundary='longest',\n                       first_boundary=None, second_boundary=None,\n                       set_uvs=False, return_filled=False)\n\n*Map cylinder mesh to unit disk by computing harmonic UV coordinates.\nOne boundary loop of the mesh is mapped to the circle with a diameter 1/2. The second boundary is filled by adding an extra vertex at its center, which is mapped to the disk center.\nWhich of the two circular boundaries is mapped to the center resp. the outer circle is set by the option outer_boundary.\nThe disk rotation angle is arbitrary.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\nouter_boundary\nstr\nlongest\nBoundary to map to the unit circle. If “longest”/“shortest”, the longer/shorter one is mapped to the unit circle. If int,the boundary containing the vertex defined by the int is used.\n\n\nfirst_boundary\nNoneType\nNone\nFirst boundary loop of cylinder. If None, computed automatically.\n\n\nsecond_boundary\nNoneType\nNone\nSecond boundary loop of cylinder. If None, computed automatically.\n\n\nset_uvs\nbool\nFalse\nwhether to set the disk coordinates as UV coordinates of the mesh.\n\n\nreturn_filled\nbool\nFalse\nWhether to return vertices and faces with filled hole.\n\n\nReturns\nnp.array, np.array, np.array\n\nuv : np.array 2d vertex coordinates mapping the mesh to the unit disk in [0,1]^1. If filled_vertices is True, the last entry is the coordinate of the added point.filled_vertices : np.array 3d vertices with extra vertex to fill second boundaryfilled_faces : np.array Faces with added faces to fill the second boundary.\n\n\n\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/cylinder.obj\") # cylinder_clean\n\n\nuv = map_cylinder_to_disk(mesh, outer_boundary=\"longest\")\n\n\nigl.flipped_triangles(uv, mesh.tris)\n\narray([], shape=(0, 0), dtype=int64)\n\n\n\nplt.triplot(*uv.T, mesh.tris, lw=0.5)\nplt.axis(\"equal\")\n\n(-0.049183316665476866,\n 1.049654650031695,\n -0.04980286502601551,\n 1.0499106507897504)\n\n\n\n\n\n\n\n\n\n\n## let's try a more challenging example - midgut mesh with two cuts to make it a cylinder\n\nmesh = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/mesh_1_cylinder_cut.obj\") # cylinder_clean\n\nWarning: readOBJ() ignored non-comment line 3:\n  o mesh_01\n\n\n\nuv = map_cylinder_to_disk(mesh, outer_boundary=\"longest\",)\nuv_reverse = map_cylinder_to_disk(mesh, outer_boundary=\"shortest\", )\n\n\nplt.triplot(*uv.T, mesh.tris, lw=0.5)\nplt.axis(\"equal\")\n\n(-0.04881346336073662,\n 1.0499255590960166,\n -0.049218476987450965,\n 1.0493085140972238)\n\n\n\n\n\n\n\n\n\n\nplt.triplot(*uv_reverse.T, mesh.tris, lw=0.5)\nplt.axis(\"equal\")\n\n(-0.04834689694663298,\n 1.047823750534748,\n -0.04965982618548542,\n 1.0499708292802348)\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nwrap_coords_via_disk_cylinder\n\n wrap_coords_via_disk_cylinder (mesh_source, mesh_target, q=0.01,\n                                n_grid=1024)\n\n*Map 3d coordinates of source mesh to target mesh via an annulus parametrization.\nAnnulus parametrization can be provided or computed on the fly via harmonic coordinates. If desired, the two disks are also rotationally aligned. Meshes must be cylindrical. Two choices exist for mapping a cylinder to the plane (depending on which boundary circle is mapped to the disk boundary resp. center). Both options are tried, and the one leading to better alignment is used.\nIf you already have a map to the disk, use wrap_coords_via_disk instead.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh_source\ntcmesh.ObjMesh\n\nMesh. Must be topologically a cylinder and should be triangular.\n\n\nmesh_target\ntcmesh.ObjMesh\n\nMesh. Must be topologically a cylinder and should be triangular.\n\n\nq\nfloat\n0.01\nConformal factors are clipped at this quantile to avoid outliers.\n\n\nn_grid\nint\n1024\nGrid for interpolation of conformal factor during alignment.Higher values increase alignment precision.\n\n\nReturns\nnp.array, float\n\nnew_coords : np.array New 3d vertex coordinates for mesh_source, lying on the surface defined by mesh_targetoverlap : float Measure of geometry overlap. 1 = perfect alignment\n\n\n\n\nmesh_source = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/mesh_1_cylinder_cut.obj\") # cylinder_clean\nmesh_target = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/mesh_1_cylinder_cut_non_centered.obj\") # cylinder_clean\n\nWarning: readOBJ() ignored non-comment line 3:\n  o mesh_01\nWarning: readOBJ() ignored non-comment line 3:\n  o mesh_01\n\n\n\nnew_coords, overlap = wrap_coords_via_disk_cylinder(mesh_source, mesh_target,\n                                                    q=0.01, n_grid=1024)\n\n\nmesh_wraped = tcmesh.ObjMesh(vertices=new_coords, faces=mesh_source.faces, texture_vertices=None)\nmesh_wraped.write_obj(\"datasets/movie_example/mesh_1_cylinder_cut_wrapepd.obj\")",
    "crumbs": [
      "Python library",
      "Harmonic mapping"
    ]
  },
  {
    "objectID": "Python library/06_harmonic_wrapping.html#sphere",
    "href": "Python library/06_harmonic_wrapping.html#sphere",
    "title": "blender-tissue-cartography",
    "section": "Sphere",
    "text": "Sphere\nFor the sphere, we follow https://www.cs.cmu.edu/~kmcrane/Projects/MobiusRegistration/paper.pdf. The Riemann mapping theorem guarantees there is a conformal map of our surface to the unit sphere. This map is unique up to Moebius transformations (inversion about a point in the unit ball, and rotations). We fix the inversions by choosing the conformal map with the least amount of area distortion, and the rotation by registering the conformal factors as functions on the unit sphere using spherical harmonics\n\nCut sphere to disk by removing a single vertex\nMap disk to plane conformally using harmonic coordinates. Turns out the least-squares conformal map is lousy at preserving angles (at least in igl)\nMap disk to a sphere using stereographic projection, adding the removed vertex at the north pole\nFix Moebius inversion by choosing a conformal map with minimal distortion using Algorithm 1 from https://www.cs.cmu.edu/~kmcrane/Projects/MobiusRegistration/paper.pdf\nRotational registration using spherical harmonics (see notebook 03c)\nInterpolation of vertex positions using spherical parametrization\n\n\nmesh = deepcopy(mesh_final_UV)\n\n\n# this is what the map of the mesh sans north pole to the plane looks like\n\nfig = plt.figure(figsize=(4,4))\n\nplt.triplot(*uv.T, faces_disk, lw=0.5)\n\n\n\n\n\n\n\n\n\n\n\narray([[ 3.79091394e-08, -3.37472892e-08],\n       [ 1.14168121e-07, -8.80032609e-08],\n       [ 3.26394385e-08, -2.57627128e-08],\n       ...,\n       [ 4.22076492e-06, -5.00112042e-06],\n       [ 4.04931220e-05, -5.08178908e-06],\n       [ 3.67645301e-05,  5.33854828e-05]])\n\n\n\nnp.diag(average_op.toarray())\n\narray([0., 0., 0., ..., 0., 0., 0.])\n\n\n\nStrereographic projection\n\nsource\n\n\nstereographic_sphere_to_plane\n\n stereographic_sphere_to_plane (pts)\n\n*Stereographic projection from unit sphere to plane from the north pole (0,0,1).\nSee https://en.wikipedia.org/wiki/Stereographic_projection. Convention: the plane is at z=0, unit sphere centered at the origin. pts should be an array of shape (…, 3)*\n\nsource\n\n\nstereographic_plane_to_sphere\n\n stereographic_plane_to_sphere (uv)\n\n*Stereographic projection from plane to the unit sphere from the north pole (0,0,1).\nSee https://en.wikipedia.org/wiki/Stereographic_projection. Convention: plane is at z=0, unit sphere centered at origin. uv should be an array of shape (…, 2)*\n\n(np.allclose(np.linalg.norm(stereographic_plane_to_sphere(uv), axis=1), 1),\n np.allclose(stereographic_sphere_to_plane(stereographic_plane_to_sphere(uv)), uv))\n\n(True, True)\n\n\n\n\nMoebius centering algorithm\nAlgorithm 1 from https://www.cs.cmu.edu/~kmcrane/Projects/MobiusRegistration/paper.pdf\n\nsource\n\n\ncenter_moebius\n\n center_moebius (vertices_3d, vertices_sphere, tris, n_iter_centering=10,\n                 alpha=0.5)\n\n*Apply Moeboius inversions to minimize area distortion of a map from mesh to sphere.\nImplementation of Algorithm 1 from: https://www.cs.cmu.edu/~kmcrane/Projects/MobiusRegistration/paper.pdf*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvertices_3d\nnp.array of shape (n_verts, 3)\n\n3d mesh vertices\n\n\nvertices_sphere\nnp.array of shape (n_verts, 3)\n\nInitial vertex positions on unit sphere\n\n\ntris\nnp.array of shape (n_faces, 3) and type int\n\nFaces of a triangular mesh\n\n\nn_iter_centering\nint\n10\nCentering algorithm iterations.\n\n\nalpha\nfloat\n0.5\nLearning rate. Lower values make the algorithm more stable\n\n\nReturns\nnp.array, float\n\nvertices_sphere_centered : np.array of shape (n_verts, 3) Centered sphere coordinatescom_norm : float Distance of sphere vertex center of mass from origin. Low values indicate convergence of the algorithm.\n\n\n\n\n\nPutting it all together …\n\nsource\n\n\nmap_to_sphere\n\n map_to_sphere (mesh, method='harmonic', R_max=100, n_iter_centering=20,\n                alpha=0.5, set_uvs=False)\n\n*Compute a map of mesh to the unit sphere.\nFirst, remove one vertex (the last one), and map the resulting disk-topology mesh to the plane using least squares conformal maps. Then map the plane to the sphere using stereographic projection.\nThe conformal map is chosen so that area distortion is as small as possible by (a) optimizing over the scale (“radius”) of the map to the disk and (b) “centering” the map using Algorithm 1 from cs.cmu.edu/~kmcrane/Projects/MobiusRegistration/paper.pdf.\nThis means the map is canonical up to rotations of the sphere.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\ntcmesh.ObjMesh\n\nMesh. Must be topologically a sphere, and should be triangular.\n\n\nmethod\nstr\nharmonic\nMethod for computing the map from mesh without the north pole to the plane.Recommended: harmonic.\n\n\nR_max\nint\n100\nMaximum radius to consider when computing inverse stereographicprojection. If you get weird results, try a lower value.\n\n\nn_iter_centering\nint\n20\nCentering algorithm iterations. If 0, no centering is performed\n\n\nalpha\nfloat\n0.5\nLearning rate. Lower values make the centering algorithm more stable\n\n\nset_uvs\nbool\nFalse\n\n\n\n\n\nmesh = deepcopy(mesh_final_UV)\ncoords_sphere = map_to_sphere(mesh, n_iter_centering=10, method=\"harmonic\")\ntcmesh.ObjMesh(vertices=coords_sphere, faces=mesh.tris).write_obj(\"datasets/movie_example/map_to_sphere.obj\")\n\n\nangles_3d = igl.internal_angles(mesh.vertices, mesh.tris)\nangles_sphere = igl.internal_angles(coords_sphere, mesh.tris)\n\nnp.nanmean(np.abs(angles_sphere.flatten()-angles_3d.flatten())) # map to sphere is close to conformal/angle-preserving\n\n\n\nRotation alignment using rotation module\nFor a map to the sphere, we can compute the amount of area distortion at each point on the sphere. We can then use this “conformal factor” as a signal to rotationally align two maps (from different meshes) to the sphere.\n\n# this is what the conformal factor (area distortion) looks like plotted vs phi, theta, for an example\n# (the same mesh mapped to the sphere, but arbitrarily rotated)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n\nax1.scatter(phi_source, theta_source, c=signal_source, s=1, vmin=-3, vmax=3)\nax2.scatter(phi_target, theta_target, c=signal_target, s=1, vmin=-3, vmax=3)\n\nax1.axis(\"equal\");\nax2.axis(\"equal\");\n\n\n\n\n\n\n\n\n\nsource\n\n\nrotational_align_sphere\n\n rotational_align_sphere (mesh_source, mesh_target, coords_sphere_source,\n                          coords_sphere_target, allow_flip=False,\n                          max_l=10, n_angle=100, n_subdiv_axes=1,\n                          maxfev=100)\n\n*Rotationally align two UV maps to the sphere by the conformal factor.\nComputes aligned spherical coordinates. Rotational alignment works by computing the conformal factor (how much triangle size changes as it is mapped to the sphere) and optimizing over rotations to find the one which leads to the best alignment. This works via an expansion in spherical harmonics. See tcrot.rotational_alignment*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh_source\ntcmesh.ObjMesh\n\nMesh. Must be topologically a sphere (potentially with holes),and should be triangular.\n\n\nmesh_target\ntcmesh.ObjMesh\n\nMesh. Must be topologically a sphere (potentially with holes),and should be triangular.\n\n\ncoords_sphere_source\nnp.array or None\n\nSphere coordinates for each vertex in the source mesh.If None, the UV coordinates are interpreted as angles 2piu=phi,2piv=theta.\n\n\ncoords_sphere_target\nnp.array or None\n\nSphere coordinates for each vertex in the source mesh.If None, the UV coordinates are interpreted as angles 2piu=phi,2piv=theta.\n\n\nallow_flip\nbool\nFalse\nWhether to allow improper rotations with determinant -1.\n\n\nmax_l\nint\n10\nMaximum angular momentum. If None, the maximum value available in the inputspherical harmonics is used.\n\n\nn_angle\nint\n100\nNumber of trial rotation angles [0,…, 2*pi]\n\n\nn_subdiv_axes\nint\n1\nControls the number of trial rotation axes. Rotation axes are vertices ofthe icosphere which can be subdivided. There will be roughly40*4**n_subdiv_axes trial axes. This parameter has the strongest influenceon the run time.\n\n\nmaxfev\nint\n100\nNumber of function evaluations during fine optimization.\n\n\nReturns\nnp.array, np.array, float\n\ncoords_sphere_source_rotated : np.array Rotationally aligned sphere verticesrot_mat : np.array of shape (3,3) Rotation matrixoverlap : float How well the conformal factors overlap. 1 = perfect overlap.\n\n\n\n\n# first test case: same mesh, but map to the sphere rotatated\n\nmesh_source = deepcopy(mesh_final_UV)\nmesh_target = deepcopy(mesh_final_UV)\n\ncoords_sphere_source = map_to_sphere(mesh_source)\nrot_mat = stats.special_ortho_group.rvs(3)\ncoords_sphere_target = coords_sphere_source @ rot_mat.T\n\n\ntransformed, R, overlap = rotational_align_sphere(mesh_source, mesh_target,\n                                                  coords_sphere_source, coords_sphere_target,\n                                                  allow_flip=False, max_l=10, n_angle=100, n_subdiv_axes=1,\n                                                  maxfev=100)\n\nnp.linalg.norm(R-rot_mat), overlap\n\n(0.0004874609816821069, 0.9999997693026267)\n\n\n\n# second test case: meshes for two frames of a movie\n\nmesh_source = deepcopy(mesh_2) # mesh_final_UV\nmesh_target = deepcopy(mesh_1) # mesh_initial_UV\n\ncoords_sphere_source = map_to_sphere(mesh_source)\ncoords_sphere_target = map_to_sphere(mesh_target)\n\n\ntransformed, R, overlap = rotational_align_sphere(mesh_source, mesh_target,\n                                                  coords_sphere_source, coords_sphere_target,\n                                                  allow_flip=False, max_l=10, n_angle=100, n_subdiv_axes=1,\n                                                  maxfev=100)\noverlap\n\n0.9908056596783977\n\n\n\n\nInterpolation\nNow that we have (a) mapped both meshes to the sphere, (b) minized distortion via Moebius centering, and (c) rotationally aligned the two maps via spherical harmonics, we can interpolate the coordinates of mesh B onto mesh A.\n\nsource\n\n\nwrap_coords_via_sphere\n\n wrap_coords_via_sphere (mesh_source, mesh_target,\n                         coords_sphere_source=None,\n                         coords_sphere_target=None, method='harmonic',\n                         n_iter_centering=10, alpha=0.5, align=True,\n                         allow_flip=False, max_l=10, n_angle=100,\n                         n_subdiv_axes=1, maxfev=100)\n\n*Map 3d coordinates of source mesh to target mesh via a sphere parametrization.\nSphere parametrizations can be provided or computed on the fly using the least-area distorting conformal map to the sphere (see map_to_sphere). If desired, the two parametrizations are also aligned with respect to 3d rotations using mesh shape, using spherical harmonics. See rotational_align_sphere for details.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh_source\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\nmesh_target\ntcmesh.ObjMesh\n\nMesh. Must be topologically a disk (potentially with holes),and should be triangular.\n\n\ncoords_sphere_source\nNoneType\nNone\nSphere coordinates for each vertex in the source mesh. Optional.If None, computed via map_to_sphere.\n\n\ncoords_sphere_target\nNoneType\nNone\nSphere coordinates for each vertex in the target mesh. Optional.If None, computed via map_to_sphere.\n\n\nmethod\nstr\nharmonic\nMethod for comuting the map from sphere without north pole to plane\n\n\nn_iter_centering\nint\n10\nCentering algorithm iterations for computing map to the sphere. If 0, no centering is performed\n\n\nalpha\nfloat\n0.5\nLearning rate for computing map to the sphere. Lower values make the centering algorithm more stable.\n\n\nalign\nbool\nTrue\nWhether to rotationally align the parametrizations. If False, they are used as-is.\n\n\nallow_flip\nbool\nFalse\nWhether to allow improper rotations with determinant -1 for rotational alignment.\n\n\nmax_l\nint\n10\nMaximum angular momentum. If None, the maximum value available in the inputspherical harmonics is used.\n\n\nn_angle\nint\n100\nNumber of trial rotation angles [0,…, 2*pi]\n\n\nn_subdiv_axes\nint\n1\nControls the number of trial rotation axes. Rotation axes are vertices ofthe icosphere which can be subdivided. There will be roughly40*4**n_subdiv_axes trial axes. This parameter has the strongest influenceon the run time.\n\n\nmaxfev\nint\n100\nNumber of function evaluations during fine optimization for rotational alignment.\n\n\nReturns\nnp.array, float\n\nnew_coords : np.array New 3d vertex coordinates for mesh_source, lying on the surface defined by mesh_targetoverlap : np.array Overlap of conformal factor (area distortion) on sphere of the two meshes. Only returned if align is True. 1 indicates perfect overlap.\n\n\n\n\n#mesh_source = deepcopy(mesh_1)\n#mesh_target = deepcopy(mesh_1)\n\n#mesh_source = deepcopy(mesh_1)\n#mesh_target = deepcopy(mesh_2)\n\nmesh_source = deepcopy(mesh_final_UV)\nmesh_target = deepcopy(mesh_initial_UV)\n\n\nmesh_source.vertices[-1], mesh_target.vertices[-1]\n\n(array([ 398.323151,  570.099976, -977.260376]),\n array([457.797119, 947.645935, 547.180786]))\n\n\n\nnew_coords, overlap  = wrap_coords_via_sphere(mesh_source, mesh_target,\n                                              coords_sphere_source=None, coords_sphere_target=None,\n                                              n_iter_centering=20, alpha=0.5, align=True,\n                                              method=\"harmonic\",\n                                              allow_flip=True, max_l=10, n_angle=100, n_subdiv_axes=1, maxfev=200)\n\n\noverlap # mesh_final_UV, mesh_initial_UV 0.918 with harmonic map, 0.919 harmonic-free-boundary\n\n0.9181743310396068\n\n\n\nmesh_wrapped = deepcopy(mesh_source)\nmesh_wrapped.vertices = new_coords\n\n\nmesh_source.write_obj(\"datasets/movie_example/source_moebius.obj\")\nmesh_target.write_obj(\"datasets/movie_example/target_moebius.obj\")\nmesh_wrapped.write_obj(\"datasets/movie_example/wrapped_moebius.obj\")",
    "crumbs": [
      "Python library",
      "Harmonic mapping"
    ]
  },
  {
    "objectID": "Python library/06_harmonic_wrapping.html#stuff-that-did-not-work-out-well",
    "href": "Python library/06_harmonic_wrapping.html#stuff-that-did-not-work-out-well",
    "title": "blender-tissue-cartography",
    "section": "Stuff that did not work out well",
    "text": "Stuff that did not work out well\n\nNon-rigid ICP\nTry it out first using trimesh, then do myself to avoid dependency. Use trimesh.registration.nricp_sumner\n-&gt; No good, takes forever and my laptop runs out of memory. Also lots of parameters to tune whose meaning I don’t know.\n\n\nLaplace Beltrami descriptors\nIdea - compute the eigenfunctions \\(\\phi_i\\) of the Laplace operator \\(\\Delta\\) on the surface, and the embed each point \\(p\\) on the surface as \\(p\\mapsto \\phi_i(p)/\\sqrt{\\lambda_i}\\).\nNot suitable - the spectral shapes of the midgut example mesh become extremely degenerate.\nSee https://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Rustamov07.pdf, https://web.archive.org/web/20100626223753id_/http://www.cs.jhu.edu/~misha/Fall07/Notes/Rustamov07.pdf\n\n\nFunctional mapping\n[https://people.csail.mit.edu/jsolomon/assets/fmaps.pdf] - uses Laplace Beltrami descriptors. There appears to be an existing implementation in python. Let’s try out their example notebook\nIt does not work that well. Ok for matching from mapping timepoints 1-&gt;2, bad for 1-&gt;20, or 20-&gt;30. I get “patchy correspondences”: the map mesh A -&gt; mesh B is not continuous and tears up mesh A into multiple patches that are stitched together in somewhat random order on mesh B. Also kinda slow.\nAs Dillon pointed out, shape descriptors of Laplace-Beltrami kind fundamentally use metric information. Laplace Beltrami is equivalent to a metric. Good for isometric deformations, like joint movements, bad for non-isometric, which is often the case for us.\nSo in principle, this is a nice method, but not quite what we need.",
    "crumbs": [
      "Python library",
      "Harmonic mapping"
    ]
  },
  {
    "objectID": "Python library/registration.html",
    "href": "Python library/registration.html",
    "title": "Rigid-body registration",
    "section": "",
    "text": "In notebook 2 we saw how to do tissue cartography on a single volumetric image. But often, we have multiple images of very similarly shaped objects - either the successive frames of a movie or multiple recordings of biological structures with very consistent shapes, like the Drosophila egg. We want to use “the same” UV map/cartographic projection for all of the images - both so that we don’t need to redo the work of creating the UV map, and so that positions in our cartographic projections always correspond to the same anatomical position on the imaged object (e.g. the anterior side of the Drosophila embryo is always left, and the dorsal midline corresponds to the line \\(v=1/2\\) in UV space).\nTo do this, we use mesh registration. The idea is that we have a reference mesh - for example, from the first frame of a movie - on which a UV map is defined. We then move and deform this mesh so that it fits our target mesh - which describes the surface we want to extract from the volumetric data - as well as possible. The registered reference mesh now fits the volumetric data but still carries the UV map, and can now be used to create a cartographic projection.\nIf you have a consistently shaped object that you know you will image many times - in the Streichan lab, the early Drosophila embryo, of which we have hundreds of in toto recordings - it might make sense to make an idealized “prototypical” mesh with a nice mesh and UV map, that you can use as a reference.\nMesh/point cloud registration and deformation are very well-studied problems and we can make use of many robust and already-implemented algorithms. We proceed in two steps: 1. Affine registration: Align reference mesh to target mesh using translations, rotations, and possible rescaling, for instance by Iterative Closest Point. 2. Wrapping: Move each point on the registered reference mesh to the closest point on the surface defined by the target mesh.\nHere we deal with step 1. You can also use the pymeshlab GUI.\nFor step 2, look at the wrapping module. You can also “wrap” interactively in blender, using the shrinkwrap modifier.",
    "crumbs": [
      "Python library",
      "Rigid-body registration"
    ]
  },
  {
    "objectID": "Python library/registration.html#loading-the-example-dataset",
    "href": "Python library/registration.html#loading-the-example-dataset",
    "title": "Rigid-body registration",
    "section": "Loading the example dataset",
    "text": "Loading the example dataset\nWe will use the same dataset - a Drosophila example - as in the Tutorial “Tissue cartography for a non-trivial topology”.\n\nmetadata_dict = {'filename': 'datasets/registration_example/Drosophila_CAAX-mCherry',\n                 'resolution_in_microns': (1.05, 1.05, 1.05), # lightsheet data has isotropic resolution\n                 'subsampling_factors': (1/2, 1/2, 1/2),\n                }\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order\n\nimage shape: (1, 190, 509, 188)\n\n\n\nLoad a 3d segmentation of embryo\n\n# after creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n\nsegmentation = tcio.read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # Select the first channel of the segmentation - it's the probability a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (95, 254, 94)\n\n\n\n# look at the segmentation in a cross section\n\nplt.imshow(segmentation[:,:,50], vmin=0, vmax=1)\n\n\n\n\n\n\n\n\n\n\nMeshing\nUsing the marching cubes method.\n\n# Now we create a 3d mesh using the marching cubes method\n\nvertices, faces = tcremesh.marching_cubes(segmentation, 0.5, sigma_smoothing=0.5)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n                                 /np.array(metadata_dict['subsampling_factors']))\n\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"Drosophila_CAAX-mCherry_mesh_marching_cubes\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")\n\n# improve mesh quality using meshlab\nms = pymeshlab.MeshSet()\nms.load_new_mesh(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")\nms.meshing_isotropic_explicit_remeshing()\nms.save_current_mesh(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")",
    "crumbs": [
      "Python library",
      "Rigid-body registration"
    ]
  },
  {
    "objectID": "Python library/registration.html#registration",
    "href": "Python library/registration.html#registration",
    "title": "Rigid-body registration",
    "section": "Registration",
    "text": "Registration\nIn the data folder, we have the mesh we just created, Drosophila_CAAX-mCherry_mesh_remeshed.obj, as well as out reference mesh Drosophila_reference.obj - an idealized Drosophila embryo with a standardized UV mapping, corresponding to a cylindrical projection. You can look at both meshes in the blender file Drosophila_reference.blend (where I recentered the mesh from the image data - which you should not do if you want to use it for interpolation!).\nOur goal will now be to register the reference mesh and the deformed mesh using an affine transformation. Affine transformations are generally written as 4d-matrices, with the extra dimension allowing to save the translation as part of the matrix.\nAffine registration proceeds in two parts. First, we get an initial guess by matching the centroid and axes of inertia of the source and target mesh. Then we refine the registration using the iterative closest point algorithm. For all calculations, we make use of the KDTree data structure which allows efficient lookup of the closest point from a given point cloud to a query point.\n\nsource\n\nalign_by_centroid_and_intertia\n\n align_by_centroid_and_intertia (source, target, q=0, scale=True,\n                                 shear=False, n_samples=10000,\n                                 choose_minimal_rotation=False)\n\n*Align source point cloud to target point cloud using affine transformation.\nAlign by matching centroids and axes of inertia tensor. Since the inertia tensor is invariant under reflections along its principal axes, all 2^3 reflections are tried and (a) the one leading to the best agreement with the target or (b) the one corresponding to the least amount of rotation is chosen. This is controlled using the choose_minimal_rotation argument.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource\nnp.array of shape (n_source, 3)\n\nPoint cloud to be aligned.\n\n\ntarget\nnp.array of shape (n_target, 3)\n\nPoint cloud to align to.\n\n\nq\nint\n0\nQuantile for outlier removal. Removes points with coordinates outside (qth, (1-1)qth) quantile fromcalculation of inertia tensor and centroid\n\n\nscale\nbool\nTrue\nWhether to allow scale transformation (True) or rotations only (False)\n\n\nshear\nbool\nFalse\nWhether to allow shear transformation (True) or rotations/scale only (False)\n\n\nn_samples\nint\n10000\nNumber of samples of source to use when estimating distances.\n\n\nchoose_minimal_rotation\nbool\nFalse\nWhether to chose the rotation matrix closest to the identity. If False, the rotation matrix(possibly with det=-1) leading to the best alignment with the target is chosen.\n\n\nReturns\nnp.array, np.array\n\naffine_matrix_rep : np.array of shape (4, 4) Affine transformation source -&gt; targetaligned : np.array of shape (n_source, 3) Aligned coordinates\n\n\n\n\nsource\n\n\nget_inertia\n\n get_inertia (pts, q=0)\n\nGet inertia tensor of 3d point cloud. q in [0, 1) removes points with outlier coordinates.\n\nsource\n\n\npackage_affine_transformation\n\n package_affine_transformation (matrix, vector)\n\nPackage matrix transformation & translation into (d+1,d+1) matrix representation of affine transformation.\n\nmesh_data = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\nmesh_ref = tcmesh.ObjMesh.read_obj(f\"datasets/registration_example/Drosophila_reference.obj\")\n\n# For illustration purposes, let's rotate and scale the reference mesh so it's off\nrandom_rotation = stats.special_ortho_group.rvs(3)\nmesh_ref.vertices = 1.1 * mesh_ref.vertices @ random_rotation\nmesh_ref.normals = mesh_ref.normals @ random_rotation\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\n# let's test this function\n\nq = 0.0\n\ntrafo_centroid_intertia, vs_prealigned = align_by_centroid_and_intertia(mesh_ref.vertices,\n                                                                        mesh_data.vertices,\n                                                                        q=q, scale=True, shear=True,\n                                                                        choose_minimal_rotation=True)\ndata_centroid = stats.trim_mean(mesh_data.vertices, q, axis=0)\ndata_inertia = get_inertia(mesh_data.vertices, q=q)\ndata_eig = np.linalg.eigh(data_inertia)\n\nref_centroid = stats.trim_mean(mesh_ref.vertices, q, axis=0)\nref_inertia = get_inertia(mesh_ref.vertices, q=q)\nref_eig = np.linalg.eigh(ref_inertia)\n\nprealigned_centroid = stats.trim_mean(vs_prealigned, q, axis=0)\nprealigned_inertia = get_inertia(vs_prealigned, q=q)\nprealigned_eig = np.linalg.eigh(prealigned_inertia)\n\n\nprealigned_eig.eigenvalues, data_eig.eigenvalues\n\n(array([ 3141.61049685,  3229.00962125, 19280.74388138]),\n array([ 3141.61049685,  3229.00962125, 19280.74388138]))\n\n\n\n# we correctly matched the axes of intertia and the centroids\n\nprealigned_eig.eigenvalues / data_eig.eigenvalues, np.linalg.norm(data_centroid - prealigned_centroid)\n\n(array([1., 1., 1.]), 1.3012829303080923e-12)\n\n\n\nprealigned_eig.eigenvectors, data_eig.eigenvectors\n\n(array([[-0.40234803,  0.91474904, -0.03674582],\n        [-0.03532074,  0.02459749,  0.99907328],\n        [ 0.91480518,  0.40327305,  0.02241286]]),\n array([[-0.40234803,  0.91474904, -0.03674582],\n        [-0.03532074,  0.02459749,  0.99907328],\n        [ 0.91480518,  0.40327305,  0.02241286]]))\n\n\n\n# mean distance to closest target\n\ntree = spatial.cKDTree(mesh_data.vertices)\nnp.mean(tree.query(mesh_ref.vertices)[0]), np.mean(tree.query(vs_prealigned)[0])\n\n(147.54268012467426, 6.674543338122944)\n\n\n\n_ = align_by_centroid_and_intertia(mesh_ref.vertices, mesh_data.vertices, q=q, scale=False)\n\n151 ms ± 15.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n# We now save the pre-registered mesh. Note that we want to transform both our\n# mesh coordinates and the corresponding normals, as well as potentially flip face orientation\n\nmesh_registered = mesh_ref.apply_affine_to_mesh(trafo_centroid_intertia)\nmesh_registered.name = \"Drosophila_reference_preregistered\"\nmesh_registered.write_obj(f\"datasets/registration_example/Drosophila_reference_preregistered.obj\")\n\n\nfor i in range(10):\n    if i % 2 == 0:\n        continue\n    print(i)\n\n1\n3\n5\n7\n9\n\n\n\n\nIterative closest point algorithm\nWe can refine the initial registration with the ICP algorithm. This code is based on igl.procrustes and trimesh.registration.\n\nsource\n\n\nprocrustes\n\n procrustes (source, target, include_scaling=True,\n             include_reflections=True)\n\n*Wrapper around igl.procrustes\nCompute rotation+scaling+translation between two sets of points.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource\nnp.array of shape (n_points, n_dimensions)\n\n\n\n\ntarget\nnp.array of shape (n_points, n_dimensions)\n\n\n\n\ninclude_scaling\nbool\nTrue\n\n\n\ninclude_reflections\nbool\nTrue\n\n\n\nReturns\nnp.array, np.array, float\n\ntrafo_affine : np.array (4,4) array representing the affine transformation from source to target.aligned : np.array The orientation of the source that best fits the target.disparity : float np.linalg.norm(aligned-target, axis=1).mean()\n\n\n\n\nmesh_ref = tcmesh.ObjMesh.read_obj(f\"datasets/registration_example/Drosophila_reference.obj\")\n\nsource = np.copy(mesh_ref.vertices)\ntarget = 1.3*mesh_ref.vertices@ stats.special_ortho_group.rvs(3) + np.array([12,12,1000])\n\ntrafo_affine, transformed, cost = procrustes(source, target)\ntrafo_affine, cost\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n(array([[  -1.27176709,    0.2129668 ,   -0.16508667,   12.        ],\n        [   0.24407945,    1.24790943,   -0.27045754,   12.        ],\n        [   0.11416518,   -0.29557943,   -1.26079305, 1000.        ],\n        [   0.        ,    0.        ,    0.        ,    1.        ]]),\n 3.4643816882932517e-12)\n\n\n\nspatial.procrustes(source, target)\n\n(array([[-0.00002796,  0.00649897, -0.00003995],\n        [ 0.00026319,  0.00643907, -0.00003871],\n        [ 0.00042143,  0.00637917, -0.00003618],\n        ...,\n        [ 0.000204  , -0.00637917,  0.00000789],\n        [ 0.0001068 , -0.00643907,  0.00000707],\n        [-0.00005934, -0.00649897,  0.00000636]]),\n array([[-0.00002796,  0.00649897, -0.00003995],\n        [ 0.00026319,  0.00643907, -0.00003871],\n        [ 0.00042143,  0.00637917, -0.00003618],\n        ...,\n        [ 0.000204  , -0.00637917,  0.00000789],\n        [ 0.0001068 , -0.00643907,  0.00000707],\n        [-0.00005934, -0.00649897,  0.00000636]]),\n 2.1869379044888485e-28)\n\n\n/opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nApply the iterative closest point algorithm to align point cloud a with\npoint cloud b. Will only produce reasonable results if the...\n  else: warn(msg)\n\nsource\n\n\nicp\n\n icp (source, target, initial=None, threshold=0.0001, max_iterations=20,\n      include_scaling=True, n_samples=1000)\n\nApply the iterative closest point algorithm to align point cloud a with point cloud b. Will only produce reasonable results if the initial transformation is roughly correct. Initial transformation can be found by applying Procrustes’ analysis to a suitable set of landmark points (often picked manually), or by inertia+centroid-based alignment, implemented in align_by_centroid_and_intertia.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource\n(n,3) float\n\nSource points in space.\n\n\ntarget\n(m,3) float or Trimesh\n\nTarget points in space or mesh.\n\n\ninitial\nNoneType\nNone\nInitial transformation.\n\n\nthreshold\nfloat\n0.0001\nStop when the change in cost is less than threshold\n\n\nmax_iterations\nint\n20\nMaximum number of iterations\n\n\ninclude_scaling\nbool\nTrue\nWhether to allow dilations. If False, orthogonal Procrustes is used\n\n\nn_samples\nint\n1000\nIf not None, n_samples sample points are randomly chosen from the source array for distance computation\n\n\nReturns\nnp.array, np.array, float\n\nmatrix : (4,4) np.array The transformation matrix sending a to btransformed : (n,3) np.array The image of a under the transformationcost : float The cost of the transformation\n\n\n\n\nmesh_data = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\nmesh_ref = tcmesh.ObjMesh.read_obj(f\"datasets/registration_example/Drosophila_reference.obj\")\n\n# for illustration purposes, let's rotate and scale the reference mesh so it's off\nmesh_ref = mesh_ref.apply_affine_to_mesh(1.1 * stats.special_ortho_group.rvs(3))\n\nWarning: readOBJ() ignored non-comment line 4:\n  o embryo_rect\n\n\n\nsource = np.copy(mesh_ref.vertices)\ntarget = np.copy(mesh_data.vertices)\ntree = spatial.cKDTree(target)\n\n\nnp.mean(tree.query(source)[0])\n\n135.49963003678982\n\n\n\ntrafo_initial, prealigned = align_by_centroid_and_intertia(source, target, q=0, scale=True, shear=True)\n\n\ntrafo_initial\n\narray([[ -0.06,  -0.6 ,   0.73,  91.62],\n       [ -0.47,  -0.53,  -0.47, 270.93],\n       [ -0.81,   0.49,   0.27,  93.89],\n       [  0.  ,   0.  ,   0.  ,   1.  ]])\n\n\n\nnp.mean(tree.query(prealigned)[0])\n\n6.2561533646459395\n\n\n\ntrafo_icp, icp_aligned, cost = icp(source, target, initial=trafo_initial, threshold=1e-4, max_iterations=50,\n                                   include_scaling=True, n_samples=1000)\n\n\nnp.mean(tree.query(icp_aligned)[0])\n\n5.8583626546496586\n\n\n\ntrafo_icp\n\narray([[ -0.05039776,  -0.61739046,   0.73713584,  92.41686432],\n       [ -0.47081468,  -0.54148091,  -0.48110009, 263.05296811],\n       [ -0.82768076,   0.48507969,   0.28067945,  94.25355359],\n       [  0.        ,   0.        ,   0.        ,   1.        ]])\n\n\n\n\nRegistration in MeshLab\nYou can also perform registration graphically in MeshLab - see this tutorial: https://www.youtube.com/watch?v=30bJcj6yA4c. Use this if you have problems with the automated method above.",
    "crumbs": [
      "Python library",
      "Rigid-body registration"
    ]
  },
  {
    "objectID": "Python library/registration.html#wrapping",
    "href": "Python library/registration.html#wrapping",
    "title": "Rigid-body registration",
    "section": "Wrapping",
    "text": "Wrapping\nNow that we have registered the mesh, we can wrap it! Let’s first do it using Blender with the shrinkwrap modifier. Go to the “layout” tab and click the “blue wrench” on the right to add a modifier. Search for shrinkwrap, select the target, and use “Tangent Normal Project” for best results:\n\n\n\nimage.png\n\n\nIf things look good, click “Apply” to make the modifier permanent and export the mesh as Drosophila_reference_wrapped.obj.\n\nNormals and normal-related problems\nThis may be a good point to note that if you have any problems with multilayer projections, your normals may be messed up. Some advice on how to visualize and if necessary, recalculate mesh normals.\nUseful tools: “Recalculate normals” (under “Mesh” in “Edit Mode”), and the modifier “Normals -&gt; Weighted Normal” (smoothes normals) and “Deform -&gt; Smooth”.\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nmesh = tcmesh.ObjMesh.read_obj(f\"datasets/registration_example/Drosophila_reference_wrapped.obj\")\nresolution = metadata_dict[\"resolution_in_microns\"]\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_reference_wrapped_smoothed_normals\n\n\n\nslice_image, slice_vertices, slice_normals = tcinterp.get_cross_section_vertices_normals(1, 100,\n                                image, mesh, metadata_dict[\"resolution_in_microns\"], width=1.5)\n\n\nplt.scatter(*slice_vertices.T, s=5, c=\"tab:red\")\nplt.quiver(*slice_vertices.T, *slice_normals.T, color=\"tab:red\")\n\nplt.imshow(slice_image[0], vmax=10000, origin=\"lower\")",
    "crumbs": [
      "Python library",
      "Rigid-body registration"
    ]
  },
  {
    "objectID": "Python library/registration.html#procedural-wrapping",
    "href": "Python library/registration.html#procedural-wrapping",
    "title": "Rigid-body registration",
    "section": "Procedural wrapping",
    "text": "Procedural wrapping\nOf course, we do not want to have to do this by hand for a movie with \\(\\sim100\\) frames! In the wrapping module we will introduce tools to automate the wrapping process.",
    "crumbs": [
      "Python library",
      "Rigid-body registration"
    ]
  },
  {
    "objectID": "Python library/registration.html#uv-projection",
    "href": "Python library/registration.html#uv-projection",
    "title": "Rigid-body registration",
    "section": "UV projection",
    "text": "UV projection\nTo see how well all of this has worked, let’s use the wrapped mesh to generate UV projections. We will compare it with the automatic sphere unwrap on the original data mesh.\n\nnormal_offsets = np.array([-4, -2, 0, 2]) ##np.linspace(-5, 2, 8) # in microns\nmetadata_dict[\"normal_offsets\"] = normal_offsets # add the info to the metadata\nuv_grid_steps = 512\n\n\ntcio.save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)\n\n\nprojected_data_wrapped, projected_coordinates_wrapped, projected_normals_wrapped = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=f\"datasets/registration_example/Drosophila_reference_wrapped.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_reference_wrapped_smoothed_normals\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/interpolation.py:215: RuntimeWarning: UV map has self-intersections, 111104 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n\n\n\nprojected_data, projected_coordinates, projected_normals = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=\"datasets/registration_example/Drosophila_CAAX-mCherry_mesh_remeshed_sphere_uv.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\nWarning: readOBJ() ignored non-comment line 4:\n  o Drosophila_CAAX-mCherry_mesh_remeshed\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/interpolation.py:215: RuntimeWarning: UV map has self-intersections, 8 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n\n\n\nfig, (ax1, ax2) = plt.subplots(figsize=(8,8), ncols=2)\nax1.imshow(projected_data_wrapped[0, 0], vmax=10000)\nax2.imshow(projected_data[0, 0][::-1,::-1].T, vmax=10000)\n\n\n\n\n\n\n\n\n\n# save images for visualization in blender\ntexture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\ntcio.save_stack_for_blender(projected_data_wrapped, texture_path, normalization=(0.01, 0.99))\n\n\n# save images as .tif stack for analysis\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data_wrapped, z_axis=1)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_3d_coordinates.tif\", projected_coordinates_wrapped)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_normals.tif\", projected_normals_wrapped)\n\nLet’s add a shader to check the texture looks good:\n\n\n\nimage.png",
    "crumbs": [
      "Python library",
      "Rigid-body registration"
    ]
  },
  {
    "objectID": "Python library/remeshing.html",
    "href": "Python library/remeshing.html",
    "title": "Mesh creation and remeshing",
    "section": "",
    "text": "This notebook contains functions for (a) meshing (computing a mesh from a 3d segmentation) and (b) subdividing and simplifying meshes while preserving UV information. The latter is useful since it allows one to increase or decrease the mesh “resolution” as needed when the mesh is deformed.\n\nMeshing using the marching cubes method\nA key part of the tissue cartography pipeline is converting a 3d segmentation into a triangular mesh. We do this using the marching cube algorithm. It is essential to convert the mesh vertex coordinates from pixels into microns!\n\nsource\n\n\nmarching_cubes\n\n marching_cubes (volume, isovalue=0.5, sigma_smoothing=0)\n\nCompute triangular mesh of isosurface using marching cubes as implemented by lib|igl.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvolume\n3d np.array\n\nArray with scalar values from which to compute the isosurface.\n\n\nisovalue\nfloat\n0.5\n\n\n\nsigma_smoothing\nint\n0\n\n\n\nReturns\nnp.array, np.array\n\nvertices : np.array of shape (n_vertices, 3) Verticesfaces : np.array of shape (n_faces, 3) Triangular faces (each face is a set of indices into the vertices array)\n\n\n\nLet’s test this on an example.\n\nmetadata_dict = {'filename': 'datasets/basics_example/basics_example',\n                 'resolution_in_microns': (1, 0.36, 0.36), # you can typically get this from the .tif metadata\n                 'subsampling_factors': (1, 1/3, 1/3), # how much you subsampled your image for segmentation\n                }\n\n\n# load the segmentation created by ilastik (see notebook 01a)\n\nsegmentation = tcio.read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # Select the first channel of the segmentation - it's the probability a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (26, 151, 170)\n\n\n\nvertices, faces = marching_cubes(segmentation, isovalue=0.5, sigma_smoothing=3)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n                                 /np.array(metadata_dict['subsampling_factors']))\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"basics_example_mesh_marching_cubes_igl\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes_igl.obj\")\n\n\n\nSubdivision with igl\nWe’ll use the igl.upsample function since it can cleanly deal with UV seams. The more sophisticated loop subdivision algorithm messes them up.\n\nsource\n\n\nsubdivide_igl\n\n subdivide_igl (mesh, reglue=True, decimals=None)\n\n*Refine mesh by edge subdivision using igl.\nSubdivides all edges by placing new vertices at edge midpoints. Preserves UV information, by cutting the mesh along seams and (optionally) gluing back after. New texture vertices are also placed at texture-edge midpoints.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmesh\nObjMesh\n\nInitial mesh.\n\n\nreglue\nbool\nTrue\nGlue back after cutting\n\n\ndecimals\nNoneType\nNone\nDecimal precision for merging vertices when regluing. If None, estimate from averageedge mesh length as -4*log_10(avg length)\n\n\nReturns\nObjMesh\n\nSubdivided mesh.\n\n\n\n\nmesh_test = tcmesh.ObjMesh.read_obj(\"datasets/movie_example/initial_uv.obj\")\nmesh_subdiv = subdivide_igl(mesh_test, reglue=True)\nmesh_subdiv.write_obj(\"datasets/movie_example/mesh_subdivided_igl.obj\", )\n\nWarning: readOBJ() ignored non-comment line 4:\n  o mesh_01_cylinder_seams_uv\n\n\n\nplt.triplot(*mesh_subdiv.texture_vertices.T, mesh_subdiv.texture_tris, lw=0.2)\n\n\n\n\n\n\n\n\n\n\nImprove mesh quality by flips\nWe can change our mesh topology while keeping vertex positions fixed by doing edge flips to avoid deformed triangles. This can be done using the intrinsic Delaunay algorithm. By cutting/regluing the mesh, we can avoid flipping edges along the seam, thus preserving UV information.\nNote: this can lead to self-overlaps in the UV-mapped triangulation. this can be fixed by a round of Laplacian smoothing of the UV coordinates.\n\nsource\n\n\nmake_delaunay\n\n make_delaunay (mesh)\n\n*Make mesh triangles less deformed by edge flips.\nThis algorithm improves mesh quality (i.e. makes triangles less deformed) without moving vertices by “edge flips” using the Delaunay algorithm. UV information is preserved by forbidding the flip of edges along the UV seams.\nNote that this algorithm can lead to self-overlap of the UV map. You can fix this using wrapping.smooth_laplacian_texture.*\n\n\n\n\nType\nDetails\n\n\n\n\nmesh\nObjMesh\nInitial mesh.\n\n\nReturns\nObjMesh\nMesh with flipped edges.\n\n\n\n\nmesh = tcmesh.ObjMesh.read_obj(f\"datasets/movie_example/meshes_wrapped/mesh_20_wrapped.obj\")\n\nmesh_new = make_delaunay(mesh)\nmesh_new.write_obj(f\"datasets/movie_example/mesh_20_wrapped_delaunay.obj\")\n\n\nplt.tripcolor(*mesh_new.texture_vertices.T, mesh_new.texture_tris,\n              mesh_new.vertices[mesh_new.get_vertex_to_texture_vertex_indices(),0])\n\n\n\n\n\n\n\n\n\n\nsimplification/decimation with igl\nUnfortunately, mesh simplification (qslim) in igl degrades UV information: Assigning texture coordinates simply based on the birth vertex leads to “jagged” UV maps. Can we do better? I guess not :\n\n\nsource\n\n\nqslim\n\n qslim (mesh, max_n_faces)\n\n*Simplify mesh by face decimation using the qslim algorithm.\nA wrapper of igl.qslim. This will destroy UV mapping information!*\n\n\n\n\nType\nDetails\n\n\n\n\nmesh\nObjMesh\nInitial mesh.\n\n\nmax_n_faces\nint\nMaximum number of faces in output\n\n\nReturns\nObjMesh\nDecimated mesh.",
    "crumbs": [
      "Python library",
      "Mesh creation and remeshing"
    ]
  },
  {
    "objectID": "Tutorials/10_analysis_in_3d.html",
    "href": "Tutorials/10_analysis_in_3d.html",
    "title": "10. 3D Image analysis with cartographic projections",
    "section": "",
    "text": "Note: this tutorial is currently a stub - sorry!\nA main advantage of cartographically projecting 3D data to 2D is that it makes quantitative analysis significantly easier, for example:\n\nsegmenting cells and measuring their shape\nquantifying intensity and anisotropy of fluorescent signals\nmeasuring morphogenetic tissue flows and tissue deformation\n\nBut we have to keep in mind that our data really comes from a 3D shape: when analyzing data in cartographic projections, we have to make sure we are always accounting for mapping distortion and curvature. Think of Greenland on a Mercator projection of the globe. Importantly, however, image intensities are not distorted by cartographic projection. The .tif output of blender_tissue_cartography is a faithful reflection of the image intensities in the original 3D data (however, the created blender textures are not: for them, brightness and constrast are automatically normalized for visualization). You only need to worry about cartographic distortion when measuring shapes.\nThere are two ways you can do this:\n\nMapping back to 3D: it is conceptually easiest and most robust to map whatever object you detected in the 2D image back to 3D, and carry out your quantifications in 3D. For example, after segmenting out an cell, you can map its outline back into 3D. For this purpose, blender_tissue_cartography always saves a second .tif file together with your projected data, which specifies the 3D coordinates of each pixel.\nCorrecting for distortion in 2D: you can use blender_tissue_cartography to correct distortion directly in 2D. For example, if you want to measure the area of a cell in 2D, you can compute the area-distortion (how much a given region in the 2D projection is inflated/deflated compared to 3D), and use that to correct for the distortion. More generally, you can compute the induced metric in 2D, allowing for correct angle measurements as well.\n\nThe relevant tools are provided and explained in the diffgeo module.\n\nVector calculus\nFor quantitative analysis of changing shapes, one often uses flow or deformation vector fields. For instance, you can use particle image velocimetry, optical flow, or simple cell tracking in 2D projected images to see how cells are moving on top of your 3D surface. Using the diffgeo module, you can map these vector fields from 2D where you computed them back to 3D, and analyze them using the (maybe) familiar tools of vector calculus like \\(\\nabla\\cdot\\) and \\(\\nabla \\times\\).\nNote If your 3D surface is also deforming, you need to add the motion on and the motion of the surface. If you use the reference mesh approach to dynamic data (tutorial 9), it is straightforward to compute the motion of the surface at each position in the 2D projection: just take the difference between the associated 3D coordinates at subsequent timepoints.",
    "crumbs": [
      "Tutorials",
      "10. 3D Image analysis with cartographic projections"
    ]
  },
  {
    "objectID": "Tutorials/01_segmentation_with_ilastik.html",
    "href": "Tutorials/01_segmentation_with_ilastik.html",
    "title": "1. 3d segmentation with Ilastik",
    "section": "",
    "text": "In this notebook, we show how to use Ilastik for 3d image segmentation. Obtaining a 3d segmentation of your sample of interest is the first step in the tissue cartography pipeline. The surface to be extracted from the 3d microscopy data is then defined as the segmentation boundary, i.e. between “inside” and “outside”. This should become a lot clearer by working through a concrete example - the mildly curved surface of an epithelium from a confocal \\(z\\)-stack (data from Lye et al. 2024.\nThere are many tools available for segmenting 3d data, and you can use whichever one you would like. The only thing that matters is that it provides you with a 3d mask that defines which part of the image is “inside” and which part is “outside”, and that can be loaded into Python to create a mesh. There are also other methods to create a mesh that do not require a segmentation but only a list of points on your surface (see tutorial “Advanced segmentation”), or you can provide the mesh directly.\nHowever, in our experience, Ilastik is the most extremely robust and versatile tool for getting segmentations and meshes, and we use it in almost all of our tissue cartography pipelines. This is why we recommend it as a starting point.\nIlastik uses machine learning algorithms to segment and classify your data. You draw the labels on your data, and the algorithm predicts a segmentation. You correct the errors interactively until you are happy. No machine learning expertise is required.\nIlastik works great because it makes a “custom” model for your dataset. This means you don’t need to worry about whether your data is similar or not to the training data of some segmentation algorithm or fiddle with parameters. Once trained, you can reuse Ilastik models for batch prediction.",
    "crumbs": [
      "Tutorials",
      "1. 3d segmentation with Ilastik"
    ]
  },
  {
    "objectID": "Tutorials/01_segmentation_with_ilastik.html#loading-and-downsampling-the-data",
    "href": "Tutorials/01_segmentation_with_ilastik.html#loading-and-downsampling-the-data",
    "title": "1. 3d segmentation with Ilastik",
    "section": "Loading and downsampling the data",
    "text": "Loading and downsampling the data\nData description myosin + membrane ventral view of Drosophila embryo during germband extension, from Lye et al. 2024.\nLet’s load the dataset, and downsample it for segmentation purposes. This will make the segmentation run much faster, without sacrificing much detail on the surface of interest. You can either do this graphically in Fiji, or in pyhton.\n\nFiji version\nLet’s open our example dataset: Tutorials/ilastik_example/basics_example.tif in Fiji. Now downscale it by a factor x3:\n\n\n\nimage.png\n\n\nThen save the result as a .h5 file. This will allow Ilastik to work much more efficiently in the next step: File -&gt; Save as -&gt; HD5. Let’s call the result basics_example_subsampled.h5.\n\n\nPython version\nLet’s start by loading the relavant python libraries\n\n# these are the modules with the tissue cartography code\nfrom blender_tissue_cartography import io as tcio # for file reading and saving\nfrom blender_tissue_cartography import mesh as tcmesh # for mesh handling\nfrom blender_tissue_cartography import remesh as tcremesh # for creating a surface mesh\n\n\nimport numpy as np\nfrom skimage import transform\nimport matplotlib.pyplot as plt\n\n\n# start by entering the filename   \nmetadata_dict = {'filename': 'ilastik_example/basics_example'}\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape)\n\nimage shape: (2, 26, 454, 511)\n\n\n\nResolution info and subsampling\nFrom the image shape, we can see that the spatial axes are in \\(z-x-y\\) order. We use this information to correctly enter the resolution in microns/pixel for each axis. You can typically get this info from the .tif metadata. We then subsample the image for rapid segmentation. This will greatly speed up the segmentation process. It is usually a good idea to make the subsampled image approximately isotropic. Subsample so much that the geometry of your object of interest is still visible (i.e. curvature and/or wrinkles in your surface).\n\nmetadata_dict['resolution_in_microns'] = (1, 0.36, 0.36)\nmetadata_dict['subsampling_factors'] = (1, 1/3, 1/3)\n\n\n# let's plot the data in a cross section\n\nplt.imshow(image[1,:,:,250],\n           aspect=metadata_dict['resolution_in_microns'][0]/metadata_dict['resolution_in_microns'][1])\n\n\n\n\n\n\n\n\n\nsubsampled_image = transform.rescale(image, metadata_dict['subsampling_factors'],\n                                     channel_axis=0, preserve_range=True)\nprint(\"subsampled image shape:\", subsampled_image.shape)\n\nsubsampled image shape: (2, 26, 151, 170)\n\n\nWe now save the subsampled image as a .h5 file for input into ilastik for segmentation. .h5 files are faster to read for ilastik and perform much better than .tif’s.\n\ntcio.write_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image, axis_order=\"CZYX\")",
    "crumbs": [
      "Tutorials",
      "1. 3d segmentation with Ilastik"
    ]
  },
  {
    "objectID": "Tutorials/01_segmentation_with_ilastik.html#create-3d-segmentation",
    "href": "Tutorials/01_segmentation_with_ilastik.html#create-3d-segmentation",
    "title": "1. 3d segmentation with Ilastik",
    "section": "Create 3d segmentation",
    "text": "Create 3d segmentation\nWe now use ilastik binary pixel classification.\nStart Ilastik, and select binary pixel classification:\n\n\n\nimage.png\n\n\nSave the project .ilp file to the ilastik_example directory.\nNow let’s add the dataset:\n\n\n\nimage.png\n\n\nClick “add separate images” and select basics_example_subsampled.h5 from the ilastik_example directory.\n\nAttention: Axis order\nIlastik does not always know what the axes in your input data mean. The axis_order argument in tcio.write_h5 allows you to write the axis order into the .h5 file. If, nevertheless, ilastik thinks the “channel” axis is the “z” axis, we can fix it as follows. “Right click” on the dataset and “edit properties”:\n\n\n\nimage.png\n\n\nOur dataset is in CZYX order (where C=channel). By convention, blender_tissue_cartography always puts the channel as the first axis. Ilastik should show you three cross-sections of your data in red/green color.\nClick on “Feature selection” on the left toolbar to continue.\n\n\nFeature selection\nIlastik internally works by applying a bunch of different image filters (like smoothing, or edge detection) to your data, and using the results as input for a machine-learning model. You now select which filters (“Features”) you want. In general, it is safe to just select everything. If you get an error, it may be the case that the size of the filter is bigger than the smallest axis (e.g. the \\(z\\)-axis) in your image. In this case, click the top bar to make the filter “2d”:\n\n\n\nimage.png\n\n\n\n\nTraining\nWe now continue to “Training”. Zoom in to one of the cross sections, and paint the “inside” of the sample in yellow, and the “outside” in blue. You can also add more labels, but here we will just need two. You just need to make a few brush strokes and ilastik will start making predictions. Click the “Live Update” button:\n\n\n\nimage-2.png\n\n\nOn the left, you can adjust the lookup table for your data, and decide what features of the prediction you want to see. I find it very helpful to look at “Uncertainty” which shows you in light blue where the algorithm is still unsure:\n\n\n\nimage-3.png\n\n\nNow paint more labels until you are happy with the classification. Target the regions of high uncertainty, and look in all three cross sections. It is important that the classification boundary tightly matches the surface you want to extract!\n\n\nPrediction Export\nOnce you are happy with the results, let’s go to “Prediction Export” on the left toolbar. Use “Source: Probabilities” and click “export all”:\n\n\n\nimage.png\n\n\nYou can adjust the axis order and output format under “Choose Export Image Settings” if you need.\n\n\nBatch processing\nYou can apply the Ilastik model to multiple datasets, using the “Batch Processing” tab on the left toolbar. In general, I recommend doing this only for datasets that are highly “comparable”, e.g. recorded with exactly the same microscopy settings. Since it’s fairly easy and quick to train a new ilastik model for each dataset, trying to build a model that will work across all your recordings is often not worth it.\nAttention: data normalization\nIf you plan on reusing the same model, it is recommended to normalize your data, so that the pixel values are always of the same approximate magnitude. Do this before you save the data as .h5’s, for example using the provided normalize_quantiles-function:\n\ntcio.write_h5(f\"{metadata_dict['filename']}_subsampled_normalized.h5\",\n              tcio.normalize_quantiles(subsampled_image))",
    "crumbs": [
      "Tutorials",
      "1. 3d segmentation with Ilastik"
    ]
  },
  {
    "objectID": "Tutorials/01_segmentation_with_ilastik.html#creating-a-mesh-from-the-segmentation",
    "href": "Tutorials/01_segmentation_with_ilastik.html#creating-a-mesh-from-the-segmentation",
    "title": "1. 3d segmentation with Ilastik",
    "section": "Creating a mesh from the segmentation",
    "text": "Creating a mesh from the segmentation\nThe next step is to convert the segmentation of our sample into a mesh representing the surface of interest. If you are using the add-on version of blender_tissue_cartography, see tutorial 3, “Tissue cartography Blender add-on”. If you are using the Pyhton library, see below.\n\nLoading segmentation into jupyter\nAfter creating an ilastik project, training the model, and exporting the probabilities, we load the exported probabilities back into Python.\nThe resulting segmentation has two channels for the probability of a pixel being “inside” and “outside”.\n\nsegmentation = tcio.read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # select the first channel of the segmentation - it's the probablity a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (26, 151, 170)\n\n\n\n# look at the segmentation in a cross section\n\nplt.imshow(segmentation[:,:,50], vmin=0, vmax=1)\n\n\n\n\n\n\n\n\n\n\nComputational post-processing\nIn our simple example, the segmentation story ends here, since the ilastik output already provides a great segmentation of our data. But we are not always so lucky, and sometimes extra processing is necessary.\nI can recommend the scipy.ndimage module (for fixing holes, removing small erroneous “blobs” in your segmentation, etc.), and the morphsnakes package which works by computationally “inflating a balloon” at a seed point, with the ilastik probability output acting as a barrier. See this example, with the “balloon” in blue:  This is very helpful when your ilastik output defines the boundary of the object you want to segment, not its volume. The morphsnakes package is explained in tutorial 6.\nDon’t worry if your segmentation contains some small errors. We can fix the resulting errors in the surface in blender’s graphical user interface in the next step.\n\n\nMeshing\nWe can convert the segmentation into a triangular mesh using the marching cubes method, and save the mesh. We save all meshes in as wavefront .obj files (see wikipedia).\nImportant convention For sanities sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\nWe then save the mesh to disk. In the next tutorial, we will load this mesh in the 3d software blender which is the heart of the tissue cartography pipeline\n\n# now we create a 3d mesh of using the marching cubes method\n\nvertices, faces = tcremesh.marching_cubes(segmentation, isovalue=0.5, sigma_smoothing=3)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n                                 /np.array(metadata_dict['subsampling_factors']))\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"basics_example_mesh_marching_cubes\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")",
    "crumbs": [
      "Tutorials",
      "1. 3d segmentation with Ilastik"
    ]
  },
  {
    "objectID": "Tutorials/01_segmentation_with_ilastik.html#next-steps",
    "href": "Tutorials/01_segmentation_with_ilastik.html#next-steps",
    "title": "1. 3d segmentation with Ilastik",
    "section": "Next steps",
    "text": "Next steps\nNow we will switch to blender to inspect our mesh and map it to the plane.",
    "crumbs": [
      "Tutorials",
      "1. 3d segmentation with Ilastik"
    ]
  },
  {
    "objectID": "Tutorials/05_UV_maps_with_seams.html",
    "href": "Tutorials/05_UV_maps_with_seams.html",
    "title": "5. Tissue cartography with “seams”",
    "section": "",
    "text": "In tutorial 4, we used tissue cartography on a very simple surface - a mildly curved plane. Here, we’ll look at how we can deal with a more complicated surface - that of the gastrulating Drosophila embryo. This is an ellipsoidal surface and will therefore require seams to map it to the plane.\nfrom blender_tissue_cartography import io as tcio\nfrom blender_tissue_cartography import mesh as tcmesh\nfrom blender_tissue_cartography import remesh as tcremesh\nfrom blender_tissue_cartography import interpolation as tcinterp\nfrom importlib import reload\nimport numpy as np\nfrom skimage import transform\nimport os\nimport matplotlib.pyplot as plt\n# only run this if you have installed pymeshlab\nimport pymeshlab\nfrom blender_tissue_cartography import remesh_pymeshlab as tcremesh_pymeshlab\n\nWarning:\nUnable to load the following plugins:\n\n    libio_e57.so: libio_e57.so does not seem to be a Qt Plugin.\n\nCannot load library /home/nikolas/Programs/miniconda3/envs/blender-tissue-cartography/lib/python3.11/site-packages/pymeshlab/lib/plugins/libio_e57.so: (/lib/x86_64-linux-gnu/libp11-kit.so.0: undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0)",
    "crumbs": [
      "Tutorials",
      "5. Tissue cartography with \"seams\""
    ]
  },
  {
    "objectID": "Tutorials/05_UV_maps_with_seams.html#load-and-subsample-data-for-segmentation",
    "href": "Tutorials/05_UV_maps_with_seams.html#load-and-subsample-data-for-segmentation",
    "title": "5. Tissue cartography with “seams”",
    "section": "Load and subsample data for segmentation",
    "text": "Load and subsample data for segmentation\nData description in toto light sheet recording of gastrulating Drosophila embryo with membrane marker CAAX-mCherry.\nWe begin by creating a directory for our project where we’ll save all related files. Let’s load the dataset. We then enter the relevant metadata - the filename, resolution in microns, and how much we want to subsample for segmentation purposes.\n\nmetadata_dict = {'filename': 'drosophila_example/Drosophila_CAAX-mCherry'}\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape)\n\nimage shape: (1, 190, 509, 188)\n\n\n\nResolution info and subsampling\nWe now enter the resolution in microns/pixel for each axis. You can typically get this info from the .tif metadata. Here we use lightsheet data which has isotropic resolution. We then subsample the image for rapid segmentation.\n\nmetadata_dict['resolution_in_microns'] = (1.05, 1.05, 1.05)\nmetadata_dict['subsampling_factors'] = (1/2, 1/2, 1/2)\n\nWe now downscale the image. For very large volumetric images, you should use the option use_block_averaging_if_possible=True and choose subsampling factors which (a) are inverse integers like 1/2, 1/3, and (b) if possible, divide the number of pixels along each axis (so for instance 1/2 would be good for an axis with 1000 pixels, but not ideal for 1001 pixels).\n\nsubsampled_image = tcio.subsample_image(image, metadata_dict['subsampling_factors'],\n                                        use_block_averaging_if_possible=False)\nprint(\"subsampled image shape:\", subsampled_image.shape)\n\nsubsampled image shape: (1, 95, 254, 94)",
    "crumbs": [
      "Tutorials",
      "5. Tissue cartography with \"seams\""
    ]
  },
  {
    "objectID": "Tutorials/05_UV_maps_with_seams.html#create-3d-segmentation",
    "href": "Tutorials/05_UV_maps_with_seams.html#create-3d-segmentation",
    "title": "5. Tissue cartography with “seams”",
    "section": "Create 3d segmentation",
    "text": "Create 3d segmentation\nNow create a 3d segmentation, in this case using ilatik. We use ilastik binary pixel classification. We could post-process the ilastik output here, for example using morphsnakes. We then load the segmentation back into the jupyter notebook.\nThe bright dots outside the embryo are fluorescent beads necessary for sample registration in lightsheet microscopy. You can ignore them.\nAttention: when importing the .h5 into ilastik, make sure the dimension order is correct! In this case, CZYX for both export and import.\n\n# we now save the subsampled image a .h5 file for input into ilastik for segmentation\n\ntcio.write_h5(f\"{metadata_dict['filename']}_subsampled.h5\", subsampled_image)\n\n\n# after creating an ilastik project, training the model, and exporting the probabilities, we load the segmentation\n\nsegmentation = tcio.read_h5(f\"{metadata_dict['filename']}_subsampled-image_Probabilities.h5\")\nsegmentation = segmentation[0] # select the first channel of the segmentation - it's the probablity a pixel\n                               # is part of the sample\nprint(\"segmentation shape:\", segmentation.shape)\n\nsegmentation shape: (95, 254, 94)\n\n\n\n# look at the segmentation in a cross section\n\nplt.imshow(segmentation[:,:,50], vmin=0, vmax=1)",
    "crumbs": [
      "Tutorials",
      "5. Tissue cartography with \"seams\""
    ]
  },
  {
    "objectID": "Tutorials/05_UV_maps_with_seams.html#meshing",
    "href": "Tutorials/05_UV_maps_with_seams.html#meshing",
    "title": "5. Tissue cartography with “seams”",
    "section": "Meshing",
    "text": "Meshing\nWe convert the segmentation into a triangular mesh using the marching cubes method and save the mesh. We save all meshes as wavefront .obj files (see wikipedia).\nImportant convention For sanity’s sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n\n# now we create a 3d mesh of using the marching cubes method\n\nvertices, faces = tcremesh.marching_cubes(segmentation, isovalue=0.5, sigma_smoothing=3)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * (np.array(metadata_dict['resolution_in_microns'])\n                                 /np.array(metadata_dict['subsampling_factors']))\n\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"Drosophila_CAAX-mCherry_mesh_marching_cubes\"\nmesh.write_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")\n\n\nOptional - improve mesh quality using MeshLab\nWe can remesh the output of the marching cubes algorithm to obtain an improved mesh, i.e. with more uniform triangle shapes. In this example, we first remesh to make the mesh more uniform. You can also try this out in the MeshLab GUI and export your workflow as a Python script. Be careful not to move the mesh or it will mess up the correspondence with the pixel coordinates!\nSee List of MeshLab filers\n\nmesh_remeshed = tcremesh_pymeshlab.remesh_pymeshlab(mesh)\nmesh_remeshed.write_obj(f\"{metadata_dict['filename']}_mesh_remeshed.obj\")\n\nTo check all went well, let’s overlay the mesh coordinates over a cross section of the image.\n\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}_mesh_marching_cubes.obj\")\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\n\n\nslice_image, slice_vertices = tcinterp.get_cross_section_vertices_normals(1, 100,\n                                image, mesh, metadata_dict[\"resolution_in_microns\"],\n                                get_normals=False, width=2)\n\n\nfig = plt.figure(figsize=(5,5))\nplt.scatter(*slice_vertices.T, s=5, c=\"tab:red\")\nplt.imshow(slice_image[0], origin=\"lower\", vmax=10000)",
    "crumbs": [
      "Tutorials",
      "5. Tissue cartography with \"seams\""
    ]
  },
  {
    "objectID": "Tutorials/05_UV_maps_with_seams.html#processing-in-blender",
    "href": "Tutorials/05_UV_maps_with_seams.html#processing-in-blender",
    "title": "5. Tissue cartography with “seams”",
    "section": "Processing in blender",
    "text": "Processing in blender\nWe now switch to blender and create a new empty project, which we will call Drosophila_CAAX-mCherry.blend\". We import the mesh just generated (File-&gt;Import).\nI recommend using the “object” tab (orange square on the right toolbar) to lock mesh position and rotation so we don’t accidentally move it.\n\nMesh editing\nThe mesh we have created unfortunately contains a few holes, where the segmentation surface touched the image boundary:\n\n\n\nimage.png\n\n\nUseful shortcuts: “A” selects all and “.” on the Numpad centers the view on the currently selected element.\nLet’s fix them using blender’s mesh editing tools. Go to the “Modeling” workspace on the top toolbar. Press “2” to get to edge mode, select the loop around the whole and then use “Faces-&gt;fill” to fill the hole.\nWe can also use the “Mesh-&gt;fill holes tool”. Afterward, we can smooth out our mesh in the “Sculpting” tab, for instance using the “Smooth” tool. This is the result:\n\n\n\nimage-2.png\n\n\nNot super beautiful but this is for illustration purposes only.\n\n\nUV mapping\nLet’s try to move forward and get a UV map of the mesh. To do so, we go to the “UV Editing” tab on the top toolbar, and press “3” then “A” to select all faces (“1” selects vertices, “2” edges, and “3” faces). Click “UV-&gt;sphere projection” on the top panel to get an automated, default UV map:\n\n\n\nimage.png\n\n\n\nDefining seams\nSince our surface is not topologically equivalent to the cartographic UV square, we need to define seams along which to cut. Blender makes some choices automatically, but it might not be what we want.\nPress “2” to go into edge selection mode, select a loop along which you want to cut, right-click, and select “Mark seam”. We’ll do this on the anterior and posterior poles of the embryo, as well as along the dorsal midline.\nFor the dorsal seam, it’s easiest to specify it by selecting two vertices and finding the shortest path between them. Press “1” for vertex mode, “right click” on the initial vertex, hold “ctrl” and “left click” on the final vertex, then press “2” to switch back to edge mode. Our seams now look like this:\n\n\n\nimage.png\n\n\nNow we can click “UV-&gt; unwrap” and get an unwrapping with our desired seams!\n\n\n\nimage-2.png\n\n\nBy default, blender will make a projection that tries to preserve areas. The two little caps in the bottom are our poles. To move stuff around in the UV map, select the objects of interest, press “g” to move, and “click” to confirm the new position. Try it by moving the pole caps a little away from one another.\nThere are various tools for editing UV maps\n\n\n\nBlender export\nOnce we are happy with our UV map, we click on export and save it as .obj with UV and normals.\n\n\n\nimage.png\n\n\nA few things are important: - Always include UV and normals. Otherwise, cartographic projection will fail! - Only export selected items! With a more complicated blender project, you might end up exporting multiple meshes. This will trip up the cartographic projection algorithm. - Savie as a triangulated mesh, since some of the other tools/python geometry libraries we will use for more advanced examples work best/only with triangular meshes. This option will subdivide any quads/polygons your mesh may have.\nThe new mesh file f\"{metadata_dict['filename']}_mesh_uv.obj\" now contains vertex normals and UV coordinates.\n\n\nCartographic projection/interpolation onto UV grid\nWe now read in the new .obj file to interpolate the image data onto the 3d mesh. The UV grid always covers the unit square \\([0,1]^2\\).\nWe additionally specify a series of offsets in the normal direction to make a multilayer projection. We also get the 3d coordinates and the vertex normals interpolated onto the UV grid from this step.\n\nnormal_offsets = np.linspace(-2, 2, 5) # in microns\nmetadata_dict[\"normal_offsets\"] = normal_offsets # add the info to the metadata\nuv_grid_steps = 512\n\n\nprojected_data, projected_coordinates, projected_normals = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=f\"{metadata_dict['filename']}_mesh_uv.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\nCPU times: user 982 ms, sys: 693 ms, total: 1.67 s\nWall time: 1.69 s\n\n\n\n# save images for visualization in blender\ntexture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\ntcio.save_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))\n\n\n# save images as .tif stack for analysis\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_projected.tif\", projected_data, z_axis=1)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_3d_coordinates.tif\", projected_coordinates)\ntcio.save_for_imageJ(f\"{metadata_dict['filename']}_normals.tif\", projected_normals)\n\n\n# save the metadata to a human and computer readable file\ntcio.save_dict_to_json(f\"{metadata_dict['filename']}_metadata.json\", metadata_dict)\n\n\n# show the projected data\n\nfig = plt.figure(figsize=(4,4))\nplt.imshow(projected_data[0, 1])\n\n\n\n\n\n\n\n\n\n# show the projected 3d coordinates\n\nfig = plt.figure(figsize=(4,4))\nplt.imshow(projected_coordinates[...,1])\n\n\n\n\n\n\n\n\n\n\nVisualization\nAs in the previous example, we now use blender’s shading tools to show the projections as textures on our 3d mesh. But let’s go a step further and add a shader (green box, “Principled BSDF”) that gives a better appreciation of depth/3D-ness.\n\n\n\nimage.png",
    "crumbs": [
      "Tutorials",
      "5. Tissue cartography with \"seams\""
    ]
  },
  {
    "objectID": "Tutorials/07_advanced_segmentation_and_meshing.html",
    "href": "Tutorials/07_advanced_segmentation_and_meshing.html",
    "title": "7. Advanced segmentation and meshing",
    "section": "",
    "text": "The first step in the tissue cartography pipeline is the creation of a 3d segmentation of your object of interest. So far, we have used ilastik’s pixel classifier to do this. However, this may not always work:\nHere, we first show how you can adress this issue using the morphsnakes package. This algorithm essentially works by computationally “inflating a balloon” at a seed point, with the ilastik probability output acting as a barrier. See this example, with the “balloon” in blue:\nWith the resulting solid segmentation in hand, you can create a triangular mesh using the marching cubes algorithm as before.",
    "crumbs": [
      "Tutorials",
      "7. Advanced segmentation and meshing"
    ]
  },
  {
    "objectID": "Tutorials/07_advanced_segmentation_and_meshing.html#meshing",
    "href": "Tutorials/07_advanced_segmentation_and_meshing.html#meshing",
    "title": "7. Advanced segmentation and meshing",
    "section": "Meshing",
    "text": "Meshing\nWe convert the segmentation into a triangular mesh using the marching cubes method, as before.\nImportant convention For sanity’s sake, we will always store all mesh coordinates in microns. This means rescaling appropriately after calculating the mesh from the 3d segmentation.\n\nmetadata_dict = {'resolution_in_microns': (1, 1, 1),}\n\n\n# now we create a 3d mesh of using the marching cubes method\n\nvertices, faces = tcremesh.marching_cubes(segmentation_filled_expanded.astype(float), isovalue=0.5,\n                                          sigma_smoothing=1)\n\n# EXTREMELY IMPORTANT - we now rescale the vertex coordinates so that they are in microns.\nvertices_in_microns = vertices * np.array(metadata_dict['resolution_in_microns'])\n\n\nmesh = tcmesh.ObjMesh(vertices_in_microns, faces)\nmesh.name = \"reconstruction_example_mesh_marching_cubes\"\n\n\nmesh.vertices.shape # number of vertices\n\n(18751, 3)\n\n\n\n# improve mesh quality using meshlab - optional\n\nmesh_simplified = tcremesh_pymeshlab.remesh_pymeshlab(mesh, iterations=10, targetlen=1)\n\n\nmesh_simplified.vertices.shape # number of vertices\n\n(10914, 3)\n\n\n\nmesh_simplified.write_obj(\"reconstruction_example/zebrafish_mesh_marching_cubes.obj\")",
    "crumbs": [
      "Tutorials",
      "7. Advanced segmentation and meshing"
    ]
  },
  {
    "objectID": "Tutorials/07_advanced_segmentation_and_meshing.html#blender-visualization",
    "href": "Tutorials/07_advanced_segmentation_and_meshing.html#blender-visualization",
    "title": "7. Advanced segmentation and meshing",
    "section": "Blender visualization:",
    "text": "Blender visualization:\nLoad the mesh into blender to look at the results:\n\n\n\nimage.png\n\n\nThe mesh has two holes as the embryo does not fully fit into the microscope field of view.",
    "crumbs": [
      "Tutorials",
      "7. Advanced segmentation and meshing"
    ]
  },
  {
    "objectID": "Tutorials/07_advanced_segmentation_and_meshing.html#surface-reconstruction-from-a-point-cloud",
    "href": "Tutorials/07_advanced_segmentation_and_meshing.html#surface-reconstruction-from-a-point-cloud",
    "title": "7. Advanced segmentation and meshing",
    "section": "Surface reconstruction from a point cloud",
    "text": "Surface reconstruction from a point cloud\nAs a first step, we’ll have to extract a point cloud from our segmentation. There are several ways you could do this, for instance, moving along an axis more or less transversal to the surface and looking for a local maximum in the segmentation intensity. We’ll do the simplest possible thing and consider all points with a segmentation intensity greater than a threshold (this is generally a bad idea). We then simplify the point cloud (reduce the number of points), and apply the surface reconstruction algorithm.\nYou can also do this graphically in the MeshLab GUI, using the “Surface reconstruction” filters.\n\n# let's load the segmentation \nsegmentation = tcio.read_h5(f\"reconstruction_example/zebrafish_probabilities.h5\")[0]\n\n# now let's select all the points where the segmentation probability exceeds some threshold\nthreshold = 0.4\nsegmentation_binary = segmentation&gt;threshold\nsegmentation_binary = ndimage.binary_erosion(segmentation_binary, iterations=1)\npoints = np.stack(np.where(segmentation_binary), axis=-1)\n\n\nzslice = 80\nplt.imshow(segmentation_binary[zslice,:,:])\n\n\n\n\n\n\n\n\n\n# a point cloud is simply a mesh with no faces\n\npoint_cloud = tcmesh.ObjMesh(vertices=points, faces=[])\npoint_cloud_pymeshlab = intmsl.convert_to_pymeshlab(point_cloud)\n\n\n# let's create a pymeshlab instance and add out point cloud to it\n\n# There are three relevant filters we will use:\n# generate_simplified_point_cloud - reduce number of points in point cloud\n# compute_normal_for_point_clouds - estimate normals for point cloid. This is required for the next step\n# generate_surface_reconstruction_screened_poisson - Surface reconstruction by Poisson reconstruction\n\nms = pymeshlab.MeshSet()\nms.add_mesh(point_cloud_pymeshlab)\n\nms.generate_simplified_point_cloud(samplenum=1000)\nms.compute_normal_for_point_clouds(k=20, smoothiter=2)\nms.generate_surface_reconstruction_screened_poisson(depth=8, fulldepth=5,)\n\nms.meshing_isotropic_explicit_remeshing(iterations=10, targetlen=pymeshlab.PercentageValue(1))\n\nmesh_reconstructed = intmsl.convert_from_pymeshlab(ms.current_mesh())\n\n\nmesh_reconstructed.faces.shape\n\n(24974, 3)\n\n\n\nmesh_reconstructed.write_obj(\"reconstruction_example/zebrafish_mesh_reconstructed.obj\")\n\n\n# we also provide a simple wrapper for this procedure\n\nmesh_reconstructed = tcremesh_pymeshlab.reconstruct_poisson(points, samplenum=1000,\n                                                            reconstruc_args={\"depth\": 8, \"fulldepth\": 5})\nmesh_reconstructed.faces.shape\n\n(7090, 3)\n\n\nCompare the result of the Poisson reconstruction and marching cubes methods in blender! The Poisson reconstruction gives us a mesh of the outer surface of the embryo.",
    "crumbs": [
      "Tutorials",
      "7. Advanced segmentation and meshing"
    ]
  },
  {
    "objectID": "Tutorials/06_improving_UV_maps.html",
    "href": "Tutorials/06_improving_UV_maps.html",
    "title": "6. Iteratively improving cartographic projections",
    "section": "",
    "text": "The tissue cartography process involves multiple steps:\nIt is likely that after your first pass, you will not be 100% satisfied with the results, and in general, you will need to iterate steps 1-4 to achieve good results. A key advantage of using blender is that you can visualize your results from the previous pass and use them as a guide. Here we will see how to improve an initial unwrapping/cartographic projection of an example dataset of the zebrafish egg.\nCrucially, almost any unwrapping map will give you good enough results to visualize the data on the blender 3d model. You can then use that as a guide to make a better unwrapping (with less distortion, focused on the region of interest, etc).\nNote: This tutorial uses the blender_tissue_cartography python library. Interactively designing your cartographic projections is now much easier using the blender_tissue_cartography Blender add-on (which I created after writing this tutorial). Please see tutorial 3.\nfrom blender_tissue_cartography import io as tcio\nfrom blender_tissue_cartography import mesh as tcmesh\nfrom blender_tissue_cartography import interpolation as tcinterp\nimport numpy as np\nfrom skimage import transform\nfrom scipy import stats, spatial, linalg\nimport matplotlib.pyplot as plt\nimport os",
    "crumbs": [
      "Tutorials",
      "6. Iteratively improving cartographic projections"
    ]
  },
  {
    "objectID": "Tutorials/06_improving_UV_maps.html#load-dataset",
    "href": "Tutorials/06_improving_UV_maps.html#load-dataset",
    "title": "6. Iteratively improving cartographic projections",
    "section": "Load dataset",
    "text": "Load dataset\nThis dataset is a lightsheet recording of the early zebrafish embryo right before epiboly, showing fluorescent nuclei. Courtesy of S. Wopat, Streichan lab UCSB.\n\nmetadata_dict = {\"filename\": \"fish_example/zebrafish_egg_nuclei\",\n                 \"resolution_in_microns\": [1.662, 1.662, 1.662]}\n\n\nimage = tcio.adjust_axis_order(tcio.imread(f\"{metadata_dict['filename']}.tif\"))\nprint(\"image shape:\", image.shape) # image shape - spatial axes are in z-x-y order\n\nimage shape: (1, 246, 194, 248)\n\n\n\n# segmentation and meshing are already done for this dataset\nmesh = tcmesh.ObjMesh.read_obj(f\"{metadata_dict['filename']}.obj\")\n\n\nslice_image, slice_vertices = tcinterp.get_cross_section_vertices_normals(\n    1, 80, image, mesh, metadata_dict[\"resolution_in_microns\"], width=2, get_normals=False)\n\n\nplt.scatter(*slice_vertices.T, s=5, c=\"tab:red\")\n\nplt.imshow(slice_image[0], vmax=10000, origin=\"lower\")",
    "crumbs": [
      "Tutorials",
      "6. Iteratively improving cartographic projections"
    ]
  },
  {
    "objectID": "Tutorials/06_improving_UV_maps.html#unwrapping-in-blender",
    "href": "Tutorials/06_improving_UV_maps.html#unwrapping-in-blender",
    "title": "6. Iteratively improving cartographic projections",
    "section": "Unwrapping in blender",
    "text": "Unwrapping in blender\nLet’s unwrap our mesh in blender. Open a new blender project, and import zebrafish_egg_nuclei.obj. We’ll start by using a standard-issue spherical projection for unwrapping. In the UV Editor, look at the egg “from above”, and choose “UV-&gt;Sphere project” with “View on poles”:\n\n\n\nimage.png\n\n\nExport the result as zebrafish_egg_nuclei_uv_first_iteration.obj.",
    "crumbs": [
      "Tutorials",
      "6. Iteratively improving cartographic projections"
    ]
  },
  {
    "objectID": "Tutorials/06_improving_UV_maps.html#first-iteration-cartographic-projection",
    "href": "Tutorials/06_improving_UV_maps.html#first-iteration-cartographic-projection",
    "title": "6. Iteratively improving cartographic projections",
    "section": "First iteration: Cartographic projection",
    "text": "First iteration: Cartographic projection\nLet’s use this “automatic” UV projection to get the 3d data as a texture on our mesh. With that as guidance, we can then refine our UV projection.\n\nnormal_offsets = np.array([-4, -2, 0, 2, 4])\nmetadata_dict[\"normal_offsets\"] = normal_offsets # add the info to the metadata\nuv_grid_steps = 512\n\n\nprojected_data, projected_coordinates, projected_normals = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=f\"fish_example/zebrafish_egg_nuclei_uv_first_iteration.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps,\n    use_fallback=True)\n# for the initial UV map, we use the fallback option since there are a lot of flipped triangles which\n# can mess up our cartographic interpolation\n\n/home/nikolas/Documents/UCSB/streichan/numerics/code/python code/jupyter notebooks/blender-tissue-cartography/blender_tissue_cartography/interpolation.py:215: RuntimeWarning: UV map has self-intersections, 20185 flipped triangles. Try use_fallback=True?\n  warnings.warn(\"UV map has self-intersections, {} flipped triangles. Try use_fallback=True?\".format(\n\n\n\nplt.imshow(projected_coordinates[...,0])\n\n\n\n\n\n\n\n\nThe projected data has a lot of distortion, and shows a large part of the embryo we don’t care about - the dark part on the top, which is just yolk.\n\nplt.imshow(projected_data[0, 0], vmax=10000)\n\n\n\n\n\n\n\n\n\n# save images for visualization in blender\ntexture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures\"\ntcio.save_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))",
    "crumbs": [
      "Tutorials",
      "6. Iteratively improving cartographic projections"
    ]
  },
  {
    "objectID": "Tutorials/06_improving_UV_maps.html#second-iteration",
    "href": "Tutorials/06_improving_UV_maps.html#second-iteration",
    "title": "6. Iteratively improving cartographic projections",
    "section": "Second iteration",
    "text": "Second iteration\nLet’s go back to blender and, in the Shading workspace, add our projected images as textures on the 3d mesh:\n\n\n\nimage.png\n\n\nNow we can see what part of the embryo we care about! Let’s go back to the UV Editor workspace, and make a seam along the edge of the region where there are cells. To do so, we want to use “Material preview” as Viewport shading:\n\n\n\nimage-2.png\n\n\nNow we can choose a nice seam. Let’s use “circle select” (“C”) and select the top part of the mesh:\n\n\n\nimage.png\n\n\nand then get the boundary loop by “Select -&gt; Select loops -&gt; Select boundary loop”. Right-click and “Mark as seam”:\n\n\n\nimage-3.png\n\n\nNow we have a seam that matches well the region we care about! Let’s use UV unwrapping to generate a new UV map:\n\n\n\nimage.png\n\n\nThe projected data is now much better - the region of interest has much less distortion. You can make the uninteresting “top” part of the mesh very small, and scale the main part of the UV map to better fill the UV square, using the scaling tool.\nClick on the patch (“island”), press “L” to select the whole patch, “S” for scale, and “G” for translate. Click to confirm.\n\nSecond iteration: cartographic projection\nLet’s export the mesh as zebrafish_egg_nuclei_uv_second_iteration.obj and create a new cartographic projection. The result will look identical when viewed in 3D in blender. But the cartographic projection now focuses on our region of interest, has no seams within that region, and has much lower distortion. This makes it superior for quantitative analysis (for example, counting the number of nuclei).\n\nnormal_offsets = np.array([-4, -2, 0, 2, 4])\nmetadata_dict[\"normal_offsets\"] = normal_offsets # add the info to the metadata\nuv_grid_steps = 512\n\n\nprojected_data, projected_coordinates, projected_normals = tcinterp.create_cartographic_projections(\n    image=f\"{metadata_dict['filename']}.tif\",\n    mesh=f\"fish_example/zebrafish_egg_nuclei_uv_second_iteration.obj\",\n    resolution=metadata_dict[\"resolution_in_microns\"],\n    normal_offsets=normal_offsets,\n    uv_grid_steps=uv_grid_steps)\n\n\nplt.imshow(projected_data[0, 0], vmax=10000)\n\n\n\n\n\n\n\n\n\n# save images for visualization in blender\ntexture_path = f\"{os.getcwd()}/{metadata_dict['filename']}_textures_second_iteration\"\ntcio.save_stack_for_blender(projected_data, texture_path, normalization=(0.01, 0.99))\n\nVisualized in blender:\n\n\n\nimage.png",
    "crumbs": [
      "Tutorials",
      "6. Iteratively improving cartographic projections"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Tissue cartography extracts and cartographically projects surfaces from volumetric image data. This turns your 3d data into 2d data which is much easier to visualize, analyze, and computationally process. Tissue cartography is particularly useful in developmental biology, analyzing 3d microscopy data by taking advantage of the laminar, sheet-like organization of many biological tissues. For more on tissue cartography, see Heemskerk & Streichan 2015 and Mitchell & Cislo 2023.\nblender_tissue_cartography comprises an add-on to do tissue cartography using the popular 3d creation software blender, as well as a python package for creating custom/automatized analysis pipelines, and a set of template analysis pipelines/tutorials.\nThe goal is to make tissue cartography as user-friendly as possible using simple, modular Python code and blender’s graphical user interface.\n\n\nThis project is a work in progress and may change rapidly.",
    "crumbs": [
      "blender-tissue-cartography"
    ]
  },
  {
    "objectID": "index.html#what-this-tool-does",
    "href": "index.html#what-this-tool-does",
    "title": "blender-tissue-cartography",
    "section": "",
    "text": "Tissue cartography extracts and cartographically projects surfaces from volumetric image data. This turns your 3d data into 2d data which is much easier to visualize, analyze, and computationally process. Tissue cartography is particularly useful in developmental biology, analyzing 3d microscopy data by taking advantage of the laminar, sheet-like organization of many biological tissues. For more on tissue cartography, see Heemskerk & Streichan 2015 and Mitchell & Cislo 2023.\nblender_tissue_cartography comprises an add-on to do tissue cartography using the popular 3d creation software blender, as well as a python package for creating custom/automatized analysis pipelines, and a set of template analysis pipelines/tutorials.\nThe goal is to make tissue cartography as user-friendly as possible using simple, modular Python code and blender’s graphical user interface.\n\n\nThis project is a work in progress and may change rapidly.",
    "crumbs": [
      "blender-tissue-cartography"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "blender-tissue-cartography",
    "section": "Installation",
    "text": "Installation\nblender_tissue_cartography comprises both an add-on that allows you to do tissue cartography purely graphically within Blender, as well as a python library for custom and/or automatized pipelines.\nSystem requirements Both the Python library and the add-on have no minimum system requirements and can run on any modern laptop. For loading and processing large volumetric image data, you will need sufficient RAM (e.g. if you have a laptop with 8GB RAM, you will likely not be able to load an process a 2GB volumetric .tif file). Most operations in Blender, in particular rendering, will run much faster if your computer has a GPU. The MeshLab library which is required for some (non-essential) operations is not available of new ARM Apple computers.\n\nBlender add-on\n\nInstall the non-python programs: Fiji (optional), Ilastik, Meshlab (optional), and Blender 4.3.\nFrom GitHub, download the file blender_addon/blender_tissue_cartography-1.0.0-[XXX].zip where [XXX] is your operating system (e.g. linux_x64).\n\nIf your operating system is not available, you can also download blender_addon/blender_tissue_cartography.py. In this case you will need to install the python library scikit-image in Blender’s Python interface.\n\nInstall the add-on: Click “Edit -&gt; Preferences -&gt; Add-ons -&gt; Add-on Settings -&gt; Install from disk” and select the file you just downloaded.\nRestart Blender. The add-on can now be found under “Scene -&gt; Tissue Cartography”.\n\n\n\nPython library\n\nInstall the non-python programs: Fiji (optional), Ilastik, Meshlab (optional), and Blender.\nInstall Python via anaconda/miniconda, if you haven’t already.\nInstall blender_tissue_cartography:\n\nrun pip install blender-tissue-cartography in a command window.\n\n(Optional) Install extra Python library for pymeshlab, required for some advanced (re)meshing functionality. This package is not available on new ARM Apple computers.\n\nrun pip install pymeshlab in a command window\n\n\nThe project is hosted on pip, with source code on GitHub.\n\nDeveloper installation\nIf you want to extend blender_tissue_cartography:\n\nClone the github repository.\nCreate a conda environment with all Python dependencies and install the blender_tissue_cartography module. Open a command window in the blender-tissue-cartography directory and type:\n\nconda env create -n blender_tissue_cartography -f environment.yml\nconda activate blender_tissue_cartography\npip install -e .\n\n(Optional) Install extra Python library for pymeshlab, required for some advanced functionality (remeshing and surface reconstruction from within Python).\n\npip install pymeshlab - Note that this package is not available on new ARM Apple computers.\n\nInstall nbdev",
    "crumbs": [
      "blender-tissue-cartography"
    ]
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "blender-tissue-cartography",
    "section": "Documentation",
    "text": "Documentation\n\nFull documentation (including tutorials) is available here: https://nikolas-claussen.github.io/blender-tissue-cartography/\nDatasets and interactive Jupyter notebooks for the tutorials can be downloaded here: https://github.com/nikolas-claussen/blender-tissue-cartography/tree/main/nbs/Tutorials/\nThe methods paper explains the general idea of tissue cartography, the design of blender_tissue_cartography, and shows several examples: https://github.com/nikolas-claussen/blender-tissue-cartography/tree/main/blender_tissue_cartography_methods_paper.pdf",
    "crumbs": [
      "blender-tissue-cartography"
    ]
  },
  {
    "objectID": "index.html#basic-usage",
    "href": "index.html#basic-usage",
    "title": "blender-tissue-cartography",
    "section": "Basic usage",
    "text": "Basic usage\nFor a complete set of tutorials, see the documentation website.\n\nTissue cartography workflow\nTissue cartography starts with a 3D, volumetric image.\n\nCreate a segmentation of your 3D data to identify the surface you want to extract\nConvert the segmentation into a mesh of your surface of interest\nCartographically unwrap the mesh into a 2D plane\nProject your 3D data onto the unwrapped mesh\nVisualize the results in 3D using blender or use the 2D projected data for quantitative analysis.\nBatch process multiple 3D images (e.g. frames of a movie)\n\n\n\nBlender add-on\nThe Blender add-on allows you to carry out steps 2-5 entirely within Blender. Here is a screenshot using the example Drosophila dataset:\n\n\n\nimage.png\n\n\nLeft: Projected 2D image. Center: 3D view of image data (volume bounding box, image slices, and extracted surface). Right: Tissue Cartography add-on panel.\nIn Blender, you can edit meshes and cartographic projections interactively - you can create a preliminary projection of your data automatically, and use it as guidance when editing your cartographic map in blender. Here, we edit the “seam” of our cartographic map based on the region occupied by cells during zebrafish epiboly (tutorial 6).\n\n\n\nimage-2.png\n\n\n\n\nPython library\nFor advanced users, the blender_tissue_cartography library allows creating custom and automated tissue cartography pipelines, typically run from a jupyter computational notebook (which can also serve as lab notebook - notes, comments on the data). blender_tissue_cartography also provides tools for correct quantitative analysis of image data on curved surfaces.\nBelow is a screenshot to give you an idea of the workflow for the example Drosophila dataset: Volumetric data in ImageJ (center), jupyter computational notebook to run the blender_tissue_cartography module (left), and blender project with extracted mesh and texture (right).\n\n\nTutorials\nFully worked-out tutoruals are provided on the documentation webpage. Test data for the tutorials can be downloaded from the nbs/Tutorials/ directory.\nFor the Python library, tutorials take the form of jupyter computational notebooks which you can download and run on your own computer (click the green button “Code” to download a .zip.) To run a tutorial on your computer, follow the installation instructions and then launch jupyter and work through the notebooks in the Tutorials directory in order. I recommended being comfortable with running simple Python code (you don’t have to do any coding yourself).\nThe tutorial notebooks can be used as templates for your own analysis pipelines. Here is an example of a jupyter computational notebook (left), and the created projection visualized in Blender (right).\n\n\n\nimage.png\n\n\n\nNotes for Python beginners\n\nYou will need a working Python installation (see here: installing anaconda/miniconda, and know how to launch jupyter notebooks. You will run the computational notebooks in your browser. Here is a video tutorial\nCreate a new folder for each tissue cartography project. Do not place them into the folder into which you unpacked blender_tissue_cartography - otherwise, your files will be overwritten if you want to update the software\nThe repository contains two sets of notebooks: in the nbs folder and in the nbs/Tutorials folder. The nbs-notebooks are for developing the code. If you don’t want to develop/adapt the code to your needs, you don’t need to look at them. Copy a notebook from the nbs/Tutorials folder - e.g. 03_basics_example.ipynb - into your project folder to use it as a template.\nYou do not need to copy functions into your notebooks manually. If you follow the installation instructions, the code will be installed as a Python package and can be “imported” by Python. See tutorials!\n\n\n\n\nDynamic datasets\nblender_tissue_cartography also allows creating cartographic projections of dynamic datasets (i.e. movies), where the surface of interest can move or deform over time. The user creates a cartographic projection for a reference timepoint which is transfered to all other time-points using surface-to-surface registration algorithms. This generates consistent projections across all timepoints - see tuorials 8 and 9.",
    "crumbs": [
      "blender-tissue-cartography"
    ]
  },
  {
    "objectID": "index.html#software-stack",
    "href": "index.html#software-stack",
    "title": "blender-tissue-cartography",
    "section": "Software stack",
    "text": "Software stack\nNote: the Python libraries will be installed automatically if you follow the installation instructions above.\n\nRequired\n\nPython, with the following libraries\n\njupyter\nnumpy / Matplotlib / Scipy\nskimage various image processing tools.\nh5py for reading/writing of .h5 files.\ntifffile for reading/writing of .tif files, including metadata.\nlibigl Geometry processing.\n\nIlastik Image classification and segmentation,\nBlender Mesh editing and UV mapping.\n\n\n\nOptional\n\nMeshlab GUI and Python library with advanced surface reconstruction tools (required for some workflows).\nPython libraries:\n\nPyMeshLab Python interface to MeshLab.\nnbdev for notebook-based development, if you want to add your own code\n\n\n\n\nOther useful software\n\nMicroscopyNodes plug-in for rendering volumetric .tif files in blender\nBoundary First Flattening advanced tool for creating UV maps with graphical and command line interface\npyFM python library for mesh-to-mesh registration (for dynamic data) which may complement the algorithms that ship with blender_tissue-cartography",
    "crumbs": [
      "blender-tissue-cartography"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "blender-tissue-cartography",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis software is being developed by Nikolas Claussen in the Streichan lab at UCSB. We thank Cecile Regis, Susan Wopat, Matthew Lefebvre, Sean Komura, Gary Han, Noah Mitchel, Boris Fosso, and Dillon Cislo, for sharing data, advice, and software testing.",
    "crumbs": [
      "blender-tissue-cartography"
    ]
  }
]